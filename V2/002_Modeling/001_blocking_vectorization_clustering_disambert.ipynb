{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9977d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import csv\n",
    "import glob\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ddb4a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu\n",
    "# !pip install h5py==2.8.0 (https://stackoverflow.com/questions/39927206/yum-install-libhdf5-dev-on-amazon-linux)\n",
    "# !pip install ujson\n",
    "# !pip install nameparser\n",
    "# !pip install swifter\n",
    "# !pip install duckdb\n",
    "# !pip install sentence-transformers\n",
    "# !pip install gputil\n",
    "# !pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6ef0e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import unittest\n",
    "\n",
    "from bert_auth_disamb.libs.disambert import disambert\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import GPUtil\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579f4744",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_embedding_model_file = \"./citation_embedding_model/\"\n",
    "text_embedding_model_file = \"./text_embedding_model/\"\n",
    "test_file_location = \"test_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "cit_vectorizer = disambert.CitationVectorizer(\n",
    "    filename=citation_embedding_model_file, data_attr=\"references\"\n",
    ")\n",
    "text_vectorizer = disambert.TextVectorizer(\n",
    "    text_embedding_model_file,\n",
    "    device=\"all\",\n",
    "    data_attrs=[\n",
    "        [\"affiliation\", \"title\"],\n",
    "        [\"author\", \"coauthors\", \"affiliation\", \"title\"],\n",
    "        [\"author\", \"coauthors\"],\n",
    "        [\"author\", \"affiliation\", \"title\"],\n",
    "        [\"title\"],\n",
    "    ],\n",
    "    batch_size=256\n",
    ")\n",
    "meta_vectorizer = disambert.MetaVectorizer(\n",
    "    vectorizers=[text_vectorizer, cit_vectorizer]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "736baa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "bucket_name = \"author-disambiguation\"\n",
    "paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
    "response = paginator.paginate(Bucket=bucket_name, Prefix=\"V1/all_data_blocked_and_partitioned/\",\n",
    "                              PaginationConfig={\"PageSize\": 50})\n",
    "\n",
    "filenames = []\n",
    "\n",
    "for page in response:\n",
    "    files = page.get(\"Contents\")\n",
    "    for file in files:\n",
    "        if file['Key'].endswith(\".json\"):\n",
    "            filenames.append(file['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f57bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9c2e512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_lists_with_None(col_list):\n",
    "    if isinstance(col_list, list):\n",
    "        if col_list:\n",
    "            return col_list\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0b568ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_affiliations(col_list):\n",
    "    if isinstance(col_list, list):\n",
    "        if len(col_list) > 0:\n",
    "            if col_list[0] == \"\":\n",
    "                return None\n",
    "            else:\n",
    "                return \" \".join(col_list)\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5c2b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_vectorized(curr_file_name):\n",
    "    print(\"-------reading data\")\n",
    "    df = pd.read_json(f\"s3://author-disambiguation/{curr_file_name}\",\n",
    "                      orient='records', lines=True)\n",
    "    \n",
    "    df['coauthors'] = df['coauthors'].apply(replace_empty_lists_with_None)\n",
    "    df['references'] = df['references'].apply(replace_empty_lists_with_None)\n",
    "    df['affiliation'] = df['affiliation'].apply(replace_empty_affiliations)\n",
    "    df['author_id'] = df['mag_author_id']\n",
    "    \n",
    "    df = df[df['author']!=''][['UID', 'title', 'pub_year', 'journal', 'author',\n",
    "           'paper_author_id', 'affiliation', 'seq_no', 'coauthors', 'references',\n",
    "           'author_id']].copy()\n",
    "    \n",
    "    file_idx = curr_file_name.split(\"/\")[2].split(\"=\")[1]\n",
    "    \n",
    "    print(\"-------saving processed data\")\n",
    "    # save into correct json\n",
    "    raw_data_file = f\"./temp_file_loc/data_file_partition_{file_idx}.json\"\n",
    "    df.to_json(raw_data_file, orient='records', lines=True)\n",
    "\n",
    "    print(\"-------starting blocking\")\n",
    "    # block\n",
    "    if os.path.exists(\"./temp_file_loc/temp_block_file_location_1\"):\n",
    "        os.remove(\"./temp_file_loc/temp_block_file_location_1\")\n",
    "    if os.path.exists(\"./temp_file_loc/temp_block_file_location_1.wal\"):\n",
    "        os.remove(\"./temp_file_loc/temp_block_file_location_1.wal\")\n",
    "    blocked_data_file = \"./temp_file_loc/temp_block_file_location_1\"\n",
    "    blocking_model = disambert.BlockingByName(name_attr=\"author\")\n",
    "    blocked_dataset = disambert.BlockedDataset(chunksize=1000)\n",
    "    blocked_dataset.blocking(\n",
    "        blocking_model,\n",
    "        raw_data_file,\n",
    "        blocked_data_file\n",
    "    )\n",
    "    \n",
    "    print(\"-------starting vectorizing\")\n",
    "    # vectorize\n",
    "    vector_data_file = f\"./temp_file_loc/vector_file_partition_{file_idx}\"\n",
    "    blocked_dataset.reset_iterator()\n",
    "    vector_dataset = disambert.BlockedVectorDataset(\n",
    "        filename_or_dataset=blocked_dataset,\n",
    "        block_id_key=\"_block_id\",\n",
    "        vectorizer=meta_vectorizer,\n",
    "        data_id_key=\"paper_author_id\",\n",
    "        attribute_keys=[\"author_id\"],\n",
    "        chunksize=1000\n",
    "    )\n",
    "    vector_dataset.save(vector_data_file, batch_size=4096)\n",
    "    \n",
    "    print(\"-------transferring to S3\")\n",
    "    # transfer to S3\n",
    "    os.system(f\"aws s3 cp {vector_data_file} s3://author-disambiguation/V1/vectorized_data/partition_{file_idx}/ --no-progress\")\n",
    "    os.system(f\"aws s3 cp {raw_data_file} s3://author-disambiguation/V1/vectorized_data/partition_{file_idx}/ --no-progress\")\n",
    "    \n",
    "    print(\"-------delete files\")\n",
    "    # delete files\n",
    "    os.remove(vector_data_file)\n",
    "    os.remove(raw_data_file)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa6858a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1/all_data_blocked_and_partitioned/random_partition_number=10/part-00000-tid-767740779014940659-7ce3996d-ba3a-475b-ac9c-f6d3b3390d40-8518-11.c000.json\n",
      "-------reading data\n",
      "-------saving processed data\n",
      "-------starting blocking\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019209861755371094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8e5db7cc5f4570a19c7d5b86b52077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------starting vectorizing\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0180361270904541,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 287,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0cfbf0f0dc4f71b990d03fc1472980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------transferring to S3\n",
      "upload: temp_file_loc/vector_file_partition_10 to s3://author-disambiguation/V1/vectorized_data/partition_10/vector_file_partition_10\n",
      "upload: temp_file_loc/data_file_partition_10.json to s3://author-disambiguation/V1/vectorized_data/partition_10/data_file_partition_10.json\n",
      "-------delete files\n",
      "\n",
      "V1/all_data_blocked_and_partitioned/random_partition_number=100/part-00000-tid-767740779014940659-7ce3996d-ba3a-475b-ac9c-f6d3b3390d40-8518-101.c000.json\n",
      "-------reading data\n",
      "-------saving processed data\n",
      "-------starting blocking\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018769264221191406,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad55576b30534f46bbc76cf8023f1709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------starting vectorizing\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01795482635498047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 295,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc81b3dd310b4840a4c9f39f3370eef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/295 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------transferring to S3\n",
      "Completed 9.7 GiB/15.8 GiB (98.2 MiB/s) with 1 file(s) remaining   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------starting vectorizing\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018047571182250977,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 351,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5926c9aa92a4d3bad7160d92000d3b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "for i in filenames[2:15]:\n",
    "    print(i)\n",
    "    get_data_vectorized(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f604cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c12a0a1",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d2e51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "bucket_name = \"author-disambiguation\"\n",
    "paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
    "response = paginator.paginate(Bucket=bucket_name, Prefix=\"V1/vectorized_data/\",\n",
    "                              PaginationConfig={\"PageSize\": 50})\n",
    "\n",
    "filenames = []\n",
    "\n",
    "for page in response:\n",
    "    files = page.get(\"Contents\")\n",
    "    for file in files:\n",
    "        if \"vector_file\" in file['Key']:\n",
    "            filenames.append(file['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d67c2fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04bbddc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V1/vectorized_data/partition_0/vector_file_partition_0',\n",
       " 'V1/vectorized_data/partition_1/vector_file_partition_1',\n",
       " 'V1/vectorized_data/partition_10/vector_file_partition_10',\n",
       " 'V1/vectorized_data/partition_100/vector_file_partition_100',\n",
       " 'V1/vectorized_data/partition_101/vector_file_partition_101',\n",
       " 'V1/vectorized_data/partition_102/vector_file_partition_102',\n",
       " 'V1/vectorized_data/partition_103/vector_file_partition_103',\n",
       " 'V1/vectorized_data/partition_104/vector_file_partition_104',\n",
       " 'V1/vectorized_data/partition_105/vector_file_partition_105',\n",
       " 'V1/vectorized_data/partition_106/vector_file_partition_106']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f1eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Clustering\n",
    "#\n",
    "device = None\n",
    "clustering_model_file = \"./clustering_model/\"\n",
    "\n",
    "clustering = disambert.PottsClustering(\n",
    "    num_neighbors=100,\n",
    "    device=device,\n",
    "    filename=clustering_model_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b92fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clustered_data(file_name):\n",
    "    \n",
    "    print(\"-------pulling data from S3\")\n",
    "    os.system(f\"aws s3 cp s3://author-disambiguation/{file_name} ./temp_file_loc/ --no-progress\")\n",
    "    \n",
    "    file_idx = file_name.split('/')[-1].split('_')[-1]\n",
    "    vector_data_file = f\"./temp_file_loc/{file_name.split('/')[-1]}\"\n",
    "    output_file = f\"./temp_file_loc/cluster_file_partition_{file_idx}\"\n",
    "\n",
    "    #\n",
    "    # Load the vector dataset\n",
    "    #\n",
    "    vector_dataset = disambert.BlockedVectorDataset(\n",
    "        filename_or_dataset=vector_data_file,\n",
    "        block_id_key=\"_block_id\",\n",
    "        chunksize=2200000,\n",
    "    )\n",
    "    \n",
    "    first_write = True\n",
    "    n_data_samples = len(vector_dataset)\n",
    "    data_keys = clustering.vector_keys\n",
    "    \n",
    "    print(\"-------clustering and writing to file\")\n",
    "    with open(output_file, \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"block_id\", \"data_id\", \"cluster_id\"])\n",
    "#         with tqdm(total=n_data_samples) as progress:\n",
    "        for data_ids, data in vector_dataset.get_one_block():\n",
    "            if len(data_ids) == 1:\n",
    "                cids = np.zeros_like(data_ids)\n",
    "                writer.writerow([data[\"_block_id\"][0], data_ids[0], 0])\n",
    "#                 progress.update(len(data_ids))\n",
    "                continue\n",
    "\n",
    "            for k in data_keys:\n",
    "                data[k] = torch.tensor(data[k], dtype=torch.float32)\n",
    "            cids = clustering.transform(data)\n",
    "            block_ids = data[\"_block_id\"]\n",
    "            block_ids, data_ids, cids = (\n",
    "                data[\"_block_id\"].tolist(),\n",
    "                data_ids.tolist(),\n",
    "                cids.tolist(),\n",
    "            )\n",
    "            writer.writerows(\n",
    "                [\n",
    "                    [block_ids[i], data_ids[i], int(cids[i])]\n",
    "                    for i in range(len(block_ids))\n",
    "                ]\n",
    "            )\n",
    "#             progress.update(len(data_ids))\n",
    "                \n",
    "    print(\"-------transferring to S3\")\n",
    "    # transfer to S3\n",
    "    os.system(f\"aws s3 cp {output_file} s3://author-disambiguation/V1/vectorized_data/partition_{file_idx}/ --no-progress\")\n",
    "    \n",
    "    print(\"-------delete files\")\n",
    "    # delete files\n",
    "    os.remove(vector_data_file)\n",
    "    os.remove(output_file)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c32bf95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d41d7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unsuccessful_filenames_1.pkl\", \"rb\") as f:\n",
    "    files_for_later_1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b486fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unsuccessful_filenames_2.pkl\", \"rb\") as f:\n",
    "    files_for_later_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2e45fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = files_for_later_1 + files_for_later_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ac98c16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1/vectorized_data/partition_336/vector_file_partition_336\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_336/vector_file_partition_336 to temp_file_loc/vector_file_partition_336\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_336 to s3://author-disambiguation/V1/vectorized_data/partition_336/cluster_file_partition_336\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_362/vector_file_partition_362\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_362/vector_file_partition_362 to temp_file_loc/vector_file_partition_362\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_362 to s3://author-disambiguation/V1/vectorized_data/partition_362/cluster_file_partition_362\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_367/vector_file_partition_367\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_367/vector_file_partition_367 to temp_file_loc/vector_file_partition_367\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_367 to s3://author-disambiguation/V1/vectorized_data/partition_367/cluster_file_partition_367\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_393/vector_file_partition_393\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_393/vector_file_partition_393 to temp_file_loc/vector_file_partition_393\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_393 to s3://author-disambiguation/V1/vectorized_data/partition_393/cluster_file_partition_393\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_416/vector_file_partition_416\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_416/vector_file_partition_416 to temp_file_loc/vector_file_partition_416\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_416 to s3://author-disambiguation/V1/vectorized_data/partition_416/cluster_file_partition_416\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_434/vector_file_partition_434\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_434/vector_file_partition_434 to temp_file_loc/vector_file_partition_434\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_434 to s3://author-disambiguation/V1/vectorized_data/partition_434/cluster_file_partition_434\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_452/vector_file_partition_452\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_452/vector_file_partition_452 to temp_file_loc/vector_file_partition_452\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_452 to s3://author-disambiguation/V1/vectorized_data/partition_452/cluster_file_partition_452\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_463/vector_file_partition_463\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_463/vector_file_partition_463 to temp_file_loc/vector_file_partition_463\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_463 to s3://author-disambiguation/V1/vectorized_data/partition_463/cluster_file_partition_463\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_467/vector_file_partition_467\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_467/vector_file_partition_467 to temp_file_loc/vector_file_partition_467\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_467 to s3://author-disambiguation/V1/vectorized_data/partition_467/cluster_file_partition_467\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_48/vector_file_partition_48\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_48/vector_file_partition_48 to temp_file_loc/vector_file_partition_48\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_48 to s3://author-disambiguation/V1/vectorized_data/partition_48/cluster_file_partition_48\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_489/vector_file_partition_489\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_489/vector_file_partition_489 to temp_file_loc/vector_file_partition_489\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_489 to s3://author-disambiguation/V1/vectorized_data/partition_489/cluster_file_partition_489\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_496/vector_file_partition_496\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_496/vector_file_partition_496 to temp_file_loc/vector_file_partition_496\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_496 to s3://author-disambiguation/V1/vectorized_data/partition_496/cluster_file_partition_496\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_54/vector_file_partition_54\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_54/vector_file_partition_54 to temp_file_loc/vector_file_partition_54\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_54 to s3://author-disambiguation/V1/vectorized_data/partition_54/cluster_file_partition_54\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_56/vector_file_partition_56\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_56/vector_file_partition_56 to temp_file_loc/vector_file_partition_56\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_56 to s3://author-disambiguation/V1/vectorized_data/partition_56/cluster_file_partition_56\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_61/vector_file_partition_61\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_61/vector_file_partition_61 to temp_file_loc/vector_file_partition_61\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_61 to s3://author-disambiguation/V1/vectorized_data/partition_61/cluster_file_partition_61\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_74/vector_file_partition_74\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_74/vector_file_partition_74 to temp_file_loc/vector_file_partition_74\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_74 to s3://author-disambiguation/V1/vectorized_data/partition_74/cluster_file_partition_74\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_83/vector_file_partition_83\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_83/vector_file_partition_83 to temp_file_loc/vector_file_partition_83\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_83 to s3://author-disambiguation/V1/vectorized_data/partition_83/cluster_file_partition_83\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_183/vector_file_partition_183\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_183/vector_file_partition_183 to temp_file_loc/vector_file_partition_183\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_183 to s3://author-disambiguation/V1/vectorized_data/partition_183/cluster_file_partition_183\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_185/vector_file_partition_185\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_185/vector_file_partition_185 to temp_file_loc/vector_file_partition_185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_185 to s3://author-disambiguation/V1/vectorized_data/partition_185/cluster_file_partition_185\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_188/vector_file_partition_188\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_188/vector_file_partition_188 to temp_file_loc/vector_file_partition_188\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_188 to s3://author-disambiguation/V1/vectorized_data/partition_188/cluster_file_partition_188\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_225/vector_file_partition_225\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_225/vector_file_partition_225 to temp_file_loc/vector_file_partition_225\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_225 to s3://author-disambiguation/V1/vectorized_data/partition_225/cluster_file_partition_225\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_228/vector_file_partition_228\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_228/vector_file_partition_228 to temp_file_loc/vector_file_partition_228\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_228 to s3://author-disambiguation/V1/vectorized_data/partition_228/cluster_file_partition_228\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_244/vector_file_partition_244\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_244/vector_file_partition_244 to temp_file_loc/vector_file_partition_244\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_244 to s3://author-disambiguation/V1/vectorized_data/partition_244/cluster_file_partition_244\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_25/vector_file_partition_25\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_25/vector_file_partition_25 to temp_file_loc/vector_file_partition_25\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_25 to s3://author-disambiguation/V1/vectorized_data/partition_25/cluster_file_partition_25\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_283/vector_file_partition_283\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_283/vector_file_partition_283 to temp_file_loc/vector_file_partition_283\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_283 to s3://author-disambiguation/V1/vectorized_data/partition_283/cluster_file_partition_283\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_284/vector_file_partition_284\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_284/vector_file_partition_284 to temp_file_loc/vector_file_partition_284\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_284 to s3://author-disambiguation/V1/vectorized_data/partition_284/cluster_file_partition_284\n",
      "-------delete files\n",
      "\n",
      "V1/vectorized_data/partition_286/vector_file_partition_286\n",
      "-------pulling data from S3\n",
      "download: s3://author-disambiguation/V1/vectorized_data/partition_286/vector_file_partition_286 to temp_file_loc/vector_file_partition_286\n",
      "-------clustering and writing to file\n",
      "-------transferring to S3\n",
      "upload: temp_file_loc/cluster_file_partition_286 to s3://author-disambiguation/V1/vectorized_data/partition_286/cluster_file_partition_286\n",
      "-------delete files\n",
      "\n",
      "Completed files list, 0 unsuccessful\n",
      "CPU times: user 10d 16h 5min 23s, sys: 2h 52min 53s, total: 10d 18h 58min 16s\n",
      "Wall time: 10h 29min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files_for_later = []\n",
    "for i in filenames:\n",
    "    print(i)\n",
    "    try:\n",
    "        get_clustered_data(i)\n",
    "    except:\n",
    "        print(\"---------------------- ERROR ----------------------\")\n",
    "        vec_file = f\"./temp_file_loc/{i.split('/')[-1]}\"\n",
    "        out_file = f\"./temp_file_loc/cluster_file_partition_{i.split('/')[-1].split('_')[-1]}\"\n",
    "        if os.path.exists(vec_file):\n",
    "            os.remove(vec_file)\n",
    "        if os.path.exists(out_file):\n",
    "            os.remove(out_file)\n",
    "        files_for_later.append(i)\n",
    "\n",
    "print(f\"Completed files list, {len(files_for_later)} unsuccessful\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a220f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unsuccessful_filenames_3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(files_for_later, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd2e928e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "018eabdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ba458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35af13f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1/vectorized_data/partition_336/vector_file_partition_336\n",
      "V1/vectorized_data/partition_362/vector_file_partition_362\n",
      "V1/vectorized_data/partition_367/vector_file_partition_367\n",
      "V1/vectorized_data/partition_393/vector_file_partition_393\n",
      "V1/vectorized_data/partition_416/vector_file_partition_416\n",
      "V1/vectorized_data/partition_434/vector_file_partition_434\n",
      "V1/vectorized_data/partition_452/vector_file_partition_452\n",
      "V1/vectorized_data/partition_463/vector_file_partition_463\n",
      "V1/vectorized_data/partition_467/vector_file_partition_467\n",
      "V1/vectorized_data/partition_48/vector_file_partition_48\n",
      "V1/vectorized_data/partition_489/vector_file_partition_489\n",
      "V1/vectorized_data/partition_496/vector_file_partition_496\n",
      "V1/vectorized_data/partition_54/vector_file_partition_54\n",
      "V1/vectorized_data/partition_56/vector_file_partition_56\n",
      "V1/vectorized_data/partition_61/vector_file_partition_61\n",
      "V1/vectorized_data/partition_74/vector_file_partition_74\n",
      "V1/vectorized_data/partition_83/vector_file_partition_83\n"
     ]
    }
   ],
   "source": [
    "for i in files_for_later_1:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92fc22ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1/vectorized_data/partition_183/vector_file_partition_183\n",
      "V1/vectorized_data/partition_185/vector_file_partition_185\n",
      "V1/vectorized_data/partition_188/vector_file_partition_188\n",
      "V1/vectorized_data/partition_225/vector_file_partition_225\n",
      "V1/vectorized_data/partition_228/vector_file_partition_228\n",
      "V1/vectorized_data/partition_244/vector_file_partition_244\n",
      "V1/vectorized_data/partition_25/vector_file_partition_25\n",
      "V1/vectorized_data/partition_283/vector_file_partition_283\n",
      "V1/vectorized_data/partition_284/vector_file_partition_284\n",
      "V1/vectorized_data/partition_286/vector_file_partition_286\n"
     ]
    }
   ],
   "source": [
    "for i in files_for_later_2:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf0eba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18809d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
