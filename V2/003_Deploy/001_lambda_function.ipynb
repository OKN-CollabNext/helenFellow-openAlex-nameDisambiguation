{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778cb7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import boto3\n",
    "import time\n",
    "import pandas as pd\n",
    "from nameparser import HumanName\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35d98792",
   "metadata": {},
   "outputs": [],
   "source": [
    "_logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    filename='log_file.log',\n",
    "    format='%(asctime)s.%(msecs)03dZ,%(pathname)s:%(lineno)d,%(levelname)s,%(module)s,%(funcName)s: %(message)s',\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "012d37d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_author_name(author):\n",
    "    author_names = author.strip().split(\" \")\n",
    "    author_names = [x for x in author_names if x != \" \"]\n",
    "    \n",
    "    if len(author_names) == 1:\n",
    "        pass\n",
    "    elif len(author_names) == 2:\n",
    "        if len(author_names[1]) == 1:\n",
    "            if author_names[0][-1]==\",\":\n",
    "                author_names = [author_names[1].upper()] + [author_names[0][:-1]]\n",
    "            else:\n",
    "                author_names = [author_names[1].upper()] + [author_names[0]]\n",
    "        elif len(author_names[1])==2:\n",
    "            if author_names[1][1] in [\".\",\",\",\";\",\":\"]:\n",
    "                if author_names[0][-1]==\",\":\n",
    "                    author_names = [author_names[1].upper()] + [author_names[0][:-1]]\n",
    "                else:\n",
    "                    author_names = [author_names[1].upper()] + [author_names[0]]\n",
    "            elif ((author_names[1].lower()[0] not in [\"a\",\"e\",\"i\",\"o\",\"u\"]) and \n",
    "                  (author_names[1].lower()[1] not in [\"a\",\"e\",\"i\",\"o\",\"u\"]) and \n",
    "                  (re.match(r'[a-zA-Z]{2}', author_names[1])) and \n",
    "                  (author_names[1].lower() not in ['ng','ty'])):\n",
    "                author_names = [author_names[1].upper()] + [author_names[0]]\n",
    "        elif len(author_names[1]) >=3:\n",
    "            if author_names[0][-1] == \",\":\n",
    "                author_names = [author_names[1]] + [author_names[0][:-1]]\n",
    "        else:\n",
    "            pass\n",
    "    elif len(author_names) == 3:\n",
    "        pass\n",
    "    elif len(author_names) >= 4:\n",
    "        if (\"(\" in author_names) and (\")\" in author_names):\n",
    "            temp_names = author_names[author_names.index(\"(\")+1:author_names.index(\")\")]\n",
    "            if len(temp_names) > 1:\n",
    "                author_names = temp_names\n",
    "            else:\n",
    "                author_names = author_names[:author_names.index(\"(\")]\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return \" \".join(author_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f102a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_coauthors(coauthors, author):\n",
    "    if coauthors:\n",
    "        if isinstance(coauthors, list):\n",
    "            final_coauthors = []\n",
    "            for coauthor in coauthors:\n",
    "                coauthor = check_author_name(coauthor)\n",
    "                if coauthor != author:\n",
    "                    final_coauthors.append(coauthor)\n",
    "            if not final_coauthors:\n",
    "                final_coauthors = None\n",
    "        else:\n",
    "            final_coauthors = None\n",
    "    else:\n",
    "        final_coauthors = None\n",
    "    return final_coauthors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bda7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block_id(name):\n",
    "    person = HumanName(name)\n",
    "    last_name = person.last\n",
    "    first_name = person.first\n",
    "    if (len(first_name) < 1) & (len(last_name) < 1):\n",
    "        return name.lower()\n",
    "    elif len(first_name) < 1:\n",
    "        return last_name\n",
    "    elif len(last_name) < 1:\n",
    "        return name.lower()\n",
    "    else:\n",
    "        initials = f\"{first_name[0]}_{last_name}\"\n",
    "        return initials.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7a23518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_data_paths_s3(bucket_name, data_prefix):\n",
    "    s3 = boto3.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "    new_data_file_paths = []\n",
    "    for my_bucket_object in my_bucket.objects.filter(Prefix=data_prefix).all():\n",
    "        if my_bucket_object.key.endswith('.jsonl.gz'):\n",
    "            _logger.info(f\"Found new data file: {my_bucket_object.key}\")\n",
    "            new_data_file_paths.append(my_bucket_object.key)\n",
    "    return new_data_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6780eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_ec2_instance(instance_id, client):\n",
    "    response = {'ResponseMetadata':{'HTTPStatusCode': 0}}\n",
    "    while response['ResponseMetadata']['HTTPStatusCode'] != 200:\n",
    "        try:\n",
    "            _logger.info(f\"Starting EC2 instance {instance_id}\")\n",
    "            response = ec2.start_instances(InstanceIds=[instance_id], DryRun=False)\n",
    "        except ClientError as e:\n",
    "            _logger.error('Error turning on the EC2', exc_info=True,\n",
    "                            stack_info=True)\n",
    "        time.sleep(1)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb4eb36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_ec2_instance(instance_id, client):\n",
    "    response = {'ResponseMetadata':{'HTTPStatusCode': 0}}\n",
    "    while response['ResponseMetadata']['HTTPStatusCode'] != 200:\n",
    "        try:\n",
    "            _logger.info(f\"Stopping EC2 instance {instance_id}\")\n",
    "            response = ec2.stop_instances(InstanceIds=[instance_id], DryRun=False)\n",
    "        except ClientError as e:\n",
    "            _logger.error('Error turning off the EC2', exc_info=True,\n",
    "                            stack_info=True)\n",
    "        time.sleep(1)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5191564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_delete_log_file(bucket, save_prefix):\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    file_name = \"log_file.log\"\n",
    "    curr_datetime = datetime.now()\n",
    "    file_date = curr_datetime.strftime(\"%Y_%m_%d\")\n",
    "    file_time = curr_datetime.strftime(\"%H_%M\")\n",
    "    object_name = f\"{save_prefix}/{file_date}/log_file_{file_time}.txt\"\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        print(f\"Log file save did not work: {e}\")\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cf99703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_transferred_data_from_001_new_data(bucket_name, data_paths):\n",
    "    s3 = boto3.resource('s3')\n",
    "    for data_path in data_paths:\n",
    "        os.system(f\"aws s3 mv s3://{bucket_name}/{data_path} s3://author-name-disambiguation/V1/data/ZZZ_Archive/\")\n",
    "#         s3.Object(bucket_name, data_path).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3031206d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def lambda_handler(event, context):\n",
    "_logger.setLevel(10)\n",
    "# _logger.info('Received event: ' + str(event))\n",
    "\n",
    "# define variables and paths\n",
    "s3_bucket = \"author-name-disambiguation\"\n",
    "new_data_prefix = \"V1/data/001_NEW_DATA\"\n",
    "curr_datetime = datetime.now()\n",
    "year_month = curr_datetime.strftime(\"%Y_%m\")\n",
    "date_str = curr_datetime.strftime(\"%Y_%m_%d_%H_%M\")\n",
    "max_num_works = 3000000\n",
    "no_author_data_prefix = \"V1/data/XXX_NO_AUTHOR\"\n",
    "new_block_id_node = 1\n",
    "node_1_data_path = \"V1/data/002_IN_PROGRESS/NODE_1/\"\n",
    "node_2_data_path = \"V1/data/002_IN_PROGRESS/NODE_2/\"\n",
    "node_3_data_path = \"V1/data/002_IN_PROGRESS/NODE_3/\"\n",
    "log_file_save_prefix = \"V1/log_files/main\"\n",
    "node_1_id = \"\"\n",
    "node_2_id = \"\"\n",
    "node_3_id = \"\"\n",
    "ec2 = boto3.client('ec2')\n",
    "data_cols = ['block_id','paper_author_id','orcid','author','coauthors']\n",
    "\n",
    "# get any data in bucket/prefix\n",
    "data_in_bucket = get_new_data_paths_s3(s3_bucket, new_data_prefix)\n",
    "\n",
    "_logger.info(f\"Found {len(data_in_bucket)} files in S3\")\n",
    "\n",
    "if data_in_bucket:\n",
    "\n",
    "    # iterate through to only take take certain number of works\n",
    "    new_data = pd.DataFrame()\n",
    "    data_transferred = []\n",
    "    for data in data_in_bucket:\n",
    "        try:\n",
    "            temp_new_data = pd.read_json(f\"s3://{s3_bucket}/{data}\", \n",
    "                                         compression='gzip', lines=True, orient='records')\n",
    "\n",
    "            if (new_data.shape[0] + temp_new_data.shape[0]) > max_num_works:\n",
    "                if new_data.shape[0] == 0:\n",
    "                    new_data = temp_new_data.copy()\n",
    "                    data_transferred = [data]\n",
    "                    _logger.info(f\"Added file with {temp_new_data.shape[0]} rows\")\n",
    "                    print(f\"Added file with {temp_new_data.shape[0]} rows\")\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                data_transferred.append(data)\n",
    "                _logger.info(f\"Added file with {temp_new_data.shape[0]} rows\")\n",
    "                print(f\"Added file with {temp_new_data.shape[0]} rows\")\n",
    "                new_data = pd.concat([new_data, temp_new_data], axis=0)\n",
    "        except:\n",
    "            _logger.error(f'Error trying to read new data: bucket[{s3_bucket}] prefix[{data}]', \n",
    "                          exc_info=True, stack_info=True)\n",
    "            \n",
    "    new_data_no_author = new_data[new_data['author'].isnull()].copy()\n",
    "    new_data = new_data[~new_data['author'].isnull()].copy()\n",
    "    \n",
    "    # save papers with no authors to bucket\n",
    "    new_data_no_author[['paper_author_id','title']] \\\n",
    "        .to_parquet(f\"s3://{s3_bucket}/{no_author_data_prefix}/{year_month}/{date_str}.parquet\")\n",
    "            \n",
    "    # normalize author and coauthors\n",
    "    print(\"\")\n",
    "    print(\"Normalizing authors and coauthors\")\n",
    "    new_data['author'] = new_data['author'].apply(check_author_name)\n",
    "    new_data['coauthors'] = new_data.apply(lambda x: transform_coauthors(x.coauthors, x.author), axis=1)\n",
    "\n",
    "    # create block_id\n",
    "    print(\"Creating block ID\")\n",
    "    new_data['block_id'] = new_data['author'].astype('str').apply(get_block_id)\n",
    "\n",
    "    # load block_id to node mapping\n",
    "    print(\"Loading block ID to node mapping file\")\n",
    "    try:\n",
    "        block_id_node_mapping = pd.read_parquet(\"block_id_node_mapping.parquet\")\n",
    "    except:\n",
    "        print(\"Error loading the mapping table\")\n",
    "        _logger.error('Error loading the mapping table', exc_info=True,\n",
    "                          stack_info=True)\n",
    "else:\n",
    "    _logger.info(f\"No new data to disambiguate!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb139bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # start up nodes\n",
    "#     _ = start_ec2_instance(node_1_id, ec2)\n",
    "#     _ = start_ec2_instance(node_2_id, ec2)\n",
    "#     _ = start_ec2_instance(node_3_id, ec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20842592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through steps to sort and send data to nodes\n",
    "data_to_write = new_data.merge(block_id_node_mapping, how='left', on='block_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f8bd984",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_write['node'] = data_to_write['node'].fillna(new_block_id_node).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f461873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    473091\n",
       "1    466945\n",
       "3    425051\n",
       "Name: node, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_write['node'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12662a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023_02_21_15_26'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82eb68d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for node 1\n",
    "if data_to_write[data_to_write['node']==1].shape[0] > 0:\n",
    "    data_to_write[data_to_write['node']==1][data_cols]\\\n",
    "        .to_parquet(f\"s3://{s3_bucket}/{node_1_data_path}{date_str}_1.parquet\")\n",
    "    # add something for triggering the node here\n",
    "else:\n",
    "    _logger.info(\"No data for node 1!\")\n",
    "    # add node shutdown here\n",
    "\n",
    "# data for node 2\n",
    "if data_to_write[data_to_write['node']==2].shape[0] > 0:\n",
    "    data_to_write[data_to_write['node']==2][data_cols]\\\n",
    "        .to_parquet(f\"s3://{s3_bucket}/{node_2_data_path}{date_str}_2.parquet\")\n",
    "    # add something for triggering the node here\n",
    "else:\n",
    "    _logger.info(\"No data for node 2!\")\n",
    "    # add node shutdown here\n",
    "\n",
    "# data for node 3\n",
    "if data_to_write[data_to_write['node']==3].shape[0] > 0:\n",
    "    data_to_write[data_to_write['node']==3][data_cols]\\\n",
    "        .to_parquet(f\"s3://{s3_bucket}/{node_3_data_path}{date_str}_3.parquet\")\n",
    "    # add something for triggering the node here\n",
    "else:\n",
    "    _logger.info(\"No data for node 3!\")\n",
    "    # add node shutdown here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b515b34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-398.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-398.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-399.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-399.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-400.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-400.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-401.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-401.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-402.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-402.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-403.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-403.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-404.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-404.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-405.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-405.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-406.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-406.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-407.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-407.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-408.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-408.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-409.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-409.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-410.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-410.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-411.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-411.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-412.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-412.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-413.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-413.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-414.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-414.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-415.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-415.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-416.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-416.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-417.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-417.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-418.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-418.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-419.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-419.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-420.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-420.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-421.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-421.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-422.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-422.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-423.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-423.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-424.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-424.jsonl.gz\n",
      "move: s3://author-name-disambiguation/V1/data/001_NEW_DATA/batch-425.jsonl.gz to s3://author-name-disambiguation/V1/data/ZZZ_Archive/batch-425.jsonl.gz\n"
     ]
    }
   ],
   "source": [
    "# delete file that was transferred to nodes\n",
    "remove_transferred_data_from_001_new_data(s3_bucket, data_transferred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31a0c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save log to S3\n",
    "_ = save_and_delete_log_file(s3_bucket, log_file_save_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1122b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"no new data to disambiguate\")\n",
    "#     _logger.info(\"No new data to disambiguate!\")\n",
    "\n",
    "#     _ = save_and_delete_log_file()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
