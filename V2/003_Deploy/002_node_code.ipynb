{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9977d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import glob\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "from unidecode import unidecode\n",
    "from datetime import datetime\n",
    "from nameparser import HumanName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a25ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting logging\n",
    "_logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    filename='log_file.log',\n",
    "    format='%(asctime)s.%(msecs)03dZ,%(pathname)s:%(lineno)d,%(levelname)s,%(module)s,%(funcName)s: %(message)s',\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "_logger.setLevel(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f881e1f",
   "metadata": {},
   "source": [
    "### Get input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd6e2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_data_paths_s3(bucket_name, data_prefix):\n",
    "    s3 = boto3.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "    new_data_file_paths = []\n",
    "    for my_bucket_object in my_bucket.objects.filter(Prefix=data_prefix).all():\n",
    "        if my_bucket_object.key.endswith('.parquet'):\n",
    "            _logger.info(f\"Found new data file: {my_bucket_object.key}\")\n",
    "            new_data_file_paths.append(my_bucket_object.key)\n",
    "    return new_data_file_paths\n",
    "\n",
    "####### ALL ABOVE IS DONE (except for variables and paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b635e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_data_file(bucket_name, data_prefix):\n",
    "    new_file_key = get_new_data_paths_s3(bucket_name, data_prefix)[0]\n",
    "    \n",
    "    new_file = pd.read_parquet(f\"s3://{bucket_name}/{new_file_key}\")\n",
    "    return new_file, new_file_key\n",
    "\n",
    "####### ALL ABOVE IS DONE (except for variables and paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e00aff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_latin_name(text):\n",
    "    try:        \n",
    "        str(text).encode('latin-1')\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "####### ALL ABOVE IS DONE (except for variables and paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c62e4300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_name(name_text):\n",
    "    name_text = name_text.strip().replace(\".\", \" \").replace(\"-\", \" \").replace(\"  \", \" \").replace(\"  \", \" \")\\\n",
    "                .replace(\"  \", \" \")\n",
    "    person = HumanName(name_text)\n",
    "    first_name = \"\".join([x for x in person.first if x not in [\".\", \"-\"]])\n",
    "    last_name = person.last\n",
    "    middle_name_1 = person.middle.strip()\n",
    "    if len(middle_name_1.split(\" \")) > 1:\n",
    "        middle_name_2 = \" \".join(middle_name_1.split(\" \")[1:]).strip()\n",
    "        middle_name_1 = middle_name_1.split(\" \")[0].strip()\n",
    "    else:\n",
    "        middle_name_2 = \"\"\n",
    "        \n",
    "    middle_name_1 = \"\".join([x for x in middle_name_1 if x not in [\".\", \"-\"]])\n",
    "    middle_name_2 = \"\".join([x for x in middle_name_2 if x not in [\".\", \"-\"]])\n",
    "        \n",
    "    if (len(first_name) == 3) and (first_name.isupper()) and (not middle_name_2 and not middle_name_1):\n",
    "        middle_name_1 = first_name[1]\n",
    "        middle_name_2 = first_name[2]\n",
    "        first_name = first_name[0]\n",
    "    elif (len(first_name) == 2) and (first_name.isupper()) and (not middle_name_1):\n",
    "        middle_name_1 = first_name[1]\n",
    "        first_name = first_name[0]\n",
    "        \n",
    "    return [unidecode(first_name), unidecode(middle_name_1), unidecode(middle_name_2), unidecode(last_name)]\n",
    "\n",
    "####### ALL ABOVE IS DONE (except for variables and paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50404ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_author_name_match(author_names):\n",
    "    \n",
    "    if not isinstance(author_names, list):\n",
    "        if isinstance(author_names, str):\n",
    "            author_names = [author_names]\n",
    "        else:\n",
    "            author_names = author_names.tolist()\n",
    "        \n",
    "    first_names = []\n",
    "    first_initials = []\n",
    "    middle_1_names = []\n",
    "    middle_1_initials = []\n",
    "    middle_2_names = []\n",
    "    middle_2_initials = []\n",
    "    \n",
    "    for author_name in author_names:\n",
    "        if not author_name:\n",
    "            pass\n",
    "        else:\n",
    "            if (check_latin_name(author_name)) and (len(author_name.split(\" \"))>1): \n",
    "                name = human_name(author_name)\n",
    "                # get all of the different versions of the name here\n",
    "\n",
    "                if name[0] and name[1] and name[2] and name[3]:\n",
    "                    # first name\n",
    "                    if len(name[0]) > 1:\n",
    "                        first_names.append(str(name[0]).lower())\n",
    "                        first_initials.append(str(name[0])[0].lower())\n",
    "                    else:\n",
    "                        first_initials.append(str(name[0]).lower())\n",
    "\n",
    "                    # middle 1 names\n",
    "                    if len(name[1]) > 1:\n",
    "                        middle_1_names.append(str(name[1]).lower())\n",
    "                        middle_1_initials.append(str(name[1])[0].lower())\n",
    "                    else:\n",
    "                        middle_1_initials.append(str(name[1]).lower())\n",
    "\n",
    "                    # middle 2 names\n",
    "                    if len(name[2]) > 1:\n",
    "                        middle_2_names.append(str(name[2]).lower())\n",
    "                        middle_2_initials.append(str(name[2])[0].lower())\n",
    "                    else:\n",
    "                        middle_2_initials.append(str(name[2]).lower())\n",
    "\n",
    "                elif name[0] and name[1] and name[3]:\n",
    "                    # first name\n",
    "                    if len(name[0]) > 1:\n",
    "                        first_names.append(str(name[0]).lower())\n",
    "                        first_initials.append(str(name[0])[0].lower())\n",
    "                    else:\n",
    "                        first_initials.append(str(name[0]).lower())\n",
    "\n",
    "                    # middle 1 names\n",
    "                    if len(name[1]) > 1:\n",
    "                        middle_1_names.append(str(name[1]).lower())\n",
    "                        middle_1_initials.append(str(name[1])[0].lower())\n",
    "                    else:\n",
    "                        middle_1_initials.append(str(name[1]).lower())\n",
    "\n",
    "                elif name[0] and name[3]:\n",
    "                    # first name\n",
    "                    if len(name[0]) > 1:\n",
    "                        first_names.append(str(name[0]).lower())\n",
    "                        first_initials.append(str(name[0])[0].lower())\n",
    "                    else:\n",
    "                        first_initials.append(str(name[0]).lower())\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "    return [list(set(first_names)), list(set(first_initials)), list(set(middle_1_names)), \n",
    "            list(set(middle_1_initials)), list(set(middle_2_names)), list(set(middle_2_initials))]\n",
    "\n",
    "####### ALL ABOVE IS DONE (except for variables and paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0245f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_block_names(block_1_names, block_1_initials, block_2_names, block_2_initials):\n",
    "    if block_1_names and block_2_names:\n",
    "        if block_1_names == block_2_names:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif block_1_names and not block_2_names:\n",
    "        if block_2_initials:\n",
    "            if block_1_initials == block_2_initials:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return True\n",
    "    elif not block_1_names and block_2_names:\n",
    "        if block_1_initials:\n",
    "            if block_1_initials == block_2_initials:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return True\n",
    "    elif block_1_initials and block_2_initials:\n",
    "        if block_1_initials == block_2_initials:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "\n",
    "####### ALL ABOVE IS DONE (except for variables and paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "506fb48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_block_vs_block(block_1_names_list, block_2_names_list):\n",
    "    \n",
    "    # check first names\n",
    "    first_check = match_block_names(block_1_names_list[0], block_1_names_list[1], block_2_names_list[0], \n",
    "                                    block_2_names_list[1])\n",
    "    \n",
    "    if first_check:\n",
    "\n",
    "        middle_1_check = match_block_names(block_1_names_list[2], block_1_names_list[3], block_2_names_list[2], \n",
    "                                           block_2_names_list[3])\n",
    "\n",
    "        if middle_1_check:\n",
    "            # check middle 2 names\n",
    "            middle_2_check = match_block_names(block_1_names_list[4], block_1_names_list[5], block_2_names_list[4], \n",
    "                                               block_2_names_list[5])\n",
    "\n",
    "            if middle_2_check:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "####### ALL ABOVE IS DONE (except for variables and paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddcacd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_cluster_coauthors(list_of_coauthors):\n",
    "    if not isinstance(list_of_coauthors, list):\n",
    "        try:\n",
    "            list_of_coauthors = list_of_coauthors.tolist()\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    if isinstance(list_of_coauthors, list):\n",
    "        try:\n",
    "            list_of_coauthors = list_of_coauthors[0].tolist()\n",
    "        except:\n",
    "            return []\n",
    "        \n",
    "    if list_of_coauthors:\n",
    "        new_list_of_coauthors = [x for x in list_of_coauthors if x]\n",
    "        if new_list_of_coauthors:\n",
    "            if isinstance(new_list_of_coauthors[0], list):\n",
    "                final_list = list(set([x.lower().replace('.', '') for y in new_list_of_coauthors for x in y]))\n",
    "                final_list = [x for x in final_list if len(x) > 5]\n",
    "                return final_list\n",
    "            else:\n",
    "                final_list = list(set([x.lower().replace('.', '') for x in new_list_of_coauthors]))\n",
    "                final_list = [x for x in final_list if len(x) > 5]\n",
    "                return final_list\n",
    "        else:\n",
    "            return []\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "####### ALL ABOVE IS DONE (except for variables and paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d73b45dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orcid_id_from_set(orcid_set):\n",
    "    orcid_list = [x for x in list(orcid_set) if x!='NONE']\n",
    "    if orcid_list:\n",
    "        return orcid_list[0]\n",
    "    else:\n",
    "        return 'NONE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e22cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_row(coauthors):\n",
    "    final_score = 0\n",
    "    if isinstance(coauthors, list):\n",
    "        final_score = len(coauthors)\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e07e4763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_data_and_transformed(bucket_name, new_data_prefix, block_id_partition_file, node_id):\n",
    "    # read data from S3 (IN PROGRESS folder)\n",
    "    new_data_init, new_data_file_path = get_new_data_file(bucket_name, new_data_prefix)\n",
    "    \n",
    "    # read mapping file\n",
    "    block_id_to_partition_mapping = pd.read_parquet(block_id_partition_file)\n",
    "    \n",
    "    # keep partitions for later\n",
    "    new_data_init = new_data_init.merge(block_id_to_partition_mapping, how='left', on='block_id').copy()\n",
    "    \n",
    "    # this should only be done for the node that takes in new block_ids\n",
    "    if node_id==\"1\":\n",
    "        new_data_init['partition'] = new_data_init['partition'].fillna(501).astype('int')\n",
    "    \n",
    "    # get partition and block ids list\n",
    "    temp_df = new_data_init.groupby('partition')['block_id'].apply(set).reset_index()\n",
    "    block_partition_list = [[x,list(y)] for x,y in zip(temp_df['partition'].tolist(), \n",
    "                                                 temp_df['block_id'].tolist())]\n",
    "    \n",
    "    return new_data_init, block_partition_list, new_data_file_path\n",
    "\n",
    "####### ALL ABOVE IS DONE (except for variables and paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b992138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data_from_files(all_files, file_type='data'):\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    if file_type in ['data','new_data']:\n",
    "        for data_file_name in all_files:\n",
    "            temp_df = pd.read_parquet(data_file_name)\n",
    "            temp_df['file_date'] = data_file_name.split(\"/\")[-1][:-5]\n",
    "            df = pd.concat([df, temp_df], axis=0)\n",
    "    else:\n",
    "        for cluster_file_name in all_files:\n",
    "            temp_df = pd.read_parquet(cluster_file_name)\n",
    "            temp_df['file_date'] = cluster_file_name.split(\"/\")[-1][:-8]\n",
    "            temp_df['final_clust_num'] = temp_df.apply(lambda x: f\"{x.block_id}_{x.final_clust}\", axis=1)\n",
    "            df = pd.concat([df, temp_df], axis=0)\n",
    "        df = df.sort_values('file_date', ascending=False).drop_duplicates(subset=['block_id','data_id']).copy()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6c0ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_filenames(local_data_path, partition_id, sub_directory='001_data_files'):\n",
    "    all_files = []\n",
    "    for data_file in glob.glob(f\"{local_data_path}/partition_{partition_id}/{sub_directory}/*\"):\n",
    "        all_files.append(data_file)\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33735123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_file_and_dedup(data_files, file_type='data'):\n",
    "    # read_all data files\n",
    "    \n",
    "    if file_type == 'new_data':\n",
    "        df = get_all_data_from_files(data_files, file_type)\n",
    "        df.columns = ['block_id', 'data_id', 'orcid', 'author', 'coauthors','partition', 'count_col', 'file_date']\n",
    "    else:\n",
    "        df = get_all_data_from_files(data_files, file_type)\n",
    "        \n",
    "    \n",
    "    # deduplicate data_ids by how much data is available\n",
    "    df['row_score'] = df.apply(lambda x: score_row(x.coauthors), axis=1)\n",
    "    \n",
    "    final_df = df.sort_values(['file_date','row_score'], ascending=False) \\\n",
    "        .drop_duplicates(subset=['data_id']) \\\n",
    "        .reset_index(drop=True) \\\n",
    "        .sort_values(['block_id']).copy()\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c207049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merged_files_for_clustering(data_files, cluster_files, block_ids):\n",
    "    old_data = get_data_file_and_dedup(data_files[:-1])\n",
    "    cluster_df = get_all_data_from_files(cluster_files, file_type='cluster')\n",
    "    \n",
    "    final_df = old_data.drop('block_id', axis=1) \\\n",
    "        .merge(cluster_df, how='inner', on='data_id')\n",
    "    \n",
    "    final_df['orcid'] = final_df['orcid'].fillna(\"NONE\")\n",
    "    \n",
    "    # get the latest df and cluster data for the given block_ids\n",
    "    blocks_to_check = final_df[final_df['block_id'].isin(block_ids)].copy()\n",
    "    \n",
    "    new_data = get_data_file_and_dedup(data_files[-1:], 'new_data')\n",
    "    new_data['orcid'] = new_data['orcid'].fillna('NONE')\n",
    "    return blocks_to_check, new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3b83d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_new_data_to_clusters(old_df, to_cluster, block_id, date_str):\n",
    "    # filter by block_id\n",
    "    df = old_df[old_df['block_id']==block_id].copy()\n",
    "    new_df = to_cluster[to_cluster['block_id']==block_id].copy()\n",
    "    \n",
    "    if df.shape[0] > 0:\n",
    "    \n",
    "        # making sure there are two columns of author text for the groupby\n",
    "        df['author_full_text'] = df['author']\n",
    "        new_df['author_full_text'] = new_df['author']\n",
    "\n",
    "        # columns to keep for cluster data\n",
    "        cluster_cols = ['block_id','data_id','final_clust_num']\n",
    "\n",
    "        # get features for new data\n",
    "        new_data = new_df.groupby(['block_id','data_id']) \\\n",
    "            .agg({\"author\": list,\n",
    "                  \"orcid\": list,\n",
    "                  \"author_full_text\": create_author_name_match,\n",
    "                  \"coauthors\": compile_cluster_coauthors}) \\\n",
    "            .reset_index()\n",
    "\n",
    "        new_data['author'] = new_data['author'].apply(lambda x: x[0])\n",
    "        new_data['orcid'] = new_data['orcid'].apply(lambda x: x[0])\n",
    "\n",
    "        # group by cluster to get features to match to\n",
    "        grouped_df = df.groupby(['block_id','final_clust_num']) \\\n",
    "            .agg({\"author\": set,\n",
    "                  \"orcid\": set,\n",
    "                  \"author_full_text\": create_author_name_match,\n",
    "                  \"coauthors\": compile_cluster_coauthors, \n",
    "                  \"data_id\": list}) \\\n",
    "            .reset_index()\n",
    "\n",
    "        grouped_df['orcid'] = grouped_df['orcid'].apply(get_orcid_id_from_set)\n",
    "        grouped_df['author'] = grouped_df['author'].apply(lambda x: list(x))\n",
    "\n",
    "        # dataframe to append all data to\n",
    "        cluster_data = pd.DataFrame()\n",
    "\n",
    "        # match new data with orcid if possible\n",
    "        orcid_cluster_data = new_data \\\n",
    "            .merge(grouped_df[grouped_df['orcid']!='NONE']\\\n",
    "                   [['orcid','block_id','final_clust_num']].drop_duplicates(), \n",
    "                   how='left', on=['block_id','orcid'])\n",
    "\n",
    "        cluster_data = pd.concat([cluster_data,\n",
    "                                  orcid_cluster_data[~orcid_cluster_data['final_clust_num'].isnull()].copy()], \n",
    "                                 axis=0)\n",
    "\n",
    "        data_left_to_cluster = orcid_cluster_data[orcid_cluster_data['final_clust_num'].isnull()].copy()\n",
    "\n",
    "        if data_left_to_cluster.shape[0] > 0:\n",
    "            # do name coauthor and coauthor matching\n",
    "\n",
    "            author_match_cluster_data = author_coauthor_match(grouped_df, data_left_to_cluster)\n",
    "\n",
    "            cluster_data = pd.concat([cluster_data,\n",
    "                                      author_match_cluster_data[~author_match_cluster_data['final_clust_num']\n",
    "                                                                .isnull()].copy()], \n",
    "                                     axis=0)\n",
    "\n",
    "            data_left_to_cluster = new_data.merge(author_match_cluster_data[author_match_cluster_data['final_clust_num']\n",
    "                                                             .isnull()][['block_id','data_id']].copy(), \n",
    "                                                  how='inner', on=['block_id','data_id'])\n",
    "            if data_left_to_cluster.shape[0] > 0:\n",
    "                # check if block has single large cluster\n",
    "                lop_cluster_bool = check_for_lop_cluster(grouped_df)\n",
    "\n",
    "                if lop_cluster_bool:\n",
    "                    # if so and name matches with no ror clash, merge into cluster\n",
    "\n",
    "                    lop_cluster_data = merge_data_into_lop_cluster(grouped_df, data_left_to_cluster)\n",
    "\n",
    "                    cluster_data = pd.concat([cluster_data,\n",
    "                                              lop_cluster_data[~lop_cluster_data['final_clust_num'].isnull()].copy()], \n",
    "                                         axis=0)\n",
    "\n",
    "                    data_left_to_cluster = new_data.merge(lop_cluster_data[lop_cluster_data['final_clust_num']\n",
    "                                                             .isnull()][['block_id','data_id']].copy(), \n",
    "                                                          how='inner', on=['block_id','data_id'])\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # after all of above if there is still data, assign it to its own cluster\n",
    "        if data_left_to_cluster.shape[0] > 0:\n",
    "            single_cols = list(data_left_to_cluster.columns)\n",
    "            single_clusters = data_left_to_cluster.reset_index().copy()\n",
    "            single_clusters.columns = ['id'] + single_cols\n",
    "            single_clusters['final_clust_num'] = single_clusters.apply(lambda x: \n",
    "                                                                       f\"{x.block_id}_{x.id}_{date_str}SC\", axis=1)\n",
    "\n",
    "            final_cluster_data = pd.concat([df[df['final_clust_num'].isin(cluster_data['final_clust_num'].tolist())]\\\n",
    "                                            [cluster_cols], \n",
    "                                            cluster_data[cluster_cols], \n",
    "                                            single_clusters[cluster_cols]], axis=0)\n",
    "        else:\n",
    "            final_cluster_data = pd.concat([df[df['final_clust_num'].isin(cluster_data['final_clust_num'].tolist())]\\\n",
    "                                            [cluster_cols], \n",
    "                                            cluster_data[cluster_cols]], axis=0)\n",
    "            \n",
    "        final_cluster_data['final_clust'] = final_cluster_data.apply(lambda x: \n",
    "                                                                 x.final_clust_num[len(x.block_id)+1:], axis=1)\n",
    "    else:\n",
    "        # for when there are no clusters to match to\n",
    "        single_cols = list(new_df.columns)\n",
    "        final_cluster_data = new_df.reset_index().copy()\n",
    "        final_cluster_data.columns = ['id'] + single_cols\n",
    "        final_cluster_data['final_clust'] = final_cluster_data.apply(lambda x: f\"{x.id}_{date_str}NCD\", axis=1)\n",
    "    \n",
    "    return final_cluster_data[['block_id','data_id','final_clust']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da4101f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_lop_cluster(cluster_df):\n",
    "    num_clusts = cluster_df['final_clust_num'].nunique()\n",
    "    \n",
    "    clust_size = cluster_df['data_id'].apply(len).tolist()[0]\n",
    "    \n",
    "    if (num_clusts == 1) & (clust_size > 10):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc3583bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data_into_lop_cluster(cluster_df, new_df):\n",
    "    new_data_ids = new_df['data_id'].tolist()\n",
    "    new_author_names = new_df['author'].tolist()\n",
    "    new_author_lists = new_df['author_full_text'].tolist()\n",
    "    new_orcid_lists = new_df['orcid'].tolist()\n",
    "    \n",
    "    cluster_ids = cluster_df['final_clust_num'].tolist()\n",
    "    cluster_author_names = cluster_df['author'].tolist()\n",
    "    cluster_author_lists = cluster_df['author_full_text'].tolist()\n",
    "    cluster_orcid_lists = cluster_df['orcid'].tolist()\n",
    "    \n",
    "    new_cluster_dict = {}\n",
    "    \n",
    "    for new_data_id, new_author_name, new_author_list, new_orcid in zip(new_data_ids,\n",
    "                                                                        new_author_names, \n",
    "                                                                        new_author_lists,\n",
    "                                                                        new_orcid_lists):\n",
    "        for cluster_id, author_names, author_list, orcid in zip(cluster_ids, \n",
    "                                                                cluster_author_names,\n",
    "                                                                cluster_author_lists,\n",
    "                                                                cluster_orcid_lists):\n",
    "            if ((author_list[0] or author_list[1] or author_list[2] or author_list[3] or author_list[4] or \n",
    "                 author_list[5]) and (new_author_list[0] or new_author_list[1] or new_author_list[2] or \n",
    "                new_author_list[3] or new_author_list[4] or new_author_list[5])):\n",
    "                # use author lists and coauthors to match\n",
    "                match_check = check_for_match_with_author_list(new_author_list, author_list)\n",
    "            else:\n",
    "                match_check = check_for_match_no_author_list(new_author_name, author_names)\n",
    "                \n",
    "            if (match_check & check_orcid(orcid, new_orcid)):\n",
    "                new_cluster_dict[new_data_id] = cluster_id\n",
    "                break\n",
    "                \n",
    "    new_cluster_data = new_df[['block_id','data_id']].copy()\n",
    "    new_cluster_data['final_clust_num'] = new_cluster_data['data_id'].apply(lambda x: new_cluster_dict.get(x, np.NaN))\n",
    "    \n",
    "    return new_cluster_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae634b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_coauthor_match(cluster_df, new_df):\n",
    "    new_data_ids = new_df['data_id'].tolist()\n",
    "    new_author_names = new_df['author'].tolist()\n",
    "    new_author_lists = new_df['author_full_text'].tolist()\n",
    "    new_coauthor_lists = new_df['coauthors'].tolist()\n",
    "    new_orcid_lists = new_df['orcid'].tolist()\n",
    "    \n",
    "    cluster_ids = cluster_df['final_clust_num'].tolist()\n",
    "    cluster_author_names = cluster_df['author'].tolist()\n",
    "    cluster_author_lists = cluster_df['author_full_text'].tolist()\n",
    "    cluster_coauthor_lists = cluster_df['coauthors'].tolist()\n",
    "    cluster_orcid_lists = cluster_df['orcid'].tolist()\n",
    "    \n",
    "    new_cluster_dict = {}\n",
    "    \n",
    "    for new_data_id, new_author_name, new_author_list, new_coauthors_list, new_orcid in zip(new_data_ids,\n",
    "                                                                                            new_author_names, \n",
    "                                                                                            new_author_lists, \n",
    "                                                                                            new_coauthor_lists,\n",
    "                                                                                            new_orcid_lists):\n",
    "        for cluster_id, author_names, author_list, coauthor_list, orcid in zip(cluster_ids, \n",
    "                                                                               cluster_author_names,\n",
    "                                                                               cluster_author_lists,\n",
    "                                                                               cluster_coauthor_lists,\n",
    "                                                                               cluster_orcid_lists):\n",
    "            if ((author_list[0] or author_list[1] or author_list[2] or author_list[3] or author_list[4] or \n",
    "                 author_list[5]) and (new_author_list[0] or new_author_list[1] or new_author_list[2] or \n",
    "                new_author_list[3] or new_author_list[4] or new_author_list[5])):\n",
    "                # use author lists and coauthors to match\n",
    "                match_check = check_for_match_with_author_list(new_author_list, author_list)\n",
    "            else:\n",
    "                match_check = check_for_match_no_author_list(new_author_name, author_names)\n",
    "                \n",
    "            if (match_check & \n",
    "                any(x in new_coauthors_list for x in coauthor_list) & \n",
    "                    check_orcid(orcid, new_orcid)):\n",
    "                new_cluster_dict[new_data_id] = cluster_id\n",
    "                break\n",
    "                \n",
    "    new_cluster_data = new_df[['block_id','data_id']].copy()\n",
    "    new_cluster_data['final_clust_num'] = new_cluster_data['data_id'].apply(lambda x: new_cluster_dict.get(x, np.NaN))\n",
    "    \n",
    "    return new_cluster_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bce36ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_orcid(cluster_orcid, new_orcid):\n",
    "    if cluster_orcid == 'NONE':\n",
    "        return True\n",
    "    elif new_orcid == 'NONE':\n",
    "        return True\n",
    "    else:\n",
    "        if new_orcid == cluster_orcid:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63389239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_match_no_author_list(new_author_name, author_names):\n",
    "    if (new_author_name.lower() in [author_name.lower() for author_name in author_names]):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f036f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_match_with_author_list(new_author_list, author_list):\n",
    "    if check_block_vs_block(new_author_list, author_list):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "755bbae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_data_to_clusters(partition_id, block_ids, local_data_path, date_str):\n",
    "    \n",
    "    _logger.info(f\"----------------- PARTITION {partition_id} -----------------\")\n",
    "    \n",
    "    # get all files for partition ID (both data files and cluster files)\n",
    "    data_files = get_all_filenames(local_data_path, partition_id, sub_directory='001_data_files')\n",
    "    cluster_files = get_all_filenames(local_data_path, partition_id, sub_directory='002_cluster_files')\n",
    "    \n",
    "    if len(cluster_files) > 0:\n",
    "    \n",
    "        _logger.info(\"-------- found data and cluster files\")\n",
    "\n",
    "        final_df, new_df = get_merged_files_for_clustering(data_files, cluster_files, block_ids)\n",
    "\n",
    "        _logger.info(\"-------- loaded data\")\n",
    "\n",
    "        # send block_ids through clustering\n",
    "        final_new_clust_df = pd.DataFrame()\n",
    "        for block_id in block_ids:\n",
    "            output_df = assign_new_data_to_clusters(final_df, new_df, block_id, date_str)\n",
    "            final_new_clust_df = pd.concat([final_new_clust_df, output_df], axis=0)\n",
    "\n",
    "    else:\n",
    "        new_df = get_data_file_and_dedup(data_files, file_type='new_data')\n",
    "        \n",
    "        new_file_cols = list(new_df.columns)\n",
    "        \n",
    "        final_new_clust_df = new_df.reset_index()\n",
    "        final_new_clust_df.columns = ['row_index'] + new_file_cols\n",
    "        final_new_clust_df['final_clust'] = final_new_clust_df['row_index'].apply(lambda x: f\"{x}NB\")\n",
    "        \n",
    "    _logger.info(f\"New works: {new_df.shape[0]}\")\n",
    "    _logger.info(f\"Number of works to be updated: {final_new_clust_df.shape[0]}\")\n",
    "    _logger.info(f\"Number of new clusters: {final_new_clust_df[final_new_clust_df['final_clust'].str.contains(f'{date_str}SC')].shape[0]}\")\n",
    "    _logger.info(f\"Number of new ids: {final_new_clust_df[final_new_clust_df['final_clust'].str.contains('NB')].shape[0]}\")\n",
    "    \n",
    "    # write out clusters to the correct partition\n",
    "    final_new_clust_df \\\n",
    "    .to_parquet(f\"{local_data_path}/partition_{partition_id}/002_cluster_files/{date_str}_clusters.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30e41e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_all_new_clusters(partition_list, local_path, date_str):\n",
    "    full_cluster_df = pd.DataFrame()\n",
    "    for partition_id in partition_list:\n",
    "        temp_df = pd\\\n",
    "            .read_parquet(\n",
    "                f\"{local_path}/partition_{partition_id}/002_cluster_files/{date_str}_clusters.parquet\")\n",
    "        full_cluster_df = pd.concat([full_cluster_df, temp_df], axis=0)\n",
    "    return full_cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8fd69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file_to_s3(clusters, save_cluster_prefix, date_str):\n",
    "    clusters['cluster_id'] = clusters.apply(lambda x: f\"{x.block_id}_{x.final_clust}\", axis=1)\n",
    "    \n",
    "    grouped_data = clusters.groupby('cluster_id')['data_id'].apply(list).reset_index()\\\n",
    "        .rename(columns={\"data_id\":\"matched_papers\"}).reset_index()\n",
    "\n",
    "    grouped_data.columns = ['cluster_id','full_id','matched_papers']\n",
    "\n",
    "    to_write = grouped_data[['cluster_id','matched_papers']].explode('matched_papers').drop_duplicates().copy()\n",
    "\n",
    "    to_write['work'] = to_write['matched_papers'].apply(lambda x: x.split(\"_\")[0][1:])\n",
    "    to_write['seq_no'] = to_write['matched_papers'].apply(lambda x: x.split(\"_\")[1])\n",
    "\n",
    "    to_write[['cluster_id','work','seq_no']] \\\n",
    "    .to_csv(f\"s3://author-name-disambiguation/{save_cluster_prefix}clusters_{date_str}.csv.gz\", \n",
    "                                                    compression='gzip', header=None, index=None)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "79e4fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # variables and paths\n",
    "    node = \"1\"\n",
    "    local_data_path = \"/home/ec2-user/data\"\n",
    "    datetime_str = datetime.now().strftime(\"%Y_%m_%d_%H\")\n",
    "    date_str = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "    time_str = datetime.now().strftime(\"%H_%M\")\n",
    "    bucket_name = \"author-name-disambiguation\"\n",
    "    new_data_prefix = f\"V1/data/002_IN_PROGRESS/NODE_{node}/\"\n",
    "    save_cluster_prefix = f\"V1/data/003_COMPLETED_CLUSTERS/NODE_{node}/{date_str}/\"\n",
    "    supp_data_path = \"/home/ec2-user/data/000_supp_data/\"\n",
    "    block_id_partition_file = f\"{supp_data_path}block_id_partition_mapping.parquet\"\n",
    "    \n",
    "    # transform new data\n",
    "    _logger.info(\"Transforming data and generating block ID and partition list\")\n",
    "    transformed_data, block_part_list, new_data_file_path = new_data_and_transformed(bucket_name, \n",
    "                                                                                     new_data_prefix, \n",
    "                                                                                     block_id_partition_file,\n",
    "                                                                                     node)\n",
    "    print(transformed_data.shape)\n",
    "    \n",
    "    if transformed_data.shape[0] > 0:\n",
    "        # write data to appropriate partition folder (using mapping)\n",
    "        _logger.info(\"Writing data to each partition\")\n",
    "        for partition_id, block_ids in block_part_list:\n",
    "            _logger.info(f\"____{partition_id} - {transformed_data[transformed_data['partition']==partition_id].shape[0]} works\")\n",
    "            transformed_data[transformed_data['partition']==partition_id] \\\n",
    "            .to_parquet(\n",
    "                f\"{local_data_path}/partition_{partition_id}/001_data_files/{datetime_str}_supp_data.parquet\")\n",
    "        \n",
    "        # run all code to put all new data into existing cluster or new cluster\n",
    "        _logger.info(\"Clustering the data\")\n",
    "        for partition_id, block_ids in block_part_list:\n",
    "            _logger.info(f\"Running clustering for partition {partition_id}\")\n",
    "            _ = new_data_to_clusters(partition_id, block_ids, local_data_path, datetime_str)\n",
    "\n",
    "        # compile all new clusters from partitions into new file\n",
    "        _logger.info(\"Compiling into single file\")\n",
    "        compiled_cluster_file = compile_all_new_clusters([i[0] for i in block_part_list], \n",
    "                                                         local_data_path, datetime_str)\n",
    "\n",
    "        # write data to S3\n",
    "        _logger.info(\"Writing to S3\")\n",
    "        _ = write_file_to_s3(compiled_cluster_file, save_cluster_prefix, datetime_str)\n",
    "        \n",
    "        # move to archive\n",
    "        _logger.info(f\"Completed file: {new_data_file_path}\")\n",
    "        os.system(f\"aws s3 mv s3://{bucket_name}/{new_data_file_path} s3://author-name-disambiguation/V1/data/ZZZ_Archive/\")\n",
    "    else:\n",
    "        _logger.info(\"Empty dataframe so exiting the program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0f0fd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(651029, 7)\n",
      "move: s3://author-name-disambiguation/V1/data/002_IN_PROGRESS/NODE_1/2023_02_20_20_10_1.parquet to s3://author-name-disambiguation/V1/data/ZZZ_Archive/2023_02_20_20_10_1.parquet\n",
      "CPU times: user 5min 44s, sys: 21.3 s, total: 6min 5s\n",
      "Wall time: 6min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9250bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304eef5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a42f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
