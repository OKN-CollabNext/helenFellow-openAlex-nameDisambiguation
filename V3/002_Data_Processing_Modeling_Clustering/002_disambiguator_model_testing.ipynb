{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa1c7929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.9.0)\n",
      "Collecting numpy<3.0,>=1.25.0 (from faiss-cpu)\n",
      "  Using cached numpy-2.1.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from faiss-cpu) (23.1)\n",
      "Using cached numpy-2.1.2-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scikit-learn 1.4.1.post1 requires numpy<2.0,>=1.19.5, but you have numpy 2.1.2 which is incompatible.\n",
      "timezonefinder 6.2.0 requires numpy<2,>=1.18, but you have numpy 2.1.2 which is incompatible.\n",
      "streamlit 1.27.2 requires numpy<2,>=1.19.3, but you have numpy 2.1.2 which is incompatible.\n",
      "matplotlib 3.7.3 requires numpy<2,>=1.20, but you have numpy 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.1.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: numpy 2.1.2\n",
      "Uninstalling numpy-2.1.2:\n",
      "  Successfully uninstalled numpy-2.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting numpy==1.24.3\n",
      "  Using cached numpy-1.24.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Using cached numpy-1.24.3-cp311-cp311-macosx_11_0_arm64.whl (13.8 MB)\n",
      "Installing collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "faiss-cpu 1.9.0 requires numpy<3.0,>=1.25.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.24.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu\n",
    "# %pip install Levenshtein\n",
    "# %pip install nameparser\n",
    "%pip uninstall -y numpy\n",
    "%pip install numpy==1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c864a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pickle\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import unidecode\n",
    "import unicodedata\n",
    "import Levenshtein as lev\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nameparser import HumanName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e98cf2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nameparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617064f",
   "metadata": {},
   "source": [
    "#### Disambiguator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac38360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/Users/nithyaab/Desktop/nithyaa helen fellow/helenFellow-openAlex-nameDisambiguation/V3/002_Data_Processing_Modeling_Clustering/Disambiguator.pkl\", \"rb\") as f:\n",
    "    disambiguator_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6102adb3",
   "metadata": {},
   "source": [
    "#### Getting data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d55e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_author_name_list_from_list(name_lists):\n",
    "    if not isinstance(name_lists, list):\n",
    "        name_lists = name_lists.tolist()\n",
    "    \n",
    "    name_list_len = len(name_lists[0])\n",
    "    \n",
    "    temp_name_list = [[j[i] for j in name_lists] for i in range(name_list_len)]\n",
    "    temp_name_list_2 = [[j[0] for j in i if j] for i in temp_name_list]\n",
    "    \n",
    "    return [list(set(x)) for x in temp_name_list_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f926942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_block_vs_block(block_1_names_list, block_2_names_list):\n",
    "    \n",
    "    # check first names\n",
    "    first_check, _ = match_block_names(block_1_names_list[0], block_1_names_list[1], block_2_names_list[0], \n",
    "                                    block_2_names_list[1])\n",
    "    # print(f\"FIRST {first_check}\")\n",
    "    \n",
    "    if first_check:\n",
    "        last_check, _ = match_block_names(block_1_names_list[-2], block_1_names_list[-1], block_2_names_list[-2], \n",
    "                                           block_2_names_list[-1])\n",
    "        # print(f\"LAST {last_check}\")\n",
    "        if last_check:\n",
    "            m1_check, more_to_go = match_block_names(block_1_names_list[2], block_1_names_list[3], block_2_names_list[2], \n",
    "                                           block_2_names_list[3])\n",
    "            if m1_check:\n",
    "                if not more_to_go:\n",
    "                    return 1\n",
    "                m2_check, more_to_go = match_block_names(block_1_names_list[4], block_1_names_list[5], block_2_names_list[4], \n",
    "                                                block_2_names_list[5])\n",
    "                \n",
    "                if m2_check:\n",
    "                    if not more_to_go:\n",
    "                        return 1\n",
    "                    m3_check, more_to_go = match_block_names(block_1_names_list[6], block_1_names_list[7], block_2_names_list[6], \n",
    "                                                block_2_names_list[7])\n",
    "                    if m3_check:\n",
    "                        if not more_to_go:\n",
    "                            return 1\n",
    "                        m4_check, more_to_go = match_block_names(block_1_names_list[8], block_1_names_list[8], block_2_names_list[8], \n",
    "                                                block_2_names_list[9])\n",
    "                        if m4_check:\n",
    "                            if not more_to_go:\n",
    "                                return 1\n",
    "                            m5_check, _ = match_block_names(block_1_names_list[10], block_1_names_list[11], block_2_names_list[10], \n",
    "                                                block_2_names_list[11])\n",
    "                            if m5_check:\n",
    "                                return 1\n",
    "                            else:\n",
    "                                return 0\n",
    "                        else:\n",
    "                            return 0\n",
    "                    else:\n",
    "                        return 0\n",
    "                else:\n",
    "                    return 0\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        swap_check = check_if_last_name_swapped_to_front_creates_match(block_1_names_list, block_2_names_list)\n",
    "        # print(f\"SWAP {swap_check}\")\n",
    "        if swap_check:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "        \n",
    "def get_name_from_name_list(name_list):\n",
    "    name = []\n",
    "    for i in range(0,12,2):\n",
    "        if name_list[i]:\n",
    "            name.append(name_list[i][0])\n",
    "        elif name_list[i+1]:\n",
    "            name.append(name_list[i+1][0])\n",
    "        else:\n",
    "            break\n",
    "    if name_list[-2]:\n",
    "        name.append(name_list[-2][0])\n",
    "    elif name_list[-1]:\n",
    "        name.append(name_list[-1][0])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return name\n",
    "        \n",
    "def check_if_last_name_swapped_to_front_creates_match(block_1, block_2):\n",
    "    name_1 = get_name_from_name_list(block_1)\n",
    "    if len(name_1) != 2:\n",
    "        return False\n",
    "    else:\n",
    "        name_2 = get_name_from_name_list(block_2)\n",
    "        if len(name_2)==2:\n",
    "            if \" \".join(name_1) == \" \".join(name_2[-1:] + name_2[:-1]):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "def check_if_first_name_has_wrong_letter(block_1_names, block_1_initials, block_2_names, block_2_initials):\n",
    "    if block_1_initials and block_2_initials:\n",
    "        if any(x in block_1_initials for x in block_2_initials):\n",
    "            if block_1_names and block_2_names:\n",
    "                for block_1 in block_1_names:\n",
    "                    for block_2 in block_2_names:\n",
    "                        dist = lev.distance(block_1, block_2)\n",
    "                        if dist <=1:\n",
    "                            if len(block_1) == len(block_2):\n",
    "                                if len(block_1) > 4:\n",
    "                                    return True\n",
    "                                    break\n",
    "                                else:\n",
    "                                    pass\n",
    "                            else:\n",
    "                                pass\n",
    "                        else:\n",
    "                            pass\n",
    "                return False\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def match_block_names(block_1_names, block_1_initials, block_2_names, block_2_initials):\n",
    "    if block_1_names and block_2_names:\n",
    "        if any(x in block_1_names for x in block_2_names):\n",
    "            return True, True\n",
    "        else:\n",
    "            return False, True\n",
    "    elif block_1_names and not block_2_names:\n",
    "        if block_2_initials:\n",
    "            if any(x in block_1_initials for x in block_2_initials):\n",
    "                return True, True\n",
    "            else:\n",
    "                return False, True\n",
    "        else:\n",
    "            return True, True\n",
    "    elif not block_1_names and block_2_names:\n",
    "        if block_1_initials:\n",
    "            if any(x in block_1_initials for x in block_2_initials):\n",
    "                return True, True\n",
    "            else:\n",
    "                return False, True\n",
    "        else:\n",
    "            return True, True\n",
    "    elif block_1_initials and block_2_initials:\n",
    "        if any(x in block_1_initials for x in block_2_initials):\n",
    "            return True, True\n",
    "        else:\n",
    "            return False, True\n",
    "    else:\n",
    "        return True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e180de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_row_label(work_1, work_2):\n",
    "    work_list = [work_1, work_2]\n",
    "    work_list.sort()\n",
    "    return \"|\".join(work_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd4f8478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_name_for_search(name):\n",
    "    name = unidecode.unidecode(unicodedata.normalize('NFKC', name))\n",
    "    name = name.lower().replace(\" \", \" \").replace(\".\", \" \").replace(\",\", \" \").replace(\"|\", \" \").replace(\")\", \"\").replace(\"(\", \"\")\\\n",
    "        .replace(\"-\", \"\").replace(\"&\", \"\").replace(\"$\", \"\").replace(\"#\", \"\").replace(\"@\", \"\").replace(\"%\", \"\").replace(\"0\", \"\") \\\n",
    "        .replace(\"1\", \"\").replace(\"2\", \"\").replace(\"3\", \"\").replace(\"4\", \"\").replace(\"5\", \"\").replace(\"6\", \"\").replace(\"7\", \"\") \\\n",
    "        .replace(\"8\", \"\").replace(\"9\", \"\").replace(\"*\", \"\").replace(\"^\", \"\").replace(\"{\", \"\").replace(\"}\", \"\").replace(\"+\", \"\") \\\n",
    "        .replace(\"=\", \"\").replace(\"_\", \"\").replace(\"~\", \"\").replace(\"`\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\\\\", \"\") \\\n",
    "        .replace(\"<\", \"\").replace(\">\", \"\").replace(\"?\", \"\").replace(\"/\", \"\").replace(\";\", \"\").replace(\":\", \"\").replace(\"\\'\", \"\") \\\n",
    "        .replace(\"\\\"\", \"\")\n",
    "    name = \" \".join(name.split())\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b92634de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_name_for_initials(author_name):\n",
    "    author_name = transform_name_for_search(author_name)\n",
    "    author_split = author_name.split(\" \")\n",
    "    if len(author_split[0]) == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d722b47a",
   "metadata": {},
   "source": [
    "#### Creating separate files for pairs that match ORCIDs vs pairs that don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b5ae5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/nithyaab/Desktop/nithyaa helen fellow/helenFellow-openAlex-nameDisambiguation/V3/002_Data_Processing_Modeling_Clustering/Disambiguator_final_test_data.parquet', '/Users/nithyaab/Desktop/nithyaa helen fellow/helenFellow-openAlex-nameDisambiguation/V3/002_Data_Processing_Modeling_Clustering/Disambiguator_final_val_data.parquet', '/Users/nithyaab/Desktop/nithyaa helen fellow/helenFellow-openAlex-nameDisambiguation/V3/002_Data_Processing_Modeling_Clustering/Disambiguator_final_train_data.parquet']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = '/Users/nithyaab/Desktop/nithyaa helen fellow/helenFellow-openAlex-nameDisambiguation/V3/002_Data_Processing_Modeling_Clustering'  # Update this to your actual path\n",
    "\n",
    "all_keys = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.parquet'):\n",
    "        all_keys.append(os.path.join(folder_path, filename))\n",
    "\n",
    "print(all_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a4c514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['inst_per','concepts_shorter_per', 'coauthors_shorter_per','exact_match_len','exact_match_spaces','citation_per','citation_work_match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "474be4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in raw_data: ['sample_type', 'author_1', 'author_2', 'inst_match', 'inst_sum', 'concepts_shortest_match', 'concepts_shortest_sum', 'concepts_shorter_match', 'concepts_shorter_sum', 'concepts_match', 'concepts_sum', 'coauthors_shorter_match', 'coauthors_shorter_sum', 'coauthors_match', 'coauthors_sum', 'citation_match', 'citation_sum', 'citation_work_match', 'author_1_name_list', 'author_2_name_list', 'author_name_check', 'exact_match', 'name_1_len', 'name_1_spaces', 'exact_match_len', 'exact_match_spaces', 'inst_per', 'concepts_per', 'concepts_shorter_per', 'concepts_shortest_per', 'coauthors_per', 'coauthors_shorter_per', 'citation_per', 'label']\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'orcid_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'orcid_1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:20\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'orcid_1'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.makedirs(\"./test_name_preds/\", exist_ok=True)\n",
    "for i, parquet_file in enumerate(all_keys):\n",
    "    print(\"Columns in raw_data:\", raw_data.columns.tolist())\n",
    "    print(i)\n",
    "    raw_data = pd.read_parquet(f\"{parquet_file}\").fillna(0.0)\n",
    "\n",
    "    raw_data['exact_match'] = raw_data.apply(lambda x: 1 if x.author_1 == x.author_2 else 0, axis=1)\n",
    "    raw_data['name_1_len'] = raw_data['author_1'].apply(len)\n",
    "    raw_data['name_1_spaces'] = raw_data['author_1'].apply(lambda x: len(x.split(\" \")))\n",
    "    raw_data['exact_match_len'] = raw_data['exact_match'] * raw_data['name_1_len']\n",
    "    raw_data['exact_match_spaces'] = raw_data['exact_match'] * raw_data['name_1_spaces']\n",
    "    raw_data['inst_per'] = raw_data['inst_per'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    #raw_data['row_label'] = raw_data.apply(lambda x: create_row_label(x.work_id_1, x.work_id_2), axis=1)\n",
    "\n",
    "    final_data_to_pred = raw_data.copy()\n",
    "\n",
    "    # Filter for ORCID matches and non-matches\n",
    "    orcid_matches = final_data_to_pred[((final_data_to_pred['orcid_1'] == final_data_to_pred['orcid_2']) & \n",
    "                                         (final_data_to_pred['orcid_1'] != ''))].copy()\n",
    "\n",
    "    final_data_to_pred = final_data_to_pred[~((final_data_to_pred['orcid_1'] == final_data_to_pred['orcid_2']) & \n",
    "                                         (final_data_to_pred['orcid_1'] != ''))].copy()\n",
    "\n",
    "    orcid_non_matches = final_data_to_pred[((final_data_to_pred['orcid_1'] != '') & \n",
    "                                             (final_data_to_pred['orcid_2'] != ''))].copy()\n",
    "\n",
    "    final_data_to_pred = final_data_to_pred[~((final_data_to_pred['orcid_1'] != '') & \n",
    "                                             (final_data_to_pred['orcid_2'] != ''))].copy()\n",
    "\n",
    "    all_to_pred = final_data_to_pred[predictors].copy()\n",
    "\n",
    "    # Make predictions\n",
    "    probs = disambiguator_model.predict_proba(np.asarray(all_to_pred))[:, 1]\n",
    "\n",
    "    final_data_to_pred['pred_score'] = probs\n",
    "    \n",
    "    # Save results to parquet files\n",
    "    orcid_matches.to_parquet(f\"./test_name_preds/{i}_orcid.parquet\")\n",
    "    final_data_to_pred[final_data_to_pred['pred_score'] > 0.2].to_parquet(f\"./test_name_preds/{i}_model.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd6069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "my_bucket = \"\"\n",
    "for my_bucket_object in my_bucket.objects.filter(Prefix='V3/final_model_data/names_to_check/work_id_info_to_join/').all():\n",
    "    if my_bucket_object.key.endswith('parquet'):\n",
    "        raw_data_file_key = my_bucket_object.key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202525c6",
   "metadata": {},
   "source": [
    "#### Inital testing of clustering method based on pairs that are scored by model\n",
    "These functions and methods are eventually developed and remade in a Pyspark notebook (in the initial clustering file) but initial testing was done using the code below to validate the method being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d6a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scored_and_orcid_data():\n",
    "    old_pairs_df = pd.DataFrame()\n",
    "    old_orcid_pairs = pd.DataFrame()\n",
    "    for i in range(10):\n",
    "        orcid_temp = pd.read_parquet(f\"./test_name_preds/{i}_orcid.parquet\", \n",
    "                                     columns=['row_label','work_id_1','work_id_2','orcid_1','original_author_1',\n",
    "                                              'original_author_2']).rename(columns={'row_label':'pairs'})\n",
    "        orcid_temp['score'] = 1.0\n",
    "        orcid_temp['pred_type'] = 'orcid'\n",
    "        model_temp = pd.read_parquet(f\"./test_name_preds/{i}_model.parquet\", \n",
    "                                     columns=['row_label', 'pred_score','work_id_1','work_id_2'])\n",
    "        model_temp['pred_type'] = 'model'\n",
    "\n",
    "        old_orcid_pairs = pd.concat([old_orcid_pairs, orcid_temp], axis=0)\n",
    "        old_pairs_df = pd.concat([old_pairs_df, model_temp], axis=0)\n",
    "        \n",
    "    return old_pairs_df, old_orcid_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5200e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_orcid(list_of_orcids):\n",
    "    if not isinstance(list_of_orcids, list):\n",
    "        list_of_orcids = list_of_orcids.tolist()\n",
    "        \n",
    "    orcids = [x for x in list_of_orcids if x]\n",
    "    \n",
    "    if orcids:\n",
    "        return orcids[0]\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446ae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_works(list_of_works):\n",
    "    if not isinstance(list_of_works, list):\n",
    "        list_of_works = list_of_works.tolist()\n",
    "        \n",
    "    works = [x for y in list_of_works for x in y]\n",
    "        \n",
    "    return list(set(works))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_latest_pairs(new_df, pairs):\n",
    "    df = new_df \\\n",
    "        [['cluster_num','work_id','name_match_list','orcid']] \\\n",
    "        .reset_index(drop=True).merge(pairs, how='left', on='cluster_num')\n",
    "    \n",
    "    df['cluster_num'] = df['cluster_num'].astype('str')\n",
    "    df['new_cluster_label'] = df.apply(lambda x: x.new_cluster_label \n",
    "                                       if isinstance(x.new_cluster_label, str) \n",
    "                                       else x.cluster_num, axis=1)\n",
    "    \n",
    "    s = df \\\n",
    "        .fillna(df['cluster_num']) \\\n",
    "        .groupby('new_cluster_label').agg({\"work_id\": get_unique_works, \n",
    "                                       \"name_match_list\": create_author_name_list_from_list, \n",
    "                                       \"orcid\": get_unique_orcid}).reset_index().drop('new_cluster_label', axis=1)\n",
    "    \n",
    "    s.columns = ['work_id','name_match_list','orcid']\n",
    "    \n",
    "    s['cluster_num'] = s.index.astype('str')\n",
    "    \n",
    "    return s[['cluster_num','work_id','name_match_list','orcid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4110c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_round_of_clustering(cluster_df, thresh=0.4):\n",
    "    pairs_df, orcid_pairs = get_scored_and_orcid_data()\n",
    "    \n",
    "    pairs_df = pairs_df[pairs_df['pred_score']>thresh] \\\n",
    "        .rename(columns={'row_label':'pairs', \n",
    "                         'work_id_1':'work_1', \n",
    "                         'work_id_2':'work_2', \n",
    "                         'pred_score':'score'}).copy()\n",
    "    \n",
    "    cluster_df, leftovers, _ = round_of_clustering(cluster_df, pairs_df)\n",
    "    \n",
    "    return cluster_df, leftovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94665ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_pairs(pairs, num_1s, num_2s):\n",
    "    taken_numbers = set()\n",
    "    result = []\n",
    "\n",
    "    for pair, num1, num2 in zip(pairs, num_1s, num_2s):\n",
    "        if num1 in taken_numbers or num2 in taken_numbers:\n",
    "            continue  # Skip this pair if either number is already taken\n",
    "        taken_numbers.add(num1)\n",
    "        taken_numbers.add(num2)\n",
    "        clust_pair = f\"{num1}|{num2}\"\n",
    "        result.append([pair, clust_pair, [num1, num2]])\n",
    "        \n",
    "    return pd.DataFrame(result, columns=['pairs','new_cluster_label','cluster_num']).explode('cluster_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ac2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_of_clustering(cluster_df, leftovers):\n",
    "    old_clust_size = cluster_df.shape[0]\n",
    "    \n",
    "    exploded_cluster_df = cluster_df.explode('work_id').copy()\n",
    "    \n",
    "    print(f\"-------Leftovers shape: {leftovers.shape}\")\n",
    "    \n",
    "    if leftovers.shape[0] == 0:\n",
    "        new_cluster_df = cluster_df.copy()\n",
    "        new_leftovers = leftovers.copy()\n",
    "        cluster_change = 0\n",
    "    else:\n",
    "\n",
    "        init_final_pairs_df = leftovers \\\n",
    "            .merge(exploded_cluster_df.rename(columns={'work_id':'work_1','orcid':'orcid_1',\n",
    "                                                  'name_match_list':'name_match_list_1', \n",
    "                                                  'cluster_num':'cluster_num_1'}).copy(), \n",
    "                   how='inner', on='work_1') \\\n",
    "            .merge(exploded_cluster_df.rename(columns={'work_id':'work_2','orcid':'orcid_2',\n",
    "                                                  'name_match_list':'name_match_list_2', \n",
    "                                                  'cluster_num':'cluster_num_2'}).copy(), \n",
    "                   how='inner', on='work_2').sort_values('score', ascending=False)\n",
    "\n",
    "        init_final_pairs_df = init_final_pairs_df[init_final_pairs_df['cluster_num_1']!=\n",
    "                                                  init_final_pairs_df['cluster_num_2']].copy()\n",
    "\n",
    "#         print(f\"%%%%%%% {init_final_pairs_df[(init_final_pairs_df['work_1']=='2059275568_1') | (init_final_pairs_df['work_2']=='2059275568_1')].shape}\")\n",
    "\n",
    "        init_final_pairs_df = init_final_pairs_df[((init_final_pairs_df['orcid_1']!='') &\n",
    "                                                   (init_final_pairs_df['orcid_2']==init_final_pairs_df['orcid_1'])) | \n",
    "                                                  ((init_final_pairs_df['orcid_1']!='') & \n",
    "                                                   (init_final_pairs_df['orcid_2']=='')) | \n",
    "                                                  ((init_final_pairs_df['orcid_1']=='') & \n",
    "                                                   (init_final_pairs_df['orcid_2']!='')) | \n",
    "                                                  ((init_final_pairs_df['orcid_1']=='') & \n",
    "                                                   (init_final_pairs_df['orcid_2']==''))].copy()\n",
    "\n",
    "#         print(f\"%%%%%%% {init_final_pairs_df[(init_final_pairs_df['work_1']=='2059275568_1') | (init_final_pairs_df['work_2']=='2059275568_1')].shape}\")\n",
    "\n",
    "        init_final_pairs_df['name_check'] = init_final_pairs_df.apply(lambda x: \n",
    "                                                                      check_block_vs_block(x.name_match_list_1,\n",
    "                                                                                           x.name_match_list_2), \n",
    "                                                                      axis=1)\n",
    "\n",
    "        init_final_pairs_df = init_final_pairs_df[init_final_pairs_df['name_check']==1] \\\n",
    "            .sort_values('score', ascending=False).copy()\n",
    "\n",
    "#         print(f\"%%%%%%% {init_final_pairs_df[(init_final_pairs_df['work_1']=='2059275568_1') | (init_final_pairs_df['work_2']=='2059275568_1')].shape}\")\n",
    "\n",
    "        final_cluster_pairs_df = get_unique_pairs(init_final_pairs_df['pairs'].tolist()[:800000], \n",
    "                                                  init_final_pairs_df['cluster_num_1'].tolist()[:800000],\n",
    "                                                  init_final_pairs_df['cluster_num_2'].tolist()[:800000])\n",
    "        print(f\"-------Number of pairs shape: {final_cluster_pairs_df.shape[0]}\")\n",
    "\n",
    "        new_cluster_df = group_latest_pairs(cluster_df, final_cluster_pairs_df[['new_cluster_label', 'cluster_num']])\n",
    "\n",
    "        new_leftovers = init_final_pairs_df.merge(final_cluster_pairs_df[['pairs','cluster_num']] \\\n",
    "                                                       .drop_duplicates(subset=['pairs']), \n",
    "                                                       how='left', on='pairs')\n",
    "        new_leftovers = new_leftovers[new_leftovers['cluster_num'].isnull()] \\\n",
    "            [['pairs','score','pred_type','work_1','work_2']].drop_duplicates(subset=['pairs']).copy()\n",
    "\n",
    "#         print(f\"%%%%%%% {new_leftovers[(new_leftovers['work_1']=='2059275568_1') | (new_leftovers['work_2']=='2059275568_1')].shape}\")\n",
    "\n",
    "        new_clust_size = new_cluster_df.shape[0]\n",
    "        cluster_change = (old_clust_size - new_clust_size)/old_clust_size\n",
    "    return new_cluster_df, new_leftovers, cluster_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_orcid_clusters(raw_data_file):\n",
    "    raw_data = pd.read_parquet(f\"s3://author-disambiguation/{raw_data_file}\")\n",
    "    raw_data['author_ind'] = raw_data.index\n",
    "    raw_data['work_id'] = raw_data['work_id'].apply(lambda x: [x])\n",
    "    raw_data['initials_name'] = raw_data['original_author'].apply(check_name_for_initials)\n",
    "    raw_data['name_match_list'] = raw_data['name_match_list'].apply(lambda x: [i.tolist() for i in x])\n",
    "    init_orcid_matches = raw_data[raw_data['orcid']!=\"\"] \\\n",
    "        .groupby('orcid').agg({\"name_match_list\": create_author_name_list_from_list,\n",
    "                               \"work_id\": get_unique_works}).reset_index()\n",
    "\n",
    "    init_non_orcid_matches = raw_data[raw_data['orcid']==\"\"][['work_id','orcid','name_match_list']].copy()\n",
    "    \n",
    "    init_non_orcid_matches['work_id'] = init_non_orcid_matches['work_id'].apply(list)\n",
    "\n",
    "    matched_orcid_to_join = pd.concat([init_orcid_matches[['work_id','orcid','name_match_list']], \n",
    "                                       init_non_orcid_matches], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    matched_orcid_to_join['cluster_num'] = matched_orcid_to_join.index.astype('str')\n",
    "    \n",
    "    return matched_orcid_to_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d67dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(raw_data_file, thresh=0.4):\n",
    "    print(f\"Iteration Number: ORCID\")\n",
    "    cluster_df = get_init_orcid_clusters(raw_data_file)\n",
    "    _ = check_cluster_stats(cluster_df, raw_data_file)\n",
    "    print(\"\")\n",
    "    \n",
    "    print(f\"Iteration Number: INIT\")\n",
    "    cluster_df, leftovers = init_round_of_clustering(cluster_df, thresh=thresh)\n",
    "    \n",
    "    _ = check_cluster_stats(cluster_df, raw_data_file)\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "    for i in range(50):\n",
    "        print(f\"Iteration Number: {i}a\")\n",
    "        cluster_df, leftovers, cluster_change = round_of_clustering(cluster_df, leftovers)\n",
    "        _ = check_cluster_stats(cluster_df, raw_data_file)\n",
    "        \n",
    "        print(f\"-------Cluster change percentage: {round(cluster_change, 4)}\")\n",
    "        print(\"\")\n",
    "        \n",
    "        if cluster_change < 0.00000001:\n",
    "            break\n",
    "        \n",
    "    return cluster_df, leftovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec4ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_clustering(raw_data_file, cluster_df, leftovers, rounds):\n",
    "    for i in range(rounds):\n",
    "        print(f\"Iteration Number: {i}a\")\n",
    "        cluster_df, leftovers, cluster_change = round_of_clustering(cluster_df, leftovers)\n",
    "        _ = check_cluster_stats(cluster_df, raw_data_file)\n",
    "        \n",
    "        print(f\"-------Cluster change percentage: {round(cluster_change, 4)}\")\n",
    "        if cluster_change < 0.00000001:\n",
    "            break\n",
    "            \n",
    "        print(\"\")\n",
    "        \n",
    "    return cluster_df, leftovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298dfe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_cluster_for_name_and_two_metrics(cluster_df, raw_data_file, metric_1, metric_2):\n",
    "    level_0_ids = ['17744445','138885662','162324750','144133560','15744967','33923547','71924100','86803240',\n",
    "               '41008148','127313418','185592680','142362112','144024400','127413603','205649164','95457728',\n",
    "               '192562407','121332964','39432304']\n",
    "    \n",
    "    cluster_df = cluster_df[['cluster_num','work_id']].copy()\n",
    "    \n",
    "    raw_data = pd.read_parquet(f\"s3://my-bucket/{raw_data_file}\")\n",
    "    \n",
    "    raw_data['name_match_list'] = raw_data['name_match_list'].apply(lambda x: [i.tolist() for i in x])\n",
    "    raw_data['concepts'] = raw_data['concepts'].apply(lambda x: [i for i in list(set(x)) if i not in level_0_ids])\n",
    "    raw_data['coauthors'] = raw_data['coauthors'].apply(lambda x: [i for i in x if len(i) > 6])\n",
    "\n",
    "    matched_works = raw_data.merge(cluster_df.explode('work_id'), how='inner', on='work_id')\n",
    "    matched_works['author_ind'] = 1\n",
    "\n",
    "    grouped_explore = matched_works.groupby('cluster_num').agg({\"original_author\": set, \n",
    "                                                              \"name_match_list\": create_author_name_list_from_list,\n",
    "                                                              \"orcid\": set,\n",
    "                                                              \"work_id\": set, \n",
    "                                                              \"concepts\": list,\n",
    "                                                              \"author_ind\": np.ma.count, \n",
    "                                                              \"institutions\": list, \n",
    "                                                              \"coauthors\": list}).reset_index()\n",
    "\n",
    "    grouped_explore['orcid'] = grouped_explore['orcid'].apply(lambda x: [i for i in list(x) if i])\n",
    "    grouped_explore['orcid_len'] = grouped_explore['orcid'].apply(lambda x: len(x))\n",
    "    grouped_explore['orcid'] = grouped_explore['orcid'].apply(lambda x: x[0] if x else \"\")\n",
    "    grouped_explore['work_id'] = grouped_explore['work_id'].apply(list)\n",
    "    grouped_explore['original_author_list'] = grouped_explore['original_author'].apply(list)\n",
    "    grouped_explore['coauthors'] = grouped_explore['coauthors'].apply(lambda x: list(set([i for j in x for i in j])))\n",
    "    grouped_explore['concepts'] = grouped_explore['concepts'].apply(lambda x: list(set([i for j in x for i in j])))\n",
    "    grouped_explore['institutions'] = grouped_explore['institutions'].apply(lambda x: list(set([i for j in x \n",
    "                                                                                                for i in j])))\n",
    "    grouped_explore['original_author'] = grouped_explore['original_author'].apply(lambda x: \"|\".join(x))\n",
    "    \n",
    "    print(grouped_explore.shape)\n",
    "    \n",
    "    author_names = grouped_explore.explode('original_author_list') \\\n",
    "        .groupby('original_author_list')['author_ind'].count().reset_index()\n",
    "    author_names_show_twice = author_names[author_names['author_ind']>1][['original_author_list']].copy()\n",
    "    \n",
    "    grouped_explore_2 = grouped_explore.explode('original_author_list') \\\n",
    "        .merge(author_names_show_twice, how='inner', on='original_author_list') \\\n",
    "        .explode(metric_1).explode(metric_2) \\\n",
    "        .groupby(['original_author_list', metric_1, metric_2]).agg({\"cluster_num\": set, \n",
    "                                                                    \"orcid\": set})\n",
    "\n",
    "    print(grouped_explore_2.shape)\n",
    "    grouped_explore_2['cluster_num'] = grouped_explore_2['cluster_num'].apply(list)\n",
    "    grouped_explore_2['cluster_num_len'] = grouped_explore_2['cluster_num'].apply(len)\n",
    "    grouped_explore_2 = grouped_explore_2[grouped_explore_2['cluster_num_len']>1].copy()\n",
    "    \n",
    "    print(grouped_explore_2.shape)\n",
    "    \n",
    "    grouped_explore_2['orcid'] = grouped_explore_2['orcid'].apply(lambda x: [i for i in list(x) if i])\n",
    "    grouped_explore_2['orcid_len'] = grouped_explore_2['orcid'].apply(lambda x: len(x))\n",
    "    grouped_explore_2 = grouped_explore_2[grouped_explore_2['orcid_len']<2].copy()\n",
    "    \n",
    "    print(grouped_explore_2.shape)\n",
    "    \n",
    "    grouped_explore_2['cluster_label'] = grouped_explore_2['cluster_num'].apply(lambda x: \"|\".join([str(i) \n",
    "                                                                                            for i in sorted(x)]))\n",
    "    \n",
    "    latest_pairs = grouped_explore_2.sort_values('cluster_num_len').drop_duplicates('cluster_label') \\\n",
    "        [['cluster_label', 'cluster_num']].copy()\n",
    "    \n",
    "    print(latest_pairs.shape)\n",
    "    final_pairs = get_unique_clusters(latest_pairs)\n",
    "    return final_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad6dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_clusters(pair_df):\n",
    "    taken_numbers = set()\n",
    "    result = []\n",
    "\n",
    "    for label, nums in zip(pair_df['cluster_label'].tolist(), \n",
    "                           pair_df['cluster_num'].tolist()):\n",
    "        new_nums = [x for x in nums if x not in taken_numbers]\n",
    "        if len(new_nums) != len(nums):\n",
    "            continue\n",
    "        for num in nums:\n",
    "            taken_numbers.add(num)\n",
    "        result.append([label, nums])\n",
    "        \n",
    "    return pd.DataFrame(result, columns=['new_cluster_label','cluster_num']).explode('cluster_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6aa734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cluster_stats(cluster_df, raw_data_file):\n",
    "    cluster_df = cluster_df[['cluster_num','work_id']].copy()\n",
    "    \n",
    "    raw_data = pd.read_parquet(f\"s3://my-bucket/{raw_data_file}\", \n",
    "                               columns=['work_id','orcid','name_match_list'])\n",
    "    \n",
    "    raw_data['name_match_list'] = raw_data['name_match_list'].apply(lambda x: [i.tolist() for i in x])\n",
    "\n",
    "    matched_works = raw_data.merge(cluster_df.explode('work_id'), how='inner', on='work_id')\n",
    "    matched_works['author_ind'] = 1\n",
    "\n",
    "    grouped_explore = matched_works.groupby('cluster_num').agg({\"name_match_list\": create_author_name_list_from_list,\n",
    "                                                                \"orcid\": set,\n",
    "                                                                \"work_id\": set, \n",
    "                                                                \"author_ind\": np.ma.count}).reset_index()\n",
    "\n",
    "    grouped_explore['orcid'] = grouped_explore['orcid'].apply(lambda x: [i for i in list(x) if i])\n",
    "    grouped_explore['orcid_len'] = grouped_explore['orcid'].apply(lambda x: len(x))\n",
    "    grouped_explore['orcid'] = grouped_explore['orcid'].apply(lambda x: x[0] if x else \"\")\n",
    "    grouped_explore['work_id'] = grouped_explore['work_id'].apply(list)\n",
    "\n",
    "    print(f\"-------Number of clusters: {grouped_explore[grouped_explore['author_ind']>1].shape[0]}\")\n",
    "    print(f\"-------Number of single clusters: {grouped_explore[grouped_explore['author_ind']==1].shape[0]}\")\n",
    "    print(f\"-------Highest number of ORCIDs in one cluster: {grouped_explore['orcid_len'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "my_bucket = \"my-bucket\"\n",
    "for my_bucket_object in my_bucket.objects.filter(Prefix='V3/final_model_data/names_to_check/work_id_info_to_join/').all():\n",
    "    if my_bucket_object.key.endswith('parquet'):\n",
    "        raw_data_file_key = my_bucket_object.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321779b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cluster_df, leftovers = perform_clustering(raw_data_file_key, thresh=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d994cb",
   "metadata": {},
   "source": [
    "#### Some code to test the clusters that are made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70322f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cluster_df = new_cluster_df[['cluster_num','work_id']].copy()\n",
    "    \n",
    "raw_data = pd.read_parquet(f\"s3://my-bucket/{raw_data_file_key}\")\n",
    "\n",
    "raw_data['name_match_list'] = raw_data['name_match_list'].apply(lambda x: [i.tolist() for i in x])\n",
    "raw_data['concepts'] = raw_data['concepts'].apply(lambda x: list(set(x)))\n",
    "\n",
    "matched_works = raw_data.merge(new_cluster_df.explode('work_id'), how='inner', on='work_id')\n",
    "matched_works['author_ind'] = 1\n",
    "\n",
    "grouped_explore = matched_works.groupby('cluster_num').agg({\"original_author\": set, \n",
    "                                                          \"name_match_list\": create_author_name_list_from_list,\n",
    "                                                          \"orcid\": set,\n",
    "                                                          \"work_id\": set, \n",
    "                                                          \"concepts\": list,\n",
    "                                                          \"author_ind\": np.ma.count, \n",
    "                                                          \"institutions\": list, \n",
    "                                                          \"coauthors\": list}).reset_index()\n",
    "\n",
    "grouped_explore['orcid'] = grouped_explore['orcid'].apply(lambda x: [i for i in list(x) if i])\n",
    "grouped_explore['orcid_len'] = grouped_explore['orcid'].apply(lambda x: len(x))\n",
    "grouped_explore['work_id'] = grouped_explore['work_id'].apply(list)\n",
    "grouped_explore['original_author_list'] = grouped_explore['original_author'].apply(list)\n",
    "grouped_explore['coauthors'] = grouped_explore['coauthors'].apply(lambda x: list(set([i for j in x for i in j])))\n",
    "grouped_explore['concepts'] = grouped_explore['concepts'].apply(lambda x: list(set([i for j in x for i in j])))\n",
    "grouped_explore['institutions'] = grouped_explore['institutions'].apply(lambda x: list(set([i for j in x for i in j])))\n",
    "grouped_explore['original_author'] = grouped_explore['original_author'].apply(lambda x: \"|\".join(x))\n",
    "\n",
    "unmatched_works = grouped_explore[grouped_explore['author_ind']==1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c044f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f44fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
