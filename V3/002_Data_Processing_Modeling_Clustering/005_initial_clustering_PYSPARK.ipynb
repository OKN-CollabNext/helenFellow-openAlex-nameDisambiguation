{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44920190-9120-4ae8-858f-14f52b073315",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import boto3\n",
    "import unicodedata\n",
    "import unidecode\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from statistics import mode\n",
    "from itertools import combinations\n",
    "from datetime import datetime, timedelta\n",
    "from nameparser import HumanName\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, ArrayType, DoubleType, StructType, StructField, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb2086b1-79ea-4dee-95c5-fc5b86ce8d3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba961a2-7e64-4844-8d78-c47e5f47c752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_save_path = \"<S3path>\"\n",
    "iteration_save_path = \"<S3path>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d25eb7f-7db2-4d90-aaab-a0e281e41b56",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Functions for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a85cca82-221f-42ee-aa69-d271bb612646",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(ArrayType(StringType())))\n",
    "def get_name_match_list(name):\n",
    "    name_split_1 = name.replace(\"-\", \"\").split()\n",
    "    name_split_2 = \"\"\n",
    "    if \"-\" in name:\n",
    "        name_split_2 = name.replace(\"-\", \" \").split()\n",
    "\n",
    "    fn = []\n",
    "    fni = []\n",
    "    \n",
    "    m1 = []\n",
    "    m1i = []\n",
    "    m2 = []\n",
    "    m2i = []\n",
    "    m3 = []\n",
    "    m3i = []\n",
    "    m4 = []\n",
    "    m4i = []\n",
    "    m5 = []\n",
    "    m5i = []\n",
    "\n",
    "    ln = []\n",
    "    lni = []\n",
    "    for name_split in [name_split_1, name_split_2]:\n",
    "        if len(name_split) == 0:\n",
    "            pass\n",
    "        elif len(name_split) == 1:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[0]) > 1:\n",
    "                ln.append(name_split[0])\n",
    "                lni.append(name_split[0][0])\n",
    "            else:\n",
    "                lni.append(name_split[0][0])\n",
    "            \n",
    "        elif len(name_split) == 2:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 3:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 4:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 5:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "                \n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 6:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "            \n",
    "            if len(name_split[4]) > 1:\n",
    "                m4.append(name_split[4])\n",
    "                m4i.append(name_split[4][0])\n",
    "            else:\n",
    "                m4i.append(name_split[4][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 7:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "            \n",
    "            if len(name_split[4]) > 1:\n",
    "                m4.append(name_split[4])\n",
    "                m4i.append(name_split[4][0])\n",
    "            else:\n",
    "                m4i.append(name_split[4][0])\n",
    "\n",
    "            if len(name_split[5]) > 1:\n",
    "                m5.append(name_split[5])\n",
    "                m5i.append(name_split[5][0])\n",
    "            else:\n",
    "                m5i.append(name_split[5][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        else:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "                \n",
    "            if len(name_split[4]) > 1:\n",
    "                m4.append(name_split[4])\n",
    "                m4i.append(name_split[4][0])\n",
    "            else:\n",
    "                m4i.append(name_split[4][0])\n",
    "\n",
    "            joined_names = \" \".join(name_split[5:-1])\n",
    "            m5.append(joined_names)\n",
    "            m5i.append(joined_names[0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "            \n",
    "\n",
    "    return [list(set(x)) for x in [fn,fni,m1,m1i,m2,m2i,m3,m3i,m4,m4i,m5,m5i,ln,lni]]\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def check_block_vs_block(block_1_names_list, block_2_names_list):\n",
    "    \n",
    "    # check first names\n",
    "    first_check, _ = match_block_names(block_1_names_list[0], block_1_names_list[1], block_2_names_list[0], \n",
    "                                    block_2_names_list[1])\n",
    "    # print(f\"FIRST {first_check}\")\n",
    "    \n",
    "    if first_check:\n",
    "        last_check, _ = match_block_names(block_1_names_list[-2], block_1_names_list[-1], block_2_names_list[-2], \n",
    "                                           block_2_names_list[-1])\n",
    "        # print(f\"LAST {last_check}\")\n",
    "        if last_check:\n",
    "            m1_check, more_to_go = match_block_names(block_1_names_list[2], block_1_names_list[3], block_2_names_list[2], \n",
    "                                           block_2_names_list[3])\n",
    "            if m1_check:\n",
    "                if not more_to_go:\n",
    "                    return 1\n",
    "                m2_check, more_to_go = match_block_names(block_1_names_list[4], block_1_names_list[5], block_2_names_list[4], \n",
    "                                                block_2_names_list[5])\n",
    "                \n",
    "                if m2_check:\n",
    "                    if not more_to_go:\n",
    "                        return 1\n",
    "                    m3_check, more_to_go = match_block_names(block_1_names_list[6], block_1_names_list[7], block_2_names_list[6], \n",
    "                                                block_2_names_list[7])\n",
    "                    if m3_check:\n",
    "                        if not more_to_go:\n",
    "                            return 1\n",
    "                        m4_check, more_to_go = match_block_names(block_1_names_list[8], block_1_names_list[8], block_2_names_list[8], \n",
    "                                                block_2_names_list[9])\n",
    "                        if m4_check:\n",
    "                            if not more_to_go:\n",
    "                                return 1\n",
    "                            m5_check, _ = match_block_names(block_1_names_list[10], block_1_names_list[11], block_2_names_list[10], \n",
    "                                                block_2_names_list[11])\n",
    "                            if m5_check:\n",
    "                                return 1\n",
    "                            else:\n",
    "                                return 0\n",
    "                        else:\n",
    "                            return 0\n",
    "                    else:\n",
    "                        return 0\n",
    "                else:\n",
    "                    return 0\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        swap_check = check_if_last_name_swapped_to_front_creates_match(block_1_names_list, block_2_names_list)\n",
    "        # print(f\"SWAP {swap_check}\")\n",
    "        if swap_check:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "def get_name_from_name_list(name_list):\n",
    "    name = []\n",
    "    for i in range(0,12,2):\n",
    "        if name_list[i]:\n",
    "            name.append(name_list[i][0])\n",
    "        elif name_list[i+1]:\n",
    "            name.append(name_list[i+1][0])\n",
    "        else:\n",
    "            break\n",
    "    if name_list[-2]:\n",
    "        name.append(name_list[-2][0])\n",
    "    elif name_list[-1]:\n",
    "        name.append(name_list[-1][0])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return name\n",
    "        \n",
    "def check_if_last_name_swapped_to_front_creates_match(block_1, block_2):\n",
    "    name_1 = get_name_from_name_list(block_1)\n",
    "    if len(name_1) != 2:\n",
    "        return False\n",
    "    else:\n",
    "        name_2 = get_name_from_name_list(block_2)\n",
    "        if len(name_2)==2:\n",
    "            if \" \".join(name_1) == \" \".join(name_2[-1:] + name_2[:-1]):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "def check_if_first_name_has_wrong_letter(block_1_names, block_1_initials, block_2_names, block_2_initials):\n",
    "    if block_1_initials and block_2_initials:\n",
    "        if any(x in block_1_initials for x in block_2_initials):\n",
    "            if block_1_names and block_2_names:\n",
    "                for block_1 in block_1_names:\n",
    "                    for block_2 in block_2_names:\n",
    "                        dist = lev.distance(block_1, block_2)\n",
    "                        if dist <=1:\n",
    "                            if len(block_1) == len(block_2):\n",
    "                                if len(block_1) > 4:\n",
    "                                    return True\n",
    "                                    break\n",
    "                                else:\n",
    "                                    pass\n",
    "                            else:\n",
    "                                pass\n",
    "                        else:\n",
    "                            pass\n",
    "                return False\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def match_block_names(block_1_names, block_1_initials, block_2_names, block_2_initials):\n",
    "    if block_1_names and block_2_names:\n",
    "        if any(x in block_1_names for x in block_2_names):\n",
    "            return True, True\n",
    "        else:\n",
    "            return False, True\n",
    "    elif block_1_names and not block_2_names:\n",
    "        if block_2_initials:\n",
    "            if any(x in block_1_initials for x in block_2_initials):\n",
    "                return True, True\n",
    "            else:\n",
    "                return False, True\n",
    "        else:\n",
    "            return True, True\n",
    "    elif not block_1_names and block_2_names:\n",
    "        if block_1_initials:\n",
    "            if any(x in block_1_initials for x in block_2_initials):\n",
    "                return True, True\n",
    "            else:\n",
    "                return False, True\n",
    "        else:\n",
    "            return True, True\n",
    "    elif block_1_initials and block_2_initials:\n",
    "        if any(x in block_1_initials for x in block_2_initials):\n",
    "            return True, True\n",
    "        else:\n",
    "            return False, True\n",
    "    else:\n",
    "        return True, False\n",
    "    \n",
    "@udf(returnType=StringType())\n",
    "def transform_name_for_search(name):\n",
    "    name = unidecode.unidecode(unicodedata.normalize('NFKC', name))\n",
    "    name = name.lower().replace(\" \", \" \").replace(\".\", \" \").replace(\",\", \" \").replace(\"|\", \" \").replace(\")\", \"\").replace(\"(\", \"\")\\\n",
    "        .replace(\"-\", \"\").replace(\"&\", \"\").replace(\"$\", \"\").replace(\"#\", \"\").replace(\"@\", \"\").replace(\"%\", \"\").replace(\"0\", \"\") \\\n",
    "        .replace(\"1\", \"\").replace(\"2\", \"\").replace(\"3\", \"\").replace(\"4\", \"\").replace(\"5\", \"\").replace(\"6\", \"\").replace(\"7\", \"\") \\\n",
    "        .replace(\"8\", \"\").replace(\"9\", \"\").replace(\"*\", \"\").replace(\"^\", \"\").replace(\"{\", \"\").replace(\"}\", \"\").replace(\"+\", \"\") \\\n",
    "        .replace(\"=\", \"\").replace(\"_\", \"\").replace(\"~\", \"\").replace(\"`\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\\\\", \"\") \\\n",
    "        .replace(\"<\", \"\").replace(\">\", \"\").replace(\"?\", \"\").replace(\"/\", \"\").replace(\";\", \"\").replace(\":\", \"\").replace(\"\\'\", \"\") \\\n",
    "        .replace(\"\\\"\", \"\")\n",
    "    name = \" \".join(name.split())\n",
    "    return name\n",
    "\n",
    "@udf(returnType=ArrayType(ArrayType(StringType())))\n",
    "def create_author_name_list_from_list(name_lists):\n",
    "    if not isinstance(name_lists, list):\n",
    "        name_lists = name_lists.tolist()\n",
    "    \n",
    "    name_list_len = len(name_lists[0])\n",
    "    \n",
    "    temp_name_list = [[j[i] for j in name_lists] for i in range(name_list_len)]\n",
    "    temp_name_list_2 = [[j[0] for j in i if j] for i in temp_name_list]\n",
    "    \n",
    "    return [list(set(x)) for x in temp_name_list_2]\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def turn_into_list(work):\n",
    "    return [work]\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def get_unique_orcid(list_of_orcids):\n",
    "    if not isinstance(list_of_orcids, list):\n",
    "        try:\n",
    "            list_of_orcids = list_of_orcids.tolist()\n",
    "        except:\n",
    "            list_of_orcids = list(list_of_orcids)\n",
    "        \n",
    "    orcids = [x for x in list_of_orcids if x]\n",
    "    \n",
    "    if len(orcids) > 1:\n",
    "        return \"MULTIPLE\" # \"|\".join(orcids)\n",
    "    elif orcids:\n",
    "        return orcids[0]\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def get_unique_works(list_of_works):\n",
    "    if not isinstance(list_of_works, list):\n",
    "        list_of_works = list_of_works.tolist()\n",
    "        \n",
    "    works = [x for y in list_of_works for x in y]\n",
    "        \n",
    "    return list(set(works))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47f9abef-de4b-45bd-8d74-3d30740105f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Load affiliations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39921bfb-8b5e-41cf-a1c4-13f82e3b1d63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "aff_data = spark.read.parquet(f\"{iteration_save_path}all_sample_data_for_all_work_authors\") \\\n",
    "    .select(F.col('work_id'), F.col('orcid'), F.col('coauthors'), F.col('citations'), F.col('institutions'), \n",
    "            F.col('original_author'), F.col('concepts')) \\\n",
    "    .filter(F.col('original_author').isNotNull()) \\\n",
    "    .filter(F.col('original_author')!='') \\\n",
    "    .withColumn('transformed_search_name', transform_name_for_search(F.col('original_author'))) \\\n",
    "    .withColumn('name_match_list', get_name_match_list(F.col('transformed_search_name'))) \\\n",
    "\n",
    "aff_data.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba09ef1-ebe2-4c5b-8a4e-1f84b76a6388",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "aff_data.filter(F.col('orcid')!='') \\\n",
    "    .groupby('orcid').agg(F.collect_set(F.col('work_id')).alias('work_ids'), \n",
    "                              F.collect_list(F.col('name_match_list')).alias('name_match_lists')) \\\n",
    "    .withColumn('name_match_list', create_author_name_list_from_list(F.col('name_match_lists'))) \\\n",
    "    .select('orcid',F.col('work_ids').alias('work_id'), 'name_match_list') \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/init_clustering/orcid_clusters/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d50429-812b-4e57-bc94-7dbd22d8a8c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "aff_data.filter(F.col('orcid')=='') \\\n",
    "    .select('orcid','work_id','name_match_list') \\\n",
    "    .dropDuplicates(subset=['work_id']) \\\n",
    "    .withColumn('work_id', turn_into_list(F.col('work_id'))) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/init_clustering/non_orcid_clusters/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5057429-b4fb-4a07-83d3-3dee4fe804c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "orcid_clusters = spark.read.parquet(f\"{temp_save_path}/init_clustering/orcid_clusters/\")\n",
    "orcid_clusters.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "943df3c6-9179-464a-a680-dfc79f010ae1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "non_orcid_clusters = spark.read.parquet(f\"{temp_save_path}/init_clustering/non_orcid_clusters/\")\n",
    "non_orcid_clusters.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80c706ab-ca09-4e60-a370-49492df729e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w1 = Window.partitionBy('work_id').orderBy(F.col('orcid_val').desc())\n",
    "\n",
    "orcid_clusters.union(non_orcid_clusters.select(*orcid_clusters.columns)) \\\n",
    "    .withColumn('cluster_num', F.monotonically_increasing_id().cast(StringType())) \\\n",
    "    .select(F.explode('work_id').alias('work_id'), 'orcid','name_match_list','cluster_num') \\\n",
    "    .withColumn('orcid_val', F.when(F.col('orcid')=='', 0).otherwise(1)) \\\n",
    "    .withColumn('work_id_rank', F.row_number().over(w1)) \\\n",
    "    .filter(F.col('work_id_rank')==1) \\\n",
    "    .groupBy('cluster_num').agg(F.collect_list(F.col('work_id')).alias('work_id'), \n",
    "                                F.collect_set(F.col('orcid')).alias('orcid'), \n",
    "                                F.collect_list(F.col('name_match_list')).alias('name_match_list')) \\\n",
    "    .withColumn('orcid', get_unique_orcid(F.col('orcid'))) \\\n",
    "    .withColumn('name_match_list', create_author_name_list_from_list(F.col('name_match_list'))) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/init_clustering/new_clusters/round=0/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b43815-38f7-44c4-8655-6b0c6e29d5c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.read.parquet(f\"{iteration_save_path}data_scored/\") \\\n",
    "    .select(F.explode('scored_data').alias('scored_data')) \\\n",
    "    .select(F.col('scored_data').getItem(1).alias('pairs'), \n",
    "            F.col('scored_data').getItem(2).alias('score').cast(FloatType())) \\\n",
    "    .dropDuplicates(subset=['pairs']) \\\n",
    "    .filter(F.col('score')>0.50) \\\n",
    "    .select('pairs', \n",
    "            F.split(F.col('pairs'), \"\\|\")[0].alias('work_1'), \n",
    "            F.split(F.col('pairs'), \"\\|\")[1].alias('work_2'), \n",
    "            'score') \\\n",
    "    .repartition(5000) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/init_clustering/leftovers/round=0/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0d25118-e126-4da2-8ddf-588878aec511",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{iteration_save_path}data_scored/\") \\\n",
    "    .select(F.explode('scored_data').alias('scored_data')) \\\n",
    "    .select(F.col('scored_data').getItem(1).alias('pairs'), \n",
    "            F.col('scored_data').getItem(2).alias('score').cast(FloatType())) \\\n",
    "    .dropDuplicates(subset=['pairs']) \\\n",
    "    .filter(F.col('score')<=0.50) \\\n",
    "    .filter(F.col('score')>0.20) \\\n",
    "    .select('pairs', \n",
    "            F.split(F.col('pairs'), \"\\|\")[0].alias('work_1'), \n",
    "            F.split(F.col('pairs'), \"\\|\")[1].alias('work_2'), \n",
    "            'score') \\\n",
    "    .repartition(5000) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/init_clustering/leftovers/round=X/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "285acd11-fac2-4526-9af5-8f2e6473037e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4738ad2d-bc91-43ba-a4d8-58448aff339b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def group_latest_pairs(cluster_round, temp_save_path):\n",
    "    unique_pairs = spark.read.parquet(f\"{temp_save_path}/init_clustering/unique_pairs/round={cluster_round}/\") \\\n",
    "        .select('new_cluster_label', F.explode(F.split(F.col('new_cluster_label'), \"\\|\")).alias('cluster_num')) \\\n",
    "        .select('new_cluster_label', F.col('cluster_num').cast(StringType())) \\\n",
    "        .dropDuplicates()\n",
    "\n",
    "    spark.read.parquet(f\"{temp_save_path}/init_clustering/new_clusters/round={cluster_round}/\") \\\n",
    "        .select(F.col('cluster_num').cast(StringType()),'work_id','name_match_list','orcid') \\\n",
    "        .join(unique_pairs, how='left', on='cluster_num') \\\n",
    "        .withColumn('final_cluster_label', F.when(F.col('new_cluster_label').isNull(), \n",
    "                                                  F.col('cluster_num')).otherwise(F.col('new_cluster_label'))) \\\n",
    "        .groupby('final_cluster_label').agg(F.collect_list(F.col('work_id')).alias('work_id'),\n",
    "                                            F.collect_list(F.col('name_match_list')).alias('name_match_list'),\n",
    "                                            F.collect_set(F.col('orcid')).alias('orcid')) \\\n",
    "        .withColumn('final_name_match_list', create_author_name_list_from_list(F.col('name_match_list'))) \\\n",
    "        .withColumn('final_work_ids', get_unique_works(F.col('work_id'))) \\\n",
    "        .withColumn('final_orcid', get_unique_orcid(F.col('orcid'))) \\\n",
    "        .select(F.col('final_work_ids').alias('work_id'), \n",
    "                F.col('final_name_match_list').alias('name_match_list'), \n",
    "                F.col('final_orcid').alias('orcid')) \\\n",
    "        .withColumn('cluster_num', F.monotonically_increasing_id().cast(StringType())) \\\n",
    "        .select('cluster_num','work_id','name_match_list','orcid') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/new_clusters/round={cluster_round+1}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeaa9920-261f-4905-b665-d9b460bb6338",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_num(num_1, num_2):\n",
    "    taken_numbers.add(num_1)\n",
    "    taken_numbers.add(num_2)\n",
    "    label = f\"{num_1}|{num_2}\" if num_1 > num_2 else f\"{num_2}|{num_1}\"\n",
    "    return [label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "173eef8b-59ab-4d45-8b19-29282e35cd4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def good_nums(num_1, num_2):\n",
    "    new_nums = [x for x in [num_1, num_2] if x not in taken_numbers]\n",
    "    if len(new_nums) == 2:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f502059-1874-48bd-abec-6f85cfda3015",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_unique_clusters(pair_df):\n",
    "\n",
    "    result = [get_num(num_1,num_2) for num_1, num_2 in \n",
    "              zip(pair_df['cluster_num_1'].tolist(), pair_df['cluster_num_2'].tolist()) \n",
    "              if good_nums(num_1,num_2)]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85b1afc6-1ed4-4e72-a218-35ee2c5a3bd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def find_score_thresh(cluster_round, score_thresh):\n",
    "    pot_pairs = spark.read.parquet(f\"{temp_save_path}/init_clustering/temp_unique_pairs/round={cluster_round}/\")\n",
    "    curr_size = pot_pairs.count()\n",
    "    if curr_size <=810000000:\n",
    "        print(\"++++SCORE: \", score_thresh, \"   SIZE: \", curr_size)\n",
    "        return score_thresh\n",
    "    else:\n",
    "        while curr_size > 810000000:\n",
    "            if score_thresh >= 0.975:\n",
    "                score_thresh += 0.0015\n",
    "            else:\n",
    "                score_thresh += 0.005\n",
    "            curr_size = pot_pairs.filter(F.col('score')>score_thresh).count()\n",
    "        print(\"++++SCORE: \", score_thresh, \"   SIZE: \", curr_size)\n",
    "        return score_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2f2fa83-cf79-42fe-8380-8962d73f3f61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_unique_pairs(cluster_round, score_thresh):\n",
    "    w1 = Window.partitionBy('new_cluster_label').orderBy(F.col('score').desc(), F.col('pairs').desc())\n",
    "\n",
    "    spark.read.parquet(f\"{temp_save_path}/init_clustering/potential_pairs/round={cluster_round}/\") \\\n",
    "        .filter(F.col('score')>=score_thresh) \\\n",
    "        .withColumn('new_cluster_label', F.when(F.col('cluster_num_1') > F.col('cluster_num_2'), \n",
    "                                                F.concat_ws(\"|\", F.col('cluster_num_1'), F.col('cluster_num_2'))) \\\n",
    "                                                .otherwise(F.concat_ws(\"|\", F.col('cluster_num_2'), F.col('cluster_num_1')))) \\\n",
    "        .select('pairs','score', 'cluster_num_1','cluster_num_2', 'new_cluster_label') \\\n",
    "        .withColumn('score_rank', F.row_number().over(w1)) \\\n",
    "        .filter(F.col('score_rank')==1) \\\n",
    "        .select('score','pairs','new_cluster_label','cluster_num_1','cluster_num_2') \\\n",
    "        .repartition(500) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/temp_unique_pairs/round={cluster_round}/\")\n",
    "\n",
    "    score_thresh_for_pandas = find_score_thresh(cluster_round, score_thresh)\n",
    "\n",
    "    pot_pairs = spark.read.parquet(f\"{temp_save_path}/init_clustering/temp_unique_pairs/round={cluster_round}/\")\n",
    "\n",
    "    pot_pairs_df = pot_pairs \\\n",
    "        .filter(F.col('score')>score_thresh_for_pandas) \\\n",
    "        .select('score','cluster_num_1','cluster_num_2') \\\n",
    "        .toPandas().sort_values('score', ascending=False)[['cluster_num_1','cluster_num_2']]\n",
    "\n",
    "    print(f\"-------Number of possible unique pairs: {pot_pairs_df.shape[0]}\")\n",
    "\n",
    "    global taken_numbers\n",
    "    taken_numbers = set()\n",
    "    unique_pairs = get_unique_clusters(pot_pairs_df)\n",
    "\n",
    "    print(f\"-------Number of unique pairs: {len(unique_pairs)}\")\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"new_cluster_label\", StringType(), True)])\n",
    "    \n",
    "    rdd = spark.sparkContext.parallelize(unique_pairs)\n",
    "    unique_pairs_df = spark.createDataFrame(rdd, schema).repartition(400)\n",
    "\n",
    "    pot_pairs \\\n",
    "        .join(unique_pairs_df, how='inner', on='new_cluster_label') \\\n",
    "        .select('pairs','score','new_cluster_label') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/unique_pairs/round={cluster_round}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd9c782b-c8b8-4677-a7ac-a7f9daea7129",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def round_of_clustering(temp_save_path, cluster_round, score_thresh):\n",
    "\n",
    "    leftovers = spark.read.parquet(f\"{temp_save_path}/init_clustering/leftovers/round={cluster_round}/\")\n",
    "    leftovers_thresh = spark.read.parquet(f\"{temp_save_path}/init_clustering/leftovers/round=X/\") \\\n",
    "        .filter(F.col('score')>0.20) \\\n",
    "        .filter(F.col('score')<=0.35)\n",
    "    cluster_df = spark.read.parquet(f\"{temp_save_path}/init_clustering/new_clusters/round={cluster_round}/\")\n",
    "    old_clust_size = cluster_df.count()\n",
    "    \n",
    "    exploded_cluster_df1 = cluster_df.select(F.col('cluster_num').alias('cluster_num_1'),\n",
    "                                             F.explode('work_id').alias('work_1'),\n",
    "                                             F.col('name_match_list').alias('name_match_list_1'),\n",
    "                                             F.col('orcid').alias('orcid_1'))\n",
    "\n",
    "    exploded_cluster_df2 = exploded_cluster_df1.alias('exploded_cluster_df2') \\\n",
    "        .select(F.col('cluster_num_1').alias('cluster_num_2'),\n",
    "                F.col('work_1').alias('work_2'),\n",
    "                F.col('name_match_list_1').alias('name_match_list_2'),\n",
    "                F.col('orcid_1').alias('orcid_2'))\n",
    "    \n",
    "    leftovers_count = leftovers.count()\n",
    "    print(f\"-------Number of leftovers: {leftovers_count}\")\n",
    "    \n",
    "    if leftovers_count == 0:\n",
    "        new_cluster_df = cluster_df.alias('new_cluster_df')\n",
    "        new_leftovers = leftovers.alias('new_leftovers')\n",
    "        cluster_change = 0\n",
    "        return lefovers\n",
    "    else:\n",
    "                    # .union(leftovers_thresh.select(*leftovers.columns)) \\\n",
    "        leftovers \\\n",
    "            .filter(F.col('score') > score_thresh) \\\n",
    "            .join(exploded_cluster_df1, how='inner', on='work_1') \\\n",
    "            .join(exploded_cluster_df2, how='inner', on='work_2') \\\n",
    "            .filter(F.col('cluster_num_1')!=F.col('cluster_num_2')) \\\n",
    "            .filter(((F.col('orcid_1')==F.col('orcid_2'))) | \n",
    "                    ((F.col('orcid_1')!='') & (F.col('orcid_2')=='')) | \n",
    "                    ((F.col('orcid_1')=='') & (F.col('orcid_2')!=''))) \\\n",
    "            .withColumn('name_check', check_block_vs_block(F.col('name_match_list_1'), F.col('name_match_list_2'))) \\\n",
    "            .filter(F.col('name_check')==1) \\\n",
    "            .select('pairs','cluster_num_1','cluster_num_2','work_1','work_2','orcid_1','orcid_2','score') \\\n",
    "            .write.mode('overwrite') \\\n",
    "            .parquet(f\"{temp_save_path}/init_clustering/potential_pairs/round={cluster_round}/\")\n",
    "        \n",
    "        potential_pairs_count = spark.read.parquet(f\"{temp_save_path}/init_clustering/potential_pairs/round={cluster_round}/\").count()\n",
    "\n",
    "        print(f\"-------Number of potential pairs: {potential_pairs_count}\")\n",
    "\n",
    "        _ = get_unique_pairs(cluster_round, score_thresh)\n",
    "        unique_pairs_count = spark.read.parquet(f\"{temp_save_path}/init_clustering/unique_pairs/round={cluster_round}/\").count()\n",
    "\n",
    "        _ = group_latest_pairs(cluster_round, temp_save_path)\n",
    "        new_clust_size = spark.read.parquet(f\"{temp_save_path}/init_clustering/new_clusters/round={cluster_round+1}/\").count()\n",
    "        new_clust_size_dedup = spark.read \\\n",
    "            .parquet(f\"{temp_save_path}/init_clustering/new_clusters/round={cluster_round+1}/\").dropDuplicates(subset=['cluster_num']).count()\n",
    "\n",
    "        if new_clust_size != new_clust_size_dedup:\n",
    "            print(\"#&#&#&#&#&# THERE ARE DUPLICATE CLUSTER NUMS &#&#&#&#&#&#&#\")\n",
    "\n",
    "        print(f\"-------Number of clusters: {new_clust_size}\")\n",
    "            \n",
    "        used_pairs = spark.read.parquet(f\"{temp_save_path}/init_clustering/unique_pairs/\") \\\n",
    "            .dropDuplicates(subset=['pairs']).select('pairs')\n",
    "\n",
    "        spark.read.parquet(f\"{temp_save_path}/init_clustering/potential_pairs/round={cluster_round}/\") \\\n",
    "            .join(used_pairs, how=\"leftanti\", on='pairs') \\\n",
    "            .select('pairs','score','work_1','work_2') \\\n",
    "            .dropDuplicates() \\\n",
    "            .repartition(1000) \\\n",
    "            .write.mode('overwrite') \\\n",
    "            .parquet(f\"{temp_save_path}/init_clustering/leftovers/round={cluster_round+1}/\")\n",
    "\n",
    "        cluster_change = round((old_clust_size - new_clust_size)/old_clust_size, 12)\n",
    "    return cluster_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffbf23b9-17d1-473e-9048-fd58c2098a64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_cluster_stats(cluster_round):\n",
    "    cluster_df = spark.read.parquet(f\"{temp_save_path}/init_clustering/new_clusters/round={cluster_round}/\") \\\n",
    "        .select('cluster_num', F.explode('work_id').alias('work_id'))\n",
    "\n",
    "    raw_data = spark.read.parquet(f\"{iteration_save_path}all_sample_data_for_all_work_authors\") \\\n",
    "        .select(F.col('work_id'), F.col('orcid'), F.col('coauthors'), F.col('citations'), F.col('institutions'), \n",
    "                F.col('original_author'), F.col('concepts')) \\\n",
    "        .filter(F.col('original_author').isNotNull()) \\\n",
    "        .filter(F.col('original_author')!='') \\\n",
    "        .withColumn('transformed_search_name', transform_name_for_search(F.col('original_author'))) \\\n",
    "        .withColumn('name_match_list', get_name_match_list(F.col('transformed_search_name'))) \\\n",
    "        .join(cluster_df, how='left', on='work_id') \\\n",
    "        .groupby('cluster_num') \\\n",
    "        .agg(F.collect_set(F.col('orcid')).alias('orcid'), \n",
    "             F.collect_set(F.col('work_id')).alias('work_id'),\n",
    "             F.size(F.collect_set(F.col('work_id'))).alias('num_works'),\n",
    "             F.collect_set(F.col('original_author')).alias('names'), \n",
    "             F.collect_list(F.col('institutions')).alias('institutions'), \n",
    "             F.collect_list(F.col('coauthors')).alias('coauthors'),\n",
    "             F.collect_list(F.col('concepts')).alias('concepts')) \\\n",
    "        .withColumn('orcid', get_unique_orcid(F.col('orcid'))) \\\n",
    "        .withColumn('concepts', get_unique_works(F.col('concepts'))) \\\n",
    "        .withColumn('institutions', get_unique_works(F.col('institutions'))) \\\n",
    "        .withColumn('coauthors', get_unique_works(F.col('coauthors'))) \\\n",
    "        .withColumn('names_join', F.array_join(F.col('names'), \"||\"))\n",
    "\n",
    "    raw_data.cache().count()\n",
    "\n",
    "    print(f\"-------Number of clusters: {raw_data.filter(F.col('num_works')>1).count()}\")\n",
    "    print(f\"-------Number of single clusters: {raw_data.filter(F.col('num_works')==1).count()}\")\n",
    "    print(f\"-------Multiple ORCIDs in one cluster: {raw_data.filter(F.col('orcid')=='MULTIPLE').count()}\")\n",
    "    print(f\"-------Number of works in the system: {int(raw_data.agg(F.sum(F.col('num_works'))).collect()[0][0])}\")\n",
    "\n",
    "    raw_data.write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/init_clustering/cluster_exploration_df/round={cluster_round}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c2bfc69-dde8-46d9-af72-3e568f01d3b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    print(i, (datetime.now() - timedelta(hours=4)).strftime(\"%m/%d/%y %H:%M\"))\n",
    "    cluster_change = round_of_clustering(temp_save_path, i, 0.90)\n",
    "    print(\"-------\", cluster_change)\n",
    "    print(\"\")\n",
    "    if cluster_change < 0.000000001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9136b53d-43b1-43af-9255-8d831b76c8b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10, 14):\n",
    "    print(i, (datetime.now() - timedelta(hours=4)).strftime(\"%m/%d/%y %H:%M\"))\n",
    "    cluster_change = round_of_clustering(temp_save_path, i, 0.80)\n",
    "    print(\"-------\", cluster_change)\n",
    "    print(\"\")\n",
    "    if cluster_change < 0.000000001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "653c9ad7-6960-454b-8b7c-c0a77181c467",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(14,30):\n",
    "    print(i, (datetime.now() - timedelta(hours=4)).strftime(\"%m/%d/%y %H:%M\"))\n",
    "    cluster_change = round_of_clustering(temp_save_path, i, 0.70)\n",
    "    print(\"-------\", cluster_change)\n",
    "    print(\"\")\n",
    "    if cluster_change < 0.000000001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1369d52-b722-4461-970e-727fd1aae8b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(30,42):\n",
    "    print(i, (datetime.now() - timedelta(hours=4)).strftime(\"%m/%d/%y %H:%M\"))\n",
    "    cluster_change = round_of_clustering(temp_save_path, i, 0.50)\n",
    "    print(\"-------\", cluster_change)\n",
    "    print(\"\")\n",
    "    if cluster_change < 0.000000001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b3d4bbc-d480-414d-a9d9-3b278f45706e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(42,65):\n",
    "    print(i, (datetime.now() - timedelta(hours=4)).strftime(\"%m/%d/%y %H:%M\"))\n",
    "    cluster_change = round_of_clustering(temp_save_path, i, 0.40)\n",
    "    print(\"-------\", cluster_change)\n",
    "    print(\"\")\n",
    "    if cluster_change < 0.000000001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d528581f-4465-4aa2-9cae-745df8892254",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_ = check_cluster_stats(i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Clustering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1f475a6-fbaa-4ebc-a3e0-02270840f5b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=IntegerType())\n",
    "def check_name_for_initials(author_name):\n",
    "    author_split = author_name.split(\" \")\n",
    "    if len(author_split[0]) == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d77cf38-81c7-4189-ab6d-5136fb09d0ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def length_greater_than_6(x):\n",
    "    return (F.length(x) > 6)\n",
    "\n",
    "def concept_L0_removed(x):\n",
    "    return ~x.isin(['17744445','138885662','162324750','144133560','15744967','33923547','71924100','86803240','41008148','127313418','185592680','142362112','144024400','127413603','205649164','95457728','192562407','121332964','39432304'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging to take care of some single-work clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92eadc10-faac-42a6-8867-75e627048c19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def group_singles(cluster_round, temp_save_path, single_inst='INIT'):\n",
    "    single_pairs = spark.read\\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/single_clusters_{single_inst}/pairs_to_join/\") \\\n",
    "        .select('new_cluster_label', F.col('cluster_num').cast(StringType())) \\\n",
    "        .dropDuplicates()\n",
    "\n",
    "    spark.read.parquet(f\"{temp_save_path}/init_clustering/new_clusters/round={cluster_round}/\") \\\n",
    "        .select(F.col('cluster_num').cast(StringType()),'work_id','name_match_list','orcid') \\\n",
    "        .join(single_pairs, how='left', on='cluster_num') \\\n",
    "        .withColumn('final_cluster_label', F.when(F.col('new_cluster_label').isNull(), \n",
    "                                                  F.col('cluster_num')).otherwise(F.col('new_cluster_label'))) \\\n",
    "        .groupby('final_cluster_label').agg(F.collect_list(F.col('work_id')).alias('work_id'),\n",
    "                                            F.collect_list(F.col('name_match_list')).alias('name_match_list'),\n",
    "                                            F.collect_set(F.col('orcid')).alias('orcid')) \\\n",
    "        .withColumn('final_name_match_list', create_author_name_list_from_list(F.col('name_match_list'))) \\\n",
    "        .withColumn('final_work_ids', get_unique_works(F.col('work_id'))) \\\n",
    "        .withColumn('final_orcid', get_unique_orcid(F.col('orcid'))) \\\n",
    "        .select(F.col('final_work_ids').alias('work_id'), \n",
    "                F.col('final_name_match_list').alias('name_match_list'), \n",
    "                F.col('final_orcid').alias('orcid')) \\\n",
    "        .withColumn('cluster_num', F.monotonically_increasing_id().cast(StringType())) \\\n",
    "        .select('cluster_num','work_id','name_match_list','orcid') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/new_clusters/round=singles_{single_inst}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4602c945-7247-4cb7-8000-08e8690012f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_single_clusters(temp_save_path, cluster_round, single_inst='INIT'):\n",
    "    cluster_df = spark.read.parquet(f\"{temp_save_path}/init_clustering/new_clusters/round={cluster_round}/\") \\\n",
    "        .select('cluster_num', F.explode('work_id').alias('work_id'))\n",
    "\n",
    "    w1 = Window.partitionBy('author_name').orderBy(F.col('num_works').desc())\n",
    "\n",
    "    raw_data = spark.read.parquet(f\"{iteration_save_path}all_sample_data_for_all_work_authors\") \\\n",
    "        .select(F.col('work_id'), F.col('orcid'), F.col('coauthors'), F.col('citations'), F.col('institutions'), \n",
    "                F.col('original_author'), F.col('concepts')) \\\n",
    "        .filter(F.col('original_author').isNotNull()) \\\n",
    "        .filter(F.col('original_author')!='') \\\n",
    "        .withColumn('transformed_search_name', transform_name_for_search(F.col('original_author'))) \\\n",
    "        .withColumn('name_match_list', get_name_match_list(F.col('transformed_search_name'))) \\\n",
    "        .join(cluster_df, how='left', on='work_id') \\\n",
    "        .groupby('cluster_num') \\\n",
    "        .agg(F.collect_set(F.col('orcid')).alias('orcid'), \n",
    "             F.size(F.collect_set(F.col('work_id'))).alias('num_works'),\n",
    "             F.collect_set(F.col('original_author')).alias('names')) \\\n",
    "        .withColumn('orcid', get_unique_orcid(F.col('orcid'))) \\\n",
    "        .select('cluster_num','num_works','orcid',F.explode('names').alias('author_name')) \\\n",
    "        .withColumn('rank', F.row_number().over(w1))\n",
    "\n",
    "    raw_data.cache().count()\n",
    "\n",
    "    raw_data \\\n",
    "        .filter((F.col('rank')==1) & (F.col('num_works')>1)) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/single_clusters_{single_inst}/first_and_many/\")\n",
    "    \n",
    "    print(\"___________ first\")\n",
    "\n",
    "    raw_data \\\n",
    "        .select('author_name',F.col('num_works').alias('num_works_2')) \\\n",
    "        .filter(F.col('rank')==2) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/single_clusters_{single_inst}/seconds/\")\n",
    "\n",
    "    print(\"___________ second\")\n",
    "\n",
    "    raw_data \\\n",
    "        .select('author_name','cluster_num','orcid') \\\n",
    "        .filter((F.col('rank')>=2) & (F.col('num_works')==1)) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/single_clusters_{single_inst}/singles/\")\n",
    "    \n",
    "    print(\"___________ others\")\n",
    "    \n",
    "    first_and_many = spark.read \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/single_clusters_{single_inst}/first_and_many/\")\n",
    "\n",
    "    seconds = spark.read.parquet(f\"{temp_save_path}/init_clustering/single_clusters_{single_inst}/seconds/\")\n",
    "\n",
    "    singles = spark.read.parquet(f\"{temp_save_path}/init_clustering/single_clusters_{single_inst}/singles/\")\n",
    "\n",
    "    names_to_group = first_and_many \\\n",
    "        .join(seconds, how='inner', on='author_name') \\\n",
    "        .filter(F.col('num_works') > F.col('num_works_2')) \\\n",
    "        .select(F.col('cluster_num').alias('new_cluster_label'), F.col('orcid').alias('orcid_new'), 'author_name') \\\n",
    "        .join(singles, how='inner', on='author_name') \\\n",
    "        .withColumn('transformed_search_name', transform_name_for_search(F.col('author_name'))) \\\n",
    "        .withColumn('initialed_name', check_name_for_initials(F.col('transformed_search_name'))) \\\n",
    "        .filter(F.col('initialed_name')==0) \\\n",
    "        .filter((F.col('orcid')==F.col('orcid_new')) | (F.col('orcid')=='')) \\\n",
    "        .select('new_cluster_label','cluster_num') \\\n",
    "        .dropDuplicates()\n",
    "    \n",
    "    print(\"___________ names to group\")\n",
    "\n",
    "    names_to_group \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/single_clusters_{single_inst}/pairs_to_join/\")\n",
    "\n",
    "    names_to_group.select('new_cluster_label', F.col('new_cluster_label').alias('cluster_num')) \\\n",
    "        .write.mode('append') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/single_clusters_{single_inst}/pairs_to_join/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44d5628e-f264-46e8-9658-1bbe14800987",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "i = 65\n",
    "print(i, (datetime.now() - timedelta(hours=4)).strftime(\"%m/%d/%y %H:%M\"))\n",
    "_ = merge_single_clusters(temp_save_path, i, 'INIT')\n",
    "_ = group_singles(i, temp_save_path, 'INIT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bbaddfa-75bd-460b-9ed1-041bee924258",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_ = check_cluster_stats(\"singles_INIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd26b47a-c00f-4d7b-a9df-60ea5d76c7f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(StringType()))\n",
    "def get_unique_orcid_array(list_of_orcids):\n",
    "    if not isinstance(list_of_orcids, list):\n",
    "        try:\n",
    "            list_of_orcids = list_of_orcids.tolist()\n",
    "        except:\n",
    "            list_of_orcids = list(list_of_orcids)\n",
    "        \n",
    "    orcids = [x for x in list_of_orcids if x]\n",
    "\n",
    "    return orcids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25afb0e1-efb9-4f5c-a229-827d2634f8c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def good_nums_merge(nums):\n",
    "    new_nums = [x for x in nums if x not in taken_numbers]\n",
    "    if len(new_nums) == len(nums):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_num_merge(nums):\n",
    "    _ = [taken_numbers.add(num) for num in nums]\n",
    "    label = \"|\".join(sorted(nums))\n",
    "    return [label]\n",
    "\n",
    "def get_unique_clusters_merge(pair_df):\n",
    "\n",
    "    result = [get_num_merge(nums) for nums in \n",
    "              pair_df['cluster_num'].tolist() \n",
    "              if good_nums_merge(nums)]\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_unique_merge_pairs(cluster_round):\n",
    "    \n",
    "    pot_pairs = spark.read.parquet(f\"{temp_save_path}/init_clustering/merge_clusters_{cluster_round}/potential_pairs/\")\n",
    "\n",
    "    print(f\"-------Number of possible unique pairs: {pot_pairs.count()}\")\n",
    "\n",
    "    pot_pairs_df = pot_pairs \\\n",
    "        .toPandas().sort_values('cluster_num_len', ascending=False)[['cluster_num']]\n",
    "\n",
    "    global taken_numbers\n",
    "    taken_numbers = set()\n",
    "    unique_pairs = get_unique_clusters_merge(pot_pairs_df)\n",
    "\n",
    "    print(f\"-------Number of unique pairs: {len(unique_pairs)}\")\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"new_cluster_label\", StringType(), True)])\n",
    "    \n",
    "    rdd = spark.sparkContext.parallelize(unique_pairs)\n",
    "    unique_pairs_df = spark.createDataFrame(rdd, schema).repartition(300)\n",
    "\n",
    "    unique_pairs_df \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/merge_clusters_{cluster_round}/unique_pairs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "872038b7-76a2-488f-980e-ad83a917629f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def group_singles_ONLY(cluster_round, temp_save_path, single_inst='INIT'):\n",
    "    single_pairs = spark.read\\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/single_clusters_{single_inst}/pairs_to_join/\") \\\n",
    "        .withColumn('temp_cluster_label', F.monotonically_increasing_id().cast(StringType())) \\\n",
    "        .select(F.concat_ws(\"_\", F.lit(\"XX\"), F.col('temp_cluster_label')).alias('temp_cluster_label'), \n",
    "                F.explode(F.split(F.col('new_cluster_label'), \"\\|\")).alias('cluster_num')) \\\n",
    "        .repartition(300)\n",
    "\n",
    "    spark.read.parquet(f\"{temp_save_path}/init_clustering/new_clusters/round={cluster_round}/\") \\\n",
    "        .select(F.col('cluster_num').cast(StringType()),'work_id','name_match_list','orcid') \\\n",
    "        .join(single_pairs, how='left', on='cluster_num') \\\n",
    "        .withColumn('final_cluster_label', F.when(F.col('temp_cluster_label').isNull(), \n",
    "                                                  F.col('cluster_num')).otherwise(F.col('temp_cluster_label'))) \\\n",
    "        .groupby('final_cluster_label').agg(F.collect_list(F.col('work_id')).alias('work_id'),\n",
    "                                            F.collect_list(F.col('name_match_list')).alias('name_match_list'),\n",
    "                                            F.collect_set(F.col('orcid')).alias('orcid')) \\\n",
    "        .withColumn('final_name_match_list', create_author_name_list_from_list(F.col('name_match_list'))) \\\n",
    "        .withColumn('final_work_ids', get_unique_works(F.col('work_id'))) \\\n",
    "        .withColumn('final_orcid', get_unique_orcid(F.col('orcid'))) \\\n",
    "        .select(F.col('final_work_ids').alias('work_id'), \n",
    "                F.col('final_name_match_list').alias('name_match_list'), \n",
    "                F.col('final_orcid').alias('orcid')) \\\n",
    "        .withColumn('cluster_num', F.monotonically_increasing_id().cast(StringType())) \\\n",
    "        .select('cluster_num','work_id','name_match_list','orcid') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/new_clusters/round=singles_{single_inst}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3158a7c8-3ad1-4319-bf7e-e1c4645ae5e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_unique_single_groups(cluster_round, single_inst):\n",
    "    \n",
    "    pot_pairs = spark.read.parquet(f\"{temp_save_path}/init_clustering/single_clusters_ONLY/singles_grouped/\")\n",
    "\n",
    "    print(f\"-------Number of possible unique pairs: {pot_pairs.count()}\")\n",
    "\n",
    "    pot_pairs_df = pot_pairs \\\n",
    "        .toPandas().sort_values(['cluster_num_len'], ascending=False).sort_values('rank')[['cluster_num']]\n",
    "\n",
    "    global taken_numbers\n",
    "    taken_numbers = set()\n",
    "    unique_pairs = get_unique_clusters_merge(pot_pairs_df)\n",
    "\n",
    "    print(f\"-------Number of unique pairs: {len(unique_pairs)}\")\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"new_cluster_label\", StringType(), True)])\n",
    "    \n",
    "    rdd = spark.sparkContext.parallelize(unique_pairs)\n",
    "    unique_pairs_df = spark.createDataFrame(rdd, schema).repartition(300)\n",
    "\n",
    "    unique_pairs_df \\\n",
    "        .select('new_cluster_label')\\\n",
    "        .repartition(300) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/single_clusters_{single_inst}/pairs_to_join/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746ab68a-cb0b-4093-8f49-8801bac6c8b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_single_clusters_only(temp_save_path, cluster_round, single_inst='ONLY'):\n",
    "    cluster_df = spark.read.parquet(f\"{temp_save_path}/init_clustering/new_clusters/round={cluster_round}/\") \\\n",
    "        .select('cluster_num', F.explode('work_id').alias('work_id'))\n",
    "\n",
    "    raw_data = spark.read.parquet(f\"{iteration_save_path}all_sample_data_for_all_work_authors\") \\\n",
    "        .select(F.col('work_id'), F.col('orcid'), F.col('coauthors'), F.col('institutions'), \n",
    "                F.col('original_author'), F.col('concepts')) \\\n",
    "        .filter(F.col('original_author').isNotNull()) \\\n",
    "        .filter(F.col('original_author')!='') \\\n",
    "        .join(cluster_df, how='left', on='work_id') \\\n",
    "        .groupby('cluster_num') \\\n",
    "        .agg(F.collect_set(F.col('orcid')).alias('orcid'), \n",
    "             F.size(F.collect_set(F.col('work_id'))).alias('num_works'),\n",
    "             F.collect_set(F.col('original_author')).alias('names'),\n",
    "             F.collect_list(F.col('coauthors')).alias('coauthors'), \n",
    "             F.collect_list(F.col('institutions')).alias('institutions'), \n",
    "             F.collect_list(F.col('concepts')).alias('concepts')) \\\n",
    "        .filter(F.col('num_works')==1) \\\n",
    "        .withColumn('coauthors', get_unique_works(F.col('coauthors'))) \\\n",
    "        .withColumn('institutions', get_unique_works(F.col('institutions'))) \\\n",
    "        .withColumn('concepts', get_unique_works(F.col('concepts'))) \\\n",
    "        .withColumn('orcid', get_unique_orcid(F.col('orcid'))) \\\n",
    "        .select('cluster_num','orcid','names','coauthors','institutions','concepts') \\\n",
    "        .repartition(300) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/single_clusters_ONLY/singles/\")\n",
    "\n",
    "    print(\"___________ others\")\n",
    "\n",
    "    singles = spark.read.parquet(f\"{temp_save_path}/init_clustering/single_clusters_{single_inst}/singles/\")\n",
    "    singles.cache().count()\n",
    "\n",
    "    singles.select(F.explode('names').alias('author_name'), 'cluster_num','institutions','orcid') \\\n",
    "        .select('author_name', F.explode('institutions').alias('institution'), 'cluster_num','orcid') \\\n",
    "        .groupBy(['author_name','institution']) \\\n",
    "        .agg(F.collect_set(F.col('orcid')).alias('orcid'),\n",
    "            F.collect_set(F.col('cluster_num')).alias('cluster_num')) \\\n",
    "        .withColumn('cluster_num_len', F.size(F.col('cluster_num'))) \\\n",
    "        .filter(F.col('cluster_num_len') > 1) \\\n",
    "        .withColumn('orcid', get_unique_orcid_array(F.col('orcid'))) \\\n",
    "        .withColumn('orcid_len', F.size(F.col('orcid'))) \\\n",
    "        .filter(F.col('orcid_len')<2) \\\n",
    "        .withColumn('rank', F.lit(1)) \\\n",
    "        .select('cluster_num','cluster_num_len','rank') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/single_clusters_ONLY/singles_grouped/\")\n",
    "\n",
    "    singles.select(F.explode('names').alias('author_name'), 'cluster_num','coauthors','orcid') \\\n",
    "        .select('author_name', F.explode('coauthors').alias('coauthor'), 'cluster_num','orcid') \\\n",
    "        .groupBy(['author_name','coauthor']) \\\n",
    "        .agg(F.collect_set(F.col('orcid')).alias('orcid'),\n",
    "            F.collect_set(F.col('cluster_num')).alias('cluster_num')) \\\n",
    "        .withColumn('cluster_num_len', F.size(F.col('cluster_num'))) \\\n",
    "        .filter(F.col('cluster_num_len') > 1) \\\n",
    "        .withColumn('orcid', get_unique_orcid_array(F.col('orcid'))) \\\n",
    "        .withColumn('orcid_len', F.size(F.col('orcid'))) \\\n",
    "        .filter(F.col('orcid_len')<2) \\\n",
    "        .withColumn('rank', F.lit(2)) \\\n",
    "        .select('cluster_num','cluster_num_len','rank') \\\n",
    "        .write.mode('append') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/single_clusters_ONLY/singles_grouped/\")\n",
    "\n",
    "\n",
    "    singles.select(F.explode('names').alias('author_name'), 'cluster_num','concepts','orcid') \\\n",
    "        .select('author_name', F.explode('concepts').alias('concept'), 'cluster_num','orcid') \\\n",
    "        .groupBy(['author_name','concept']) \\\n",
    "        .agg(F.collect_set(F.col('orcid')).alias('orcid'),\n",
    "            F.collect_set(F.col('cluster_num')).alias('cluster_num')) \\\n",
    "        .withColumn('cluster_num_len', F.size(F.col('cluster_num'))) \\\n",
    "        .filter(F.col('cluster_num_len') > 1) \\\n",
    "        .withColumn('orcid', get_unique_orcid_array(F.col('orcid'))) \\\n",
    "        .withColumn('orcid_len', F.size(F.col('orcid'))) \\\n",
    "        .filter(F.col('orcid_len')<2) \\\n",
    "        .withColumn('rank', F.lit(2)) \\\n",
    "        .select('cluster_num','cluster_num_len','rank') \\\n",
    "        .write.mode('append') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/single_clusters_ONLY/singles_grouped/\")\n",
    "\n",
    "    _ = get_unique_single_groups(cluster_round, single_inst)\n",
    "\n",
    "    _ = group_singles_ONLY('singles_INIT', temp_save_path, single_inst='ONLY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad8f9a7-79a7-482d-86cc-66da3a29c8a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_ = merge_single_clusters_only(temp_save_path, 'singles_INIT', single_inst='ONLY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9996eb27-637e-409b-bec1-0ead2433581a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_ = check_cluster_stats(\"singles_ONLY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8698fcdf-8a38-4b66-b9be-be30e6d62e96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "i = \"singles_ONLY\"\n",
    "print(i, (datetime.now() - timedelta(hours=4)).strftime(\"%m/%d/%y %H:%M\"))\n",
    "_ = merge_single_clusters(temp_save_path, i, 'INIT_V2')\n",
    "_ = group_singles(i, temp_save_path, 'INIT_V2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fcaa397-6931-44f9-a61a-228d3569781b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_ = check_cluster_stats(\"singles_INIT_V2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merges based on name match and 2 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d845f21-f053-4ab2-9574-3b3d6b64ec8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(ArrayType(StringType())))\n",
    "def get_list_combos(cluster_list):\n",
    "    return list(combinations(cluster_list, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a47479b8-88a5-45fd-80e6-90fd703a2023",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def good_nums_merge(nums):\n",
    "    new_nums = [x for x in nums if x not in taken_numbers]\n",
    "    if len(new_nums) == len(nums):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_num_merge(nums):\n",
    "    _ = [taken_numbers.add(num) for num in nums]\n",
    "    label = \"|\".join(sorted(nums))\n",
    "    return [label]\n",
    "\n",
    "def get_unique_clusters_merge(pair_df):\n",
    "\n",
    "    result = [get_num_merge(nums) for nums in \n",
    "              pair_df['cluster_num'].tolist() \n",
    "              if good_nums_merge(nums)]\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_unique_merge_pairs(cluster_round):\n",
    "    \n",
    "    pot_pairs = spark.read.parquet(f\"{temp_save_path}/init_clustering/merge_clusters_{cluster_round}/potential_pairs/\")\n",
    "\n",
    "    print(f\"-------Number of possible unique pairs: {pot_pairs.count()}\")\n",
    "\n",
    "    pot_pairs_df = pot_pairs \\\n",
    "        .toPandas().sort_values('cluster_num_len', ascending=False)[['cluster_num']]\n",
    "\n",
    "    global taken_numbers\n",
    "    taken_numbers = set()\n",
    "    unique_pairs = get_unique_clusters_merge(pot_pairs_df)\n",
    "\n",
    "    print(f\"-------Number of unique pairs: {len(unique_pairs)}\")\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"new_cluster_label\", StringType(), True)])\n",
    "    \n",
    "    rdd = spark.sparkContext.parallelize(unique_pairs)\n",
    "    unique_pairs_df = spark.createDataFrame(rdd, schema).repartition(300)\n",
    "\n",
    "    unique_pairs_df \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/merge_clusters_{cluster_round}/unique_pairs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0fb2922-2d40-48ca-95f3-0d28ceb7495c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def group_merges(cluster_round, temp_save_path, prev_cluster_round):\n",
    "    if prev_cluster_round == 'singles_INIT_V2':\n",
    "        cluster_df = spark.read.parquet(f\"{temp_save_path}/init_clustering/new_clusters/round=singles_INIT_V2/\") \\\n",
    "            .select(F.col('cluster_num').cast(StringType()),'work_id','name_match_list','orcid')\n",
    "    else:\n",
    "        cluster_df = spark.read.parquet(f\"{temp_save_path}/init_clustering/new_clusters/round=merge_{prev_cluster_round}/\") \\\n",
    "            .select(F.col('cluster_num').cast(StringType()),'work_id','name_match_list','orcid')\n",
    "\n",
    "    merge_pairs = spark.read\\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/merge_clusters_{cluster_round}/unique_pairs/\") \\\n",
    "        .withColumn('temp_cluster_label', F.monotonically_increasing_id().cast(StringType())) \\\n",
    "        .select(F.concat_ws(\"_\", F.lit(\"XX\"), F.col('temp_cluster_label')).alias('temp_cluster_label'), \n",
    "                F.split(F.col('new_cluster_label'), \"\\|\").alias('cluster_num')) \\\n",
    "        .select('temp_cluster_label','cluster_num',F.col('cluster_num').getItem(0).alias('cluster_num_1'), \n",
    "                F.col('cluster_num').getItem(1).alias('cluster_num_2')) \\\n",
    "        .dropDuplicates()\n",
    "\n",
    "    cluster_df_1 = cluster_df.alias('cluster_df_1').select(F.col('cluster_num').alias('cluster_num_1'), \n",
    "                                                           F.col('name_match_list').alias('name_match_list_1'), \n",
    "                                                           F.col('orcid').alias('orcid_1'))\n",
    "    \n",
    "    cluster_df_2 = cluster_df.alias('cluster_df_2').select(F.col('cluster_num').alias('cluster_num_2'), \n",
    "                                                           F.col('name_match_list').alias('name_match_list_2'), \n",
    "                                                           F.col('orcid').alias('orcid_2'))\n",
    "    \n",
    "    final_merge_pairs = merge_pairs.join(cluster_df_1, how='inner', on='cluster_num_1') \\\n",
    "        .join(cluster_df_2, how='inner', on='cluster_num_2') \\\n",
    "        .filter((F.col('orcid_1')==F.col('orcid_2')) | (F.col('orcid_1')=='') | (F.col('orcid_2')=='')) \\\n",
    "        .withColumn('matched_names', check_block_vs_block(F.col('name_match_list_1'), F.col('name_match_list_2'))) \\\n",
    "        .filter(F.col('matched_names')==1) \\\n",
    "        .select('temp_cluster_label', F.explode(F.col('cluster_num')).alias('cluster_num'))\n",
    "\n",
    "    final_merge_pairs.cache().count()\n",
    "    \n",
    "    cluster_df \\\n",
    "        .select(F.col('cluster_num').cast(StringType()),'work_id','name_match_list','orcid') \\\n",
    "        .join(final_merge_pairs, how='left', on='cluster_num') \\\n",
    "        .withColumn('final_cluster_label', F.when(F.col('temp_cluster_label').isNull(), \n",
    "                                                  F.col('cluster_num')).otherwise(F.col('temp_cluster_label'))) \\\n",
    "        .groupby('final_cluster_label').agg(F.collect_list(F.col('work_id')).alias('work_id'),\n",
    "                                            F.collect_list(F.col('name_match_list')).alias('name_match_list'),\n",
    "                                            F.collect_set(F.col('orcid')).alias('orcid')) \\\n",
    "        .withColumn('final_name_match_list', create_author_name_list_from_list(F.col('name_match_list'))) \\\n",
    "        .withColumn('final_work_ids', get_unique_works(F.col('work_id'))) \\\n",
    "        .withColumn('final_orcid', get_unique_orcid(F.col('orcid'))) \\\n",
    "        .select(F.col('final_work_ids').alias('work_id'), \n",
    "                F.col('final_name_match_list').alias('name_match_list'), \n",
    "                F.col('final_orcid').alias('orcid')) \\\n",
    "        .withColumn('cluster_num', F.monotonically_increasing_id().cast(StringType())) \\\n",
    "        .select('cluster_num','work_id','name_match_list','orcid') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/new_clusters/round=merge_{cluster_round}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd1b71a3-2354-4819-bd00-d04ae6da2d78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def final_cluster_for_name_and_two_metrics(temp_save_path, cluster_round, prev_cluster_round, metric_1, metric_2, metric_3):\n",
    "    if prev_cluster_round == 'singles_INIT_V2':\n",
    "        cluster_df = spark.read.parquet(f\"{temp_save_path}/init_clustering/new_clusters/round=singles_INIT_V2/\") \\\n",
    "            .select('cluster_num', F.explode('work_id').alias('work_id')) \\\n",
    "            .repartition(300)\n",
    "    else:\n",
    "        cluster_df = spark.read.parquet(f\"{temp_save_path}/init_clustering/new_clusters/round=merge_{prev_cluster_round}/\") \\\n",
    "            .select('cluster_num', F.explode('work_id').alias('work_id'))\n",
    "        \n",
    "\n",
    "    raw_data = spark.read.parquet(f\"{iteration_save_path}all_sample_data_for_all_work_authors\") \\\n",
    "        .select(F.col('work_id'), F.col('orcid'), F.col('coauthors'), F.col('citations'), F.col('institutions'), \n",
    "                F.col('original_author'), F.col('concepts')) \\\n",
    "        .filter(F.col('original_author').isNotNull()) \\\n",
    "        .filter(F.col('original_author')!='') \\\n",
    "        .withColumn('transformed_search_name', transform_name_for_search(F.col('original_author'))) \\\n",
    "        .withColumn('initialed_name', check_name_for_initials(F.col('transformed_search_name'))) \\\n",
    "        .filter(F.col('initialed_name')==0) \\\n",
    "        .withColumn('concepts', F.filter(F.col('concepts'), concept_L0_removed)) \\\n",
    "        .withColumn('coauthors', F.filter(F.col('coauthors'), length_greater_than_6)) \\\n",
    "        .join(cluster_df, how='left', on='work_id') \\\n",
    "        .groupby('cluster_num') \\\n",
    "        .agg(F.collect_set(F.col('orcid')).alias('orcid'), \n",
    "             F.collect_set(F.col('transformed_search_name')).alias('original_author_list'), \n",
    "             F.collect_list(F.col(metric_1)).alias(metric_1), \n",
    "             F.collect_list(F.col(metric_2)).alias(metric_2), \n",
    "             F.collect_list(F.col(metric_3)).alias(metric_3)) \\\n",
    "        .withColumn(metric_1, get_unique_works(F.col(metric_1))) \\\n",
    "        .withColumn(metric_2, get_unique_works(F.col(metric_2))) \\\n",
    "        .withColumn(metric_3, get_unique_works(F.col(metric_3))) \\\n",
    "        .withColumn(f\"{metric_1}_len\", F.size(F.col(metric_1))) \\\n",
    "        .withColumn(f\"{metric_2}_len\", F.size(F.col(metric_2))) \\\n",
    "        .withColumn(f\"{metric_3}_len\", F.size(F.col(metric_3))) \\\n",
    "        .filter(((F.col(f\"{metric_1}_len\")>0) & (F.col(f\"{metric_2}_len\")>0)) | \n",
    "                ((F.col(f\"{metric_2}_len\")>0) & (F.col(f\"{metric_3}_len\")>0)) | \n",
    "                ((F.col(f\"{metric_3}_len\")>0) & (F.col(f\"{metric_1}_len\")>0))) \\\n",
    "        .withColumn('orcid', get_unique_orcid_array(F.col('orcid'))) \\\n",
    "        .withColumn('orcid_len', F.size(F.col('orcid'))) \\\n",
    "        .filter(F.col('orcid_len')<2) \\\n",
    "        .withColumn('orcid', get_unique_orcid(F.col('orcid'))) \\\n",
    "        .repartition(1000)\n",
    "\n",
    "    raw_data.cache().count()\n",
    "\n",
    "    print((datetime.now() - timedelta(hours=4)).strftime(\"%m/%d/%y %H:%M\"), \" raw data\")\n",
    "    raw_data \\\n",
    "        .filter(F.col(f\"{metric_1}_len\")>0) \\\n",
    "        .select(F.explode('original_author_list').alias('original_author_list'), metric_1, 'cluster_num', 'orcid')\\\n",
    "        .select(F.explode(metric_1).alias(metric_1), 'original_author_list', 'cluster_num','orcid') \\\n",
    "        .groupby(['original_author_list', metric_1]) \\\n",
    "        .agg(F.collect_set(F.col('orcid')).alias('orcid'),\n",
    "             F.collect_set(F.col('cluster_num')).alias('cluster_num')) \\\n",
    "        .withColumn('cluster_num_len', F.size(F.col('cluster_num'))) \\\n",
    "        .filter(F.col('cluster_num_len') > 1) \\\n",
    "        .filter(F.col('cluster_num_len') < 10) \\\n",
    "        .withColumn('orcid', get_unique_orcid_array(F.col('orcid'))) \\\n",
    "        .withColumn('orcid_len', F.size(F.col('orcid'))) \\\n",
    "        .filter(F.col('orcid_len')<2) \\\n",
    "        .withColumn('cluster_combos', get_list_combos(F.col('cluster_num'))) \\\n",
    "        .select(F.explode('cluster_combos').alias('cluster_num')) \\\n",
    "        .withColumn('new_cluster_label', F.array_join(F.array_sort(F.col('cluster_num')), \"|\")) \\\n",
    "        .dropDuplicates() \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/merge_clusters_{cluster_round}/metric_1/\")\n",
    "\n",
    "    print((datetime.now() - timedelta(hours=4)).strftime(\"%m/%d/%y %H:%M\"), \" metric_1\")\n",
    "    raw_data \\\n",
    "        .filter(F.col(f\"{metric_2}_len\")>0) \\\n",
    "        .select(F.explode('original_author_list').alias('original_author_list'), metric_2, 'cluster_num', 'orcid')\\\n",
    "        .select(F.explode(metric_2).alias(metric_2), 'original_author_list', 'cluster_num','orcid') \\\n",
    "        .groupby(['original_author_list', metric_2]) \\\n",
    "        .agg(F.collect_set(F.col('orcid')).alias('orcid'),\n",
    "             F.collect_set(F.col('cluster_num')).alias('cluster_num')) \\\n",
    "        .withColumn('cluster_num_len', F.size(F.col('cluster_num'))) \\\n",
    "        .filter(F.col('cluster_num_len') > 1) \\\n",
    "        .filter(F.col('cluster_num_len') < 10) \\\n",
    "        .withColumn('orcid', get_unique_orcid_array(F.col('orcid'))) \\\n",
    "        .withColumn('orcid_len', F.size(F.col('orcid'))) \\\n",
    "        .filter(F.col('orcid_len')<2) \\\n",
    "        .withColumn('cluster_combos', get_list_combos(F.col('cluster_num'))) \\\n",
    "        .select(F.explode('cluster_combos').alias('cluster_num')) \\\n",
    "        .withColumn('new_cluster_label', F.array_join(F.array_sort(F.col('cluster_num')), \"|\")) \\\n",
    "        .dropDuplicates() \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/merge_clusters_{cluster_round}/metric_2/\")\n",
    "\n",
    "    print((datetime.now() - timedelta(hours=4)).strftime(\"%m/%d/%y %H:%M\"), \" metric_2\")\n",
    "    raw_data \\\n",
    "        .filter(F.col(f\"{metric_3}_len\")>0) \\\n",
    "        .select(F.explode('original_author_list').alias('original_author_list'), metric_3, 'cluster_num', 'orcid')\\\n",
    "        .select(F.explode(metric_3).alias(metric_3), 'original_author_list', 'cluster_num','orcid') \\\n",
    "        .groupby(['original_author_list', metric_3]) \\\n",
    "        .agg(F.collect_set(F.col('orcid')).alias('orcid'),\n",
    "             F.collect_set(F.col('cluster_num')).alias('cluster_num')) \\\n",
    "        .withColumn('cluster_num_len', F.size(F.col('cluster_num'))) \\\n",
    "        .filter(F.col('cluster_num_len') > 1) \\\n",
    "        .filter(F.col('cluster_num_len') < 10) \\\n",
    "        .withColumn('orcid', get_unique_orcid_array(F.col('orcid'))) \\\n",
    "        .withColumn('orcid_len', F.size(F.col('orcid'))) \\\n",
    "        .filter(F.col('orcid_len')<2) \\\n",
    "        .withColumn('cluster_combos', get_list_combos(F.col('cluster_num'))) \\\n",
    "        .select(F.explode('cluster_combos').alias('cluster_num')) \\\n",
    "        .withColumn('new_cluster_label', F.array_join(F.array_sort(F.col('cluster_num')), \"|\")) \\\n",
    "        .dropDuplicates() \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/merge_clusters_{cluster_round}/metric_3/\")\n",
    "\n",
    "    print((datetime.now() - timedelta(hours=4)).strftime(\"%m/%d/%y %H:%M\"), \" metric_3\")\n",
    "\n",
    "    metric_1_df = spark.read.parquet(f\"{temp_save_path}/init_clustering/merge_clusters_{cluster_round}/metric_1/\")\n",
    "    metric_1_df.cache().count()\n",
    "\n",
    "    metric_2_df = spark.read.parquet(f\"{temp_save_path}/init_clustering/merge_clusters_{cluster_round}/metric_2/\")\n",
    "    metric_2_df.cache().count()\n",
    "\n",
    "    metric_3_df = spark.read.parquet(f\"{temp_save_path}/init_clustering/merge_clusters_{cluster_round}/metric_3/\")\n",
    "    metric_3_df.cache().count()\n",
    "\n",
    "    metric_1_df.join(metric_2_df.select('new_cluster_label'), how='inner', on='new_cluster_label') \\\n",
    "        .dropDuplicates() \\\n",
    "        .select('cluster_num') \\\n",
    "        .withColumn('cluster_num_len', F.lit(1)) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/merge_clusters_{cluster_round}/potential_pairs/\")\n",
    "\n",
    "    metric_1_df.join(metric_3_df.select('new_cluster_label'), how='inner', on='new_cluster_label') \\\n",
    "        .dropDuplicates() \\\n",
    "        .select('cluster_num') \\\n",
    "        .withColumn('cluster_num_len', F.lit(1)) \\\n",
    "        .write.mode('append') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/merge_clusters_{cluster_round}/potential_pairs/\")\n",
    "\n",
    "    metric_2_df.join(metric_3_df.select('new_cluster_label'), how='inner', on='new_cluster_label') \\\n",
    "        .dropDuplicates() \\\n",
    "        .select('cluster_num') \\\n",
    "        .withColumn('cluster_num_len', F.lit(1)) \\\n",
    "        .write.mode('append') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/merge_clusters_{cluster_round}/potential_pairs/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85074ead-2e4a-4b69-ad1c-426925089509",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    print((datetime.now() - timedelta(hours=4)).strftime(\"%m/%d/%y %H:%M\"), f\"Iteration Number: METRICS MERGE - {i}\")\n",
    "    # # Getting final merges from features\n",
    "    _ = final_cluster_for_name_and_two_metrics(temp_save_path, i, i-1, 'coauthors', 'institutions', 'concepts')\n",
    "\n",
    "    _ = get_unique_merge_pairs(i)\n",
    "\n",
    "    pairs_shape = spark.read.parquet(f\"{temp_save_path}/init_clustering/merge_clusters_{i}/unique_pairs/\").count()\n",
    "    print(f\"-------Pairs shape: {pairs_shape}\")\n",
    "\n",
    "    if pairs_shape > 0:\n",
    "        _ = group_merges(i, temp_save_path, i-1)\n",
    "    else:\n",
    "        print(\"NO MERGES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18bfe7ab-c8f5-46f1-ac75-80864d827df6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_ = check_cluster_stats(\"merge_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fb25874-7dfa-430d-a6ac-a2ce49cfb992",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_unique_pairs_manual(pot_pairs_prefix):\n",
    "    pot_pairs = spark.read.parquet(f\"{temp_save_path}/{pot_pairs_prefix}\") \\\n",
    "        .withColumn('orcid_uni', get_unique_orcid_array(F.col('orcid'))) \\\n",
    "        .withColumn('orcid_len', F.size(F.col('orcid_uni'))) \\\n",
    "        .filter(F.col('orcid_len')<2)\n",
    "\n",
    "    print(f\"-------Number of possible unique pairs: {pot_pairs.count()}\")\n",
    "\n",
    "    pot_pairs_df = pot_pairs \\\n",
    "        .toPandas()[['cluster_num']]\n",
    "\n",
    "    global taken_numbers\n",
    "    taken_numbers = set()\n",
    "    unique_pairs = get_unique_clusters_merge(pot_pairs_df)\n",
    "\n",
    "    print(f\"-------Number of unique pairs: {len(unique_pairs)}\")\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"new_cluster_label\", StringType(), True)])\n",
    "    \n",
    "    rdd = spark.sparkContext.parallelize(unique_pairs)\n",
    "    unique_pairs_df = spark.createDataFrame(rdd, schema).repartition(300)\n",
    "\n",
    "    unique_pairs_df \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/{pot_pairs_prefix}unique_pairs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0970e95-3ac8-40d1-81ea-c23fdefdf649",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def group_manual(groups_manual_path, cluster_round, new_cluster_round):\n",
    "    single_pairs = spark.read\\\n",
    "        .parquet(f\"{temp_save_path}/{groups_manual_path}unique_pairs/\") \\\n",
    "        .withColumn('temp_cluster_label', F.monotonically_increasing_id().cast(StringType())) \\\n",
    "        .select(F.concat_ws(\"_\", F.lit(\"XX\"), F.col('temp_cluster_label')).alias('temp_cluster_label'), \n",
    "                F.explode(F.split(F.col('new_cluster_label'), \"\\|\")).alias('cluster_num')) \\\n",
    "        .repartition(300)\n",
    "\n",
    "    spark.read.parquet(f\"{temp_save_path}/init_clustering/new_clusters/round={cluster_round}/\") \\\n",
    "        .select(F.col('cluster_num').cast(StringType()),'work_id','name_match_list','orcid') \\\n",
    "        .join(single_pairs, how='left', on='cluster_num') \\\n",
    "        .withColumn('final_cluster_label', F.when(F.col('temp_cluster_label').isNull(), \n",
    "                                                  F.col('cluster_num')).otherwise(F.col('temp_cluster_label'))) \\\n",
    "        .groupby('final_cluster_label').agg(F.collect_list(F.col('work_id')).alias('work_id'),\n",
    "                                            F.collect_list(F.col('name_match_list')).alias('name_match_list'),\n",
    "                                            F.collect_set(F.col('orcid')).alias('orcid')) \\\n",
    "        .withColumn('final_name_match_list', create_author_name_list_from_list(F.col('name_match_list'))) \\\n",
    "        .withColumn('final_work_ids', get_unique_works(F.col('work_id'))) \\\n",
    "        .withColumn('final_orcid', get_unique_orcid(F.col('orcid'))) \\\n",
    "        .select(F.col('final_work_ids').alias('work_id'), \n",
    "                F.col('final_name_match_list').alias('name_match_list'), \n",
    "                F.col('final_orcid').alias('orcid')) \\\n",
    "        .withColumn('cluster_num', F.monotonically_increasing_id().cast(StringType())) \\\n",
    "        .select('cluster_num','work_id','name_match_list','orcid') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/init_clustering/new_clusters/round={new_cluster_round}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68ed3b54-724f-40fd-976c-3232682a9158",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "explore_df = spark.read.parquet(\"s3://author-name-disambiguation/V3/temp_saving_location_for_init_clustering/init_clustering/cluster_exploration_df/round=merge_8/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "028a5e24-e1f4-443e-ac7c-53c07a8471f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "explore_df.select('cluster_num','orcid','names','institutions','coauthors') \\\n",
    "    .withColumn('institutions_len', F.size(F.col('institutions'))) \\\n",
    "    .withColumn('coauthors_len', F.size(F.col('coauthors'))) \\\n",
    "    .filter((F.col('institutions_len') > 0) & (F.col('coauthors_len')>0)) \\\n",
    "    .select(F.explode('names').alias('names'), 'institutions','cluster_num','orcid','coauthors') \\\n",
    "    .repartition(1000) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/init_clustering/testing_should_be_merges_3/data_1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33ec85ac-ebd0-443e-b644-9f3bb4bfe371",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{temp_save_path}/init_clustering/testing_should_be_merges_3/data_1/\") \\\n",
    "    .withColumn('coauthors_len', F.size(F.col('coauthors'))) \\\n",
    "    .filter(F.col('coauthors_len')<200) \\\n",
    "    .select(F.explode('coauthors').alias('coauthors'), 'names','cluster_num','orcid','institutions') \\\n",
    "    .repartition(1000) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/init_clustering/testing_should_be_merges_3/data_2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca8a9632-7b5c-4172-b190-144d8c547b5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{temp_save_path}/init_clustering/testing_should_be_merges_3/data_2/\") \\\n",
    "    .select(F.explode('institutions').alias('institutions'), 'names','cluster_num','orcid','coauthors') \\\n",
    "    .groupBy(['names','institutions','coauthors']) \\\n",
    "    .agg(F.collect_set(F.col('orcid')).alias('orcid'),\n",
    "         F.collect_set(F.col('cluster_num')).alias('cluster_num')) \\\n",
    "    .withColumn('cluster_num', F.sort_array(F.col('cluster_num'))) \\\n",
    "    .withColumn('clust_num_len', F.size(F.col('cluster_num')))\\\n",
    "    .filter(F.col('clust_num_len')>1) \\\n",
    "    .dropDuplicates(subset=['cluster_num'])\\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/init_clustering/testing_should_be_merges_3/institutions_coauthors/potential_pairs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ba6f9aa-5fb1-4245-9546-343edbf7102f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_ = get_unique_pairs_manual(\"init_clustering/testing_should_be_merges_3/institutions_coauthors/potential_pairs/\")\n",
    "\n",
    "_ = group_manual(\"init_clustering/testing_should_be_merges_3/institutions_coauthors/potential_pairs/\", \n",
    "             'merge_8', \"merge_9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de11e82-f6e6-4279-afa2-2a7d16b1e8f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_ = check_cluster_stats(\"merge_9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Singles Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6cb60c8-5c24-4a5c-8f5a-41e9b04ef78e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "i = 'merge_12'\n",
    "print(i, (datetime.now() - timedelta(hours=4)).strftime(\"%m/%d/%y %H:%M\"))\n",
    "_ = merge_single_clusters(temp_save_path, i, 'FINAL')\n",
    "_ = group_singles(i, temp_save_path, 'FINAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13ab76e1-1aa0-429b-9a48-dd97ac00eecf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_ = check_cluster_stats(\"singles_FINAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1eb11830-b755-4989-b4aa-76f1158355e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Writing to authors tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "813bfbc2-01b1-4ab8-b11d-149feaf55461",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def get_most_frequent_name(x):\n",
    "    return mode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e29c8862-5d95-45a8-a2bd-9d3cf794ea14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def get_unique_orcid_for_author_table(list_of_orcids):\n",
    "    if not isinstance(list_of_orcids, list):\n",
    "        try:\n",
    "            list_of_orcids = list_of_orcids.tolist()\n",
    "        except:\n",
    "            list_of_orcids = list(list_of_orcids)\n",
    "        \n",
    "    orcids = [x for x in list_of_orcids if x]\n",
    "    \n",
    "    if orcids:\n",
    "        return orcids[0]\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8270614f-9d3e-4cea-afd2-4c26cb45e92d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: 633803287"
     ]
    }
   ],
   "source": [
    "w1 = Window.orderBy(F.col('cluster_num'))\n",
    "\n",
    "cluster_df = spark.read.parquet(f\"{temp_save_path}/init_clustering/new_clusters/round=singles_FINAL/\") \\\n",
    "    .withColumn('temp_cluster_num', F.row_number().over(w1)) \\\n",
    "    .withColumn('author_id', F.lit(5000000000) + F.col('temp_cluster_num')) \\\n",
    "    .select('author_id', F.explode('work_id').alias('work_id')) \\\n",
    "    .select('work_id','author_id')\n",
    "\n",
    "cluster_df.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5dded95-68a7-4347-864b-5eb0dcb824e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|min(author_id)|max(author_id)|\n",
      "+--------------+--------------+\n",
      "|    5000000001|    5091914972|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_df.select(F.min('author_id'), F.max('author_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b774b299-93c8-4b64-bad2-b85b8697f9d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "authors_data = spark.read.parquet(f\"{iteration_save_path}all_sample_data_for_all_work_authors\") \\\n",
    "    .select(F.col('work_id'), F.col('orcid'),F.col('original_author')) \\\n",
    "    .filter(F.col('original_author').isNotNull()) \\\n",
    "    .filter(F.col('original_author')!='') \\\n",
    "    .join(cluster_df, how='left', on='work_id') \\\n",
    "    .groupby('author_id') \\\n",
    "    .agg(F.collect_set(F.col('orcid')).alias('orcid'), \n",
    "         F.collect_set(F.col('work_id')).alias('work_id'),\n",
    "         F.collect_set(F.col('original_author')).alias('alternate_names'),\n",
    "         F.collect_list(F.col('original_author')).alias('names')) \\\n",
    "    .withColumn('orcid', get_unique_orcid_for_author_table(F.col('orcid'))) \\\n",
    "    .withColumn('display_name', get_most_frequent_name(F.col('names'))) \\\n",
    "    .withColumn(\"created_date\", F.current_timestamp()) \\\n",
    "    .withColumn(\"modified_date\", F.current_timestamp()) \\\n",
    "    .select(F.col('work_id').alias('work_author_id'), \n",
    "            F.col('author_id').cast(LongType()), \n",
    "            'display_name',\n",
    "            'alternate_names', \n",
    "            'orcid', \n",
    "            'created_date', \n",
    "            'modified_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3caaf4b4-34a8-4f6d-821e-5517a4a9fcaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "authors_data.select(F.explode('work_author_id').alias('work_author_id'), \n",
    "            F.col('author_id').cast(LongType()), \n",
    "            'display_name',\n",
    "            'alternate_names', \n",
    "            'orcid', \n",
    "            'created_date', \n",
    "            'modified_date')\\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(\"s3://author-name-disambiguation/V3/PROD/current_authors_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e6a8549-4d31-44ae-a1df-229d213bdb63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "initial_clustering",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
