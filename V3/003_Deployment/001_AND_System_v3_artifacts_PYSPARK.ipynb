{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44920190-9120-4ae8-858f-14f52b073315",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import unicodedata\n",
    "import unidecode\n",
    "from nameparser import HumanName\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba961a2-7e64-4844-8d78-c47e5f47c752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, ArrayType, DoubleType, StructType, StructField, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f29191d-1059-4a1a-a42b-0a11e8599c1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prod_save_path = \"<S3path>\"\n",
    "temp_save_path = \"<S3path>\"\n",
    "iteration_save_path = \"<S3path>\"\n",
    "base_save_path = \"<S3path>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c753d87-388e-42ed-ba99-7a8b923eb8f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Current Authors Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e0ff8df-1500-4024-9587-92c0e2789ce6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(ArrayType(StringType())))\n",
    "def get_name_match_list(name):\n",
    "    name_split_1 = name.replace(\"-\", \"\").split()\n",
    "    name_split_2 = \"\"\n",
    "    if \"-\" in name:\n",
    "        name_split_2 = name.replace(\"-\", \" \").split()\n",
    "\n",
    "    fn = []\n",
    "    fni = []\n",
    "    \n",
    "    m1 = []\n",
    "    m1i = []\n",
    "    m2 = []\n",
    "    m2i = []\n",
    "    m3 = []\n",
    "    m3i = []\n",
    "    m4 = []\n",
    "    m4i = []\n",
    "    m5 = []\n",
    "    m5i = []\n",
    "\n",
    "    ln = []\n",
    "    lni = []\n",
    "    for name_split in [name_split_1, name_split_2]:\n",
    "        if len(name_split) == 0:\n",
    "            pass\n",
    "        elif len(name_split) == 1:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[0]) > 1:\n",
    "                ln.append(name_split[0])\n",
    "                lni.append(name_split[0][0])\n",
    "            else:\n",
    "                lni.append(name_split[0][0])\n",
    "            \n",
    "        elif len(name_split) == 2:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 3:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 4:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 5:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "                \n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 6:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "            \n",
    "            if len(name_split[4]) > 1:\n",
    "                m4.append(name_split[4])\n",
    "                m4i.append(name_split[4][0])\n",
    "            else:\n",
    "                m4i.append(name_split[4][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 7:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "            \n",
    "            if len(name_split[4]) > 1:\n",
    "                m4.append(name_split[4])\n",
    "                m4i.append(name_split[4][0])\n",
    "            else:\n",
    "                m4i.append(name_split[4][0])\n",
    "\n",
    "            if len(name_split[5]) > 1:\n",
    "                m5.append(name_split[5])\n",
    "                m5i.append(name_split[5][0])\n",
    "            else:\n",
    "                m5i.append(name_split[5][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        else:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "                \n",
    "            if len(name_split[4]) > 1:\n",
    "                m4.append(name_split[4])\n",
    "                m4i.append(name_split[4][0])\n",
    "            else:\n",
    "                m4i.append(name_split[4][0])\n",
    "\n",
    "            joined_names = \" \".join(name_split[5:-1])\n",
    "            m5.append(joined_names)\n",
    "            m5i.append(joined_names[0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "            \n",
    "\n",
    "    return [list(set(x)) for x in [fn,fni,m1,m1i,m2,m2i,m3,m3i,m4,m4i,m5,m5i,ln,lni]]\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def check_block_vs_block(block_1_names_list, block_2_names_list):\n",
    "    \n",
    "    # check first names\n",
    "    first_check, _ = match_block_names(block_1_names_list[0], block_1_names_list[1], block_2_names_list[0], \n",
    "                                    block_2_names_list[1])\n",
    "    # print(f\"FIRST {first_check}\")\n",
    "    \n",
    "    if first_check:\n",
    "        last_check, _ = match_block_names(block_1_names_list[-2], block_1_names_list[-1], block_2_names_list[-2], \n",
    "                                           block_2_names_list[-1])\n",
    "        # print(f\"LAST {last_check}\")\n",
    "        if last_check:\n",
    "            m1_check, more_to_go = match_block_names(block_1_names_list[2], block_1_names_list[3], block_2_names_list[2], \n",
    "                                           block_2_names_list[3])\n",
    "            if m1_check:\n",
    "                if not more_to_go:\n",
    "                    return 1\n",
    "                m2_check, more_to_go = match_block_names(block_1_names_list[4], block_1_names_list[5], block_2_names_list[4], \n",
    "                                                block_2_names_list[5])\n",
    "                \n",
    "                if m2_check:\n",
    "                    if not more_to_go:\n",
    "                        return 1\n",
    "                    m3_check, more_to_go = match_block_names(block_1_names_list[6], block_1_names_list[7], block_2_names_list[6], \n",
    "                                                block_2_names_list[7])\n",
    "                    if m3_check:\n",
    "                        if not more_to_go:\n",
    "                            return 1\n",
    "                        m4_check, more_to_go = match_block_names(block_1_names_list[8], block_1_names_list[8], block_2_names_list[8], \n",
    "                                                block_2_names_list[9])\n",
    "                        if m4_check:\n",
    "                            if not more_to_go:\n",
    "                                return 1\n",
    "                            m5_check, _ = match_block_names(block_1_names_list[10], block_1_names_list[11], block_2_names_list[10], \n",
    "                                                block_2_names_list[11])\n",
    "                            if m5_check:\n",
    "                                return 1\n",
    "                            else:\n",
    "                                return 0\n",
    "                        else:\n",
    "                            return 0\n",
    "                    else:\n",
    "                        return 0\n",
    "                else:\n",
    "                    return 0\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        swap_check = check_if_last_name_swapped_to_front_creates_match(block_1_names_list, block_2_names_list)\n",
    "        # print(f\"SWAP {swap_check}\")\n",
    "        if swap_check:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "def get_name_from_name_list(name_list):\n",
    "    name = []\n",
    "    for i in range(0,12,2):\n",
    "        if name_list[i]:\n",
    "            name.append(name_list[i][0])\n",
    "        elif name_list[i+1]:\n",
    "            name.append(name_list[i+1][0])\n",
    "        else:\n",
    "            break\n",
    "    if name_list[-2]:\n",
    "        name.append(name_list[-2][0])\n",
    "    elif name_list[-1]:\n",
    "        name.append(name_list[-1][0])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return name\n",
    "        \n",
    "def check_if_last_name_swapped_to_front_creates_match(block_1, block_2):\n",
    "    name_1 = get_name_from_name_list(block_1)\n",
    "    if len(name_1) != 2:\n",
    "        return False\n",
    "    else:\n",
    "        name_2 = get_name_from_name_list(block_2)\n",
    "        if len(name_2)==2:\n",
    "            if \" \".join(name_1) == \" \".join(name_2[-1:] + name_2[:-1]):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "def check_if_first_name_has_wrong_letter(block_1_names, block_1_initials, block_2_names, block_2_initials):\n",
    "    if block_1_initials and block_2_initials:\n",
    "        if any(x in block_1_initials for x in block_2_initials):\n",
    "            if block_1_names and block_2_names:\n",
    "                for block_1 in block_1_names:\n",
    "                    for block_2 in block_2_names:\n",
    "                        dist = lev.distance(block_1, block_2)\n",
    "                        if dist <=1:\n",
    "                            if len(block_1) == len(block_2):\n",
    "                                if len(block_1) > 4:\n",
    "                                    return True\n",
    "                                    break\n",
    "                                else:\n",
    "                                    pass\n",
    "                            else:\n",
    "                                pass\n",
    "                        else:\n",
    "                            pass\n",
    "                return False\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def match_block_names(block_1_names, block_1_initials, block_2_names, block_2_initials):\n",
    "    if block_1_names and block_2_names:\n",
    "        if any(x in block_1_names for x in block_2_names):\n",
    "            return True, True\n",
    "        else:\n",
    "            return False, True\n",
    "    elif block_1_names and not block_2_names:\n",
    "        if block_2_initials:\n",
    "            if any(x in block_1_initials for x in block_2_initials):\n",
    "                return True, True\n",
    "            else:\n",
    "                return False, True\n",
    "        else:\n",
    "            return True, True\n",
    "    elif not block_1_names and block_2_names:\n",
    "        if block_1_initials:\n",
    "            if any(x in block_1_initials for x in block_2_initials):\n",
    "                return True, True\n",
    "            else:\n",
    "                return False, True\n",
    "        else:\n",
    "            return True, True\n",
    "    elif block_1_initials and block_2_initials:\n",
    "        if any(x in block_1_initials for x in block_2_initials):\n",
    "            return True, True\n",
    "        else:\n",
    "            return False, True\n",
    "    else:\n",
    "        return True, False\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def transform_name_for_search(name):\n",
    "    name = unidecode.unidecode(unicodedata.normalize('NFKC', name))\n",
    "    name = name.lower().replace(\" \", \" \").replace(\".\", \" \").replace(\",\", \" \").replace(\"|\", \" \").replace(\")\", \"\").replace(\"(\", \"\")\\\n",
    "        .replace(\"-\", \"\").replace(\"&\", \"\").replace(\"$\", \"\").replace(\"#\", \"\").replace(\"@\", \"\").replace(\"%\", \"\").replace(\"0\", \"\") \\\n",
    "        .replace(\"1\", \"\").replace(\"2\", \"\").replace(\"3\", \"\").replace(\"4\", \"\").replace(\"5\", \"\").replace(\"6\", \"\").replace(\"7\", \"\") \\\n",
    "        .replace(\"8\", \"\").replace(\"9\", \"\").replace(\"*\", \"\").replace(\"^\", \"\").replace(\"{\", \"\").replace(\"}\", \"\").replace(\"+\", \"\") \\\n",
    "        .replace(\"=\", \"\").replace(\"_\", \"\").replace(\"~\", \"\").replace(\"`\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\\\\", \"\") \\\n",
    "        .replace(\"<\", \"\").replace(\">\", \"\").replace(\"?\", \"\").replace(\"/\", \"\").replace(\";\", \"\").replace(\":\", \"\").replace(\"\\'\", \"\") \\\n",
    "        .replace(\"\\\"\", \"\")\n",
    "    name = \" \".join(name.split())\n",
    "    return name\n",
    "\n",
    "@udf(returnType=ArrayType(ArrayType(StringType())))\n",
    "def create_author_name_list_from_list(name_lists):\n",
    "    if not isinstance(name_lists, list):\n",
    "        name_lists = name_lists.tolist()\n",
    "    \n",
    "    name_list_len = len(name_lists[0])\n",
    "    \n",
    "    temp_name_list = [[j[i] for j in name_lists] for i in range(name_list_len)]\n",
    "    temp_name_list_2 = [[j[0] for j in i if j] for i in temp_name_list]\n",
    "    \n",
    "    return [list(set(x)) for x in temp_name_list_2]\n",
    "\n",
    "@udf(returnType=ArrayType(ArrayType(StringType())))\n",
    "def get_name_match_from_alternate_names(alt_names):\n",
    "    trans_names = [transform_name_for_search(x) for x in alt_names]\n",
    "    name_lists = [get_name_match_list(x) for x in trans_names]\n",
    "    return create_author_name_list_from_list(name_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32bbc2d5-04bd-40bd-b61e-ab1554ce1012",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(\"s3://my-bucket/V3/PROD/current_authors_table/\") \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/temp_loc_for_author_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0449694e-94f7-421f-b0f5-66758b57fe3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{temp_save_path}/temp_loc_for_author_table/\") \\\n",
    "    .select('work_author_id', \n",
    "            'author_id', \n",
    "            'display_name',\n",
    "            'alternate_names', \n",
    "            'orcid', \n",
    "            'created_date', \n",
    "            'modified_date') \\\n",
    "    .withColumn('name_match_list', get_name_match_from_alternate_names('alternate_names')) \\\n",
    "    .repartition(256) \\\n",
    "    .select('work_author_id', \n",
    "            'author_id', \n",
    "            'display_name',\n",
    "            'alternate_names', \n",
    "            'orcid', \n",
    "            'name_match_list',\n",
    "            'created_date', \n",
    "            'modified_date')\\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/temp_loc_for_new_current_author_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6896d39f-98c5-475b-a61b-bb93ec0ff0e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dup_drop = spark.read.parquet('s3://my-bucket/work_ids_and_authors_to_drop/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3400784b-d4ad-4f3f-aa9c-fa75ea98c11e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "authors_table = spark.read.parquet(f\"s3://temp-prod-working-bucket/temp_loc_for_new_current_author_table/\") \\\n",
    "    .join(dup_drop, how='leftanti', on=['work_author_id','author_id']) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"s3://my-bucket/temp_loc_for_new_current_author_table_INIT/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d486df2-f73a-43d3-bf45-4eac4f21cef9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Current Features Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0158d12-c6d7-429b-9688-f3b87c42e760",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def transform_author_name(author):\n",
    "    if author.startswith(\"None \"):\n",
    "        author = author.replace(\"None \", \"\")\n",
    "    elif author.startswith(\"Array \"):\n",
    "        author = author.replace(\"Array \", \"\")\n",
    "\n",
    "    author = unicodedata.normalize('NFKC', author)\n",
    "    \n",
    "    author_name = HumanName(\" \".join(author.split()))\n",
    "\n",
    "    if (author_name.title == 'Dr.') | (author_name.title == ''):\n",
    "        temp_new_author_name = f\"{author_name.first} {author_name.middle} {author_name.last}\"\n",
    "    else:\n",
    "        temp_new_author_name = f\"{author_name.title} {author_name.first} {author_name.middle} {author_name.last}\"\n",
    "\n",
    "    new_author_name = \" \".join(temp_new_author_name.split())\n",
    "\n",
    "    author_names = new_author_name.split(\" \")\n",
    "    \n",
    "    if (author_name.title != '') : \n",
    "        final_author_name = new_author_name\n",
    "    else:\n",
    "        if len(author_names) == 1:\n",
    "            final_author_name = new_author_name\n",
    "        elif len(author_names) == 2:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        elif len(author_names) == 3:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[2]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[2]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\") & (author_names[2][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        elif len(author_names) == 4:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[2]) == 1) & (len(author_names[3]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[3]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[2]) == 2) & (len(author_names[3]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\") & (author_names[2][1]==\".\") & (author_names[3][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[3]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        else:\n",
    "            final_author_name = new_author_name\n",
    "    return final_author_name\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))  \n",
    "def remove_current_author(author, coauthors):\n",
    "    return [x for x in coauthors if x!=author][:250]\n",
    "\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def transform_list_col_for_nulls_string(col_with_nulls):\n",
    "    if isinstance(col_with_nulls, list):\n",
    "        return col_with_nulls\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "@udf(returnType=ArrayType(LongType()))\n",
    "def transform_list_col_for_nulls_long(col_with_nulls):\n",
    "    if isinstance(col_with_nulls, list):\n",
    "        return col_with_nulls\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def remove_current_author(author, coauthors):\n",
    "    return [x for x in coauthors if x!=author][:250]\n",
    "\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def coauthor_transform(coauthors):\n",
    "    final_coauthors = []\n",
    "    skip_list = [\" \", \",\" ,\".\" ,\"-\" ,\":\" ,\"/\"]\n",
    "\n",
    "    for coauthor in coauthors:\n",
    "        split_coauthor = coauthor.split(\" \")\n",
    "        if len(split_coauthor) > 1:\n",
    "            temp_coauthor = f\"{split_coauthor[0][0]}_{split_coauthor[-1]}\".lower()\n",
    "            final_coauthors.append(\"\".join([i for i in temp_coauthor if i not in skip_list]))\n",
    "        else:\n",
    "            final_coauthors.append(\"\".join([i for i in coauthor if i not in skip_list]))\n",
    "\n",
    "    return list(set(final_coauthors))\n",
    "\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def get_orcid_from_list(orcid_list):\n",
    "    if isinstance(orcid_list, list):\n",
    "        if orcid_list:\n",
    "            orcid = orcid_list[0]\n",
    "        else:\n",
    "            orcid = ''\n",
    "    elif isinstance(orcid_list, set):\n",
    "        orcid_list = list(orcid_list)\n",
    "        if orcid_list:\n",
    "            orcid = orcid_list[0]\n",
    "        else:\n",
    "            orcid = ''\n",
    "    else:\n",
    "        orcid = ''\n",
    "    return orcid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9c494b8-b2eb-493c-9115-0fc4d2d4390b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def length_greater_than_6(x):\n",
    "    return (F.length(x) > 6)\n",
    "\n",
    "def concept_L0_removed(x):\n",
    "    return ~x.isin(['17744445','138885662','162324750','144133560','15744967','33923547','71924100','86803240','41008148','127313418','185592680','142362112','144024400','127413603','205649164','95457728','192562407','121332964','39432304'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ed4eb64-5a55-44fc-9485-7976de74cd15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aff_data = spark.read.parquet(f\"{base_save_path}static_affiliations\") \\\n",
    "    .filter(F.col('paper_id')<=highest_work_in_bulk) \\\n",
    "    .select(F.col('paper_id').alias('paper_id'), F.col('author_sequence_number').alias('seq_no'),\n",
    "        F.trim(F.col('original_author')).alias('original_author'), F.col('original_orcid').alias('orcid'), \n",
    "        F.col('affiliation_id').alias('institution')) \\\n",
    "    .filter(F.col('original_author')!=\"\") \\\n",
    "    .filter(F.col('original_author').isNotNull()) \\\n",
    "    .withColumn('transformed_name', transform_author_name(F.col('original_author'))) \\\n",
    "    .groupBy(['paper_id','seq_no','original_author','transformed_name']) \\\n",
    "    .agg(F.collect_set(F.col('orcid')).alias('orcid'), \n",
    "         F.collect_set(F.col('institution')).alias('institutions'))\n",
    "\n",
    "aff_data.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec548e24-2a8a-4511-8f58-44c5f239eb5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "citations = spark.read.parquet(f\"{base_save_path}static_citations\") \\\n",
    "    .filter(F.col('paper_id')<=highest_work_in_bulk) \\\n",
    "    .select(F.col('paper_id').cast(LongType()).alias('paper_id'), \n",
    "            F.col('paper_reference_id').cast(LongType()).alias('citations')) \\\n",
    "    .dropDuplicates() \\\n",
    "    .groupBy('paper_id') \\\n",
    "    .agg(F.collect_list(F.col('citations')).alias('citations'))\n",
    "\n",
    "citations.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92450ada-7723-4929-9f7b-1e805c272700",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for_coauthors = aff_data.alias('for_coauthors') \\\n",
    "    .select('paper_id','transformed_name') \\\n",
    "    .dropDuplicates() \\\n",
    "    .groupBy('paper_id') \\\n",
    "    .agg(F.collect_list(F.col('transformed_name')).alias('all_authors'))\n",
    "\n",
    "for_coauthors.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c195c96-f79a-4174-b670-7f4b0afa6303",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "concepts = spark.read.parquet(f\"{base_save_path}static_concepts\") \\\n",
    "    .filter(F.col('paper_id')<=highest_work_in_bulk) \\\n",
    "    .select('paper_id',F.col('field_of_study').cast(LongType()),F.col('score').cast(FloatType()).alias('score')) \\\n",
    "    .filter(F.col('score') >=0.32) \\\n",
    "    .dropDuplicates(subset=['paper_id','field_of_study']) \\\n",
    "    .groupby('paper_id') \\\n",
    "    .agg(F.collect_list(F.col('field_of_study')).alias('concepts'))\n",
    "\n",
    "concepts.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c65b8be-9ecb-4bfd-a6b6-e48aef633905",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w1 = Window.partitionBy('work_author_id_2').orderBy(F.col('name_len').desc())\n",
    "\n",
    "aff_data \\\n",
    ".join(concepts, how='left', on='paper_id') \\\n",
    ".join(for_coauthors, how='left', on='paper_id') \\\n",
    ".join(citations, how='left', on='paper_id') \\\n",
    ".withColumn('citations', transform_list_col_for_nulls_long(F.col('citations'))) \\\n",
    ".withColumn('all_authors', transform_list_col_for_nulls_string(F.col('all_authors'))) \\\n",
    ".withColumn('concepts', transform_list_col_for_nulls_long(F.col('concepts'))) \\\n",
    ".withColumn('institutions', transform_list_col_for_nulls_long(F.col('institutions'))) \\\n",
    ".withColumn('coauthors', remove_current_author(F.col('transformed_name'),F.col('all_authors'))) \\\n",
    ".withColumn('coauthors', coauthor_transform(F.col('coauthors'))) \\\n",
    ".withColumn('orcid', get_orcid_from_list(F.col('orcid'))) \\\n",
    ".withColumn(\"created_date\", F.current_timestamp()) \\\n",
    ".select(F.concat_ws(\"_\", F.col('paper_id'), F.col('seq_no')).alias('work_author_id_2'),\n",
    "        F.col('paper_id').cast(LongType()).alias('paper_id_2'),'original_author', F.col('transformed_name').alias('author_2'),\n",
    "        F.col('orcid').alias('orcid_2'),F.col('coauthors').alias('coauthors_2'), \n",
    "        F.col('citations').alias('citations_2'), F.col('institutions').alias('institutions_2'), \n",
    "        F.col('concepts').alias('concepts_2')) \\\n",
    ".withColumn('name_len', F.length(F.col('original_author'))) \\\n",
    ".withColumn('rank', F.row_number().over(w1)) \\\n",
    ".filter(F.col('rank')==1) \\\n",
    ".withColumn('concepts_2', F.array_distinct(F.col('concepts_2'))) \\\n",
    ".withColumn('concepts_shorter_2', F.filter(F.col('concepts_2'), concept_L0_removed)) \\\n",
    ".withColumn('coauthors_shorter_2', F.filter(F.col('coauthors_2'), length_greater_than_6)) \\\n",
    ".select('work_author_id_2','paper_id_2','original_author','author_2','orcid_2','coauthors_shorter_2','concepts_shorter_2',\n",
    "        'institutions_2','citations_2') \\\n",
    ".repartition(256) \\\n",
    ".write.mode('overwrite') \\\n",
    ".parquet(f\"{temp_save_path}/temp_loc_for_new_current_features_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09aed021-2f71-4ef0-9f99-90b0a1ad1b96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dup_drop = spark.read.parquet('s3://my-bucket/work_ids_and_authors_to_drop/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e67f4b-712f-4f68-8b51-24ee50a51dd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dup_drop.coalesce(1).write.mode('overwrite').parquet('s3://temp-prod-working-bucket/work_ids_and_authors_to_drop_single_file/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7620a9c-5112-4a7e-974b-0544977a0111",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e781f6bb-bb01-404d-8cf5-bf2eda379061",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Author Names Match Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bacd92ca-6dd0-41dc-9724-9f69e53875cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def get_starting_letter(names):\n",
    "    temp_letters = [x[0] for x in names.split(\" \") if x]\n",
    "    return temp_letters[0] if temp_letters else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "295b9607-6348-41ef-9a13-424a9d0e28bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def only_get_last(all_names):\n",
    "    all_names = all_names.split(\" \")\n",
    "    if len(all_names) > 1:\n",
    "        return all_names[-1]\n",
    "    else:\n",
    "        return all_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55fc1d98-599c-407f-9c31-33873642e7fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{temp_save_path}/temp_loc_for_new_current_features_table/\") \\\n",
    "    .select('work_author_id_2', 'orcid_2', F.col('author_2').alias('transformed_name')) \\\n",
    "    .filter(F.col('transformed_name')!=\"\") \\\n",
    "    .filter(F.col('transformed_name').isNotNull()) \\\n",
    "    .withColumn('transformed_search_name', transform_name_for_search(F.col('transformed_name'))) \\\n",
    "    .withColumn('name_len', F.length(F.col('transformed_search_name'))) \\\n",
    "    .filter(F.col('name_len')>1) \\\n",
    "    .withColumn('name_match_list_2', get_name_match_list(F.col('transformed_search_name'))) \\\n",
    "    .withColumn('block', only_get_last(F.col('transformed_search_name'))) \\\n",
    "    .select('work_author_id_2','name_match_list_2', 'orcid_2', 'transformed_search_name', 'block')\\\n",
    "    .withColumn('block_removed', F.expr(\"regexp_replace(transformed_search_name, block, '')\")) \\\n",
    "    .withColumn('new_block_removed', F.trim(F.expr(\"regexp_replace(block_removed, '  ', ' ')\"))) \\\n",
    "    .withColumn('letter', get_starting_letter(F.col('new_block_removed'))) \\\n",
    "    .select('work_author_id_2','orcid_2','name_match_list_2', 'block', 'letter') \\\n",
    "    .dropDuplicates() \\\n",
    "    .repartition(250) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/temp_loc_for_new_author_names_match_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acdda625-0182-410b-b21f-80d857672042",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{temp_save_path}/temp_loc_for_new_author_names_match_table/\").sample(0.0001).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef2f785e-ce0a-465d-8431-e61510b841f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8f48a80-9290-4287-9c9a-5abd04907810",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AND_System_v3_artifacts",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
