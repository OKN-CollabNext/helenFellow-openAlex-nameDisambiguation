{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44920190-9120-4ae8-858f-14f52b073315",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import unicodedata\n",
    "import unidecode\n",
    "import datetime\n",
    "from statistics import mode\n",
    "from nameparser import HumanName\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba961a2-7e64-4844-8d78-c47e5f47c752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, ArrayType, DoubleType, StructType, StructField, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f29191d-1059-4a1a-a42b-0a11e8599c1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "curr_date = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "prod_save_path = \"<S3path>\"\n",
    "temp_save_path = f\"<S3path>/{curr_date}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48537f80-c03a-4170-b5b6-e50abb2306ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Load Disambiguator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cd221af-a5eb-4e1a-8a0b-a49784900d1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(\"<local-model-path>/Disambiguator.pkl\", \"rb\") as f:\n",
    "    disambiguator_model = pickle.load(f)\n",
    "\n",
    "broadcast_disambiguator_model = spark.sparkContext.broadcast(disambiguator_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a6936b8-e68a-4b21-bf74-b52e2123411e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Get Latest Data to Disambiguate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aa07445-3a65-40d2-b491-2dacad7fc07e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_secret():\n",
    "\n",
    "    ### code for getting AWS secrets ###\n",
    "    return secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f340b908-7b23-4623-808e-3eafd3e7ab58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "secret = get_secret()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70b6e65c-76be-4cf8-8f4a-25905e619d4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "    .format(\"postgresql\")\n",
    "    .option(\"dbtable\", \"<input-data-postgres-table>\")\n",
    "    .option(\"host\", secret['host'])\n",
    "    .option(\"port\", secret['port'])\n",
    "    .option(\"database\", secret['dbname'])\n",
    "    .option(\"user\", secret['username'])\n",
    "    .option(\"password\", secret['password'])\n",
    "    .option(\"partitionColumn\", \"partition\")\n",
    "    .option(\"lowerBound\", 0)\n",
    "    .option(\"upperBound\", 21)\n",
    "    .option(\"numPartitions\", 6)\n",
    "    .option(\"fetchSize\", \"15\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87a355fa-3de1-43da-b7ea-3448a97eabc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode('overwrite') \\\n",
    "    .filter(F.col('original_author').isNotNull()) \\\n",
    "    .filter(F.col('original_author')!='') \\\n",
    "    .parquet(f\"<S3path>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "322b2469-676b-411c-a711-99993200785a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Transform Data for Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac143a42-2ad6-4ec6-9222-c2ac11411e2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def transform_author_name(author):\n",
    "    if author.startswith(\"None \"):\n",
    "        author = author.replace(\"None \", \"\")\n",
    "    elif author.startswith(\"Array \"):\n",
    "        author = author.replace(\"Array \", \"\")\n",
    "\n",
    "    author = unicodedata.normalize('NFKC', author)\n",
    "    \n",
    "    author_name = HumanName(\" \".join(author.split()))\n",
    "\n",
    "    if (author_name.title == 'Dr.') | (author_name.title == ''):\n",
    "        temp_new_author_name = f\"{author_name.first} {author_name.middle} {author_name.last}\"\n",
    "    else:\n",
    "        temp_new_author_name = f\"{author_name.title} {author_name.first} {author_name.middle} {author_name.last}\"\n",
    "\n",
    "    new_author_name = \" \".join(temp_new_author_name.split())\n",
    "\n",
    "    author_names = new_author_name.split(\" \")\n",
    "    \n",
    "    if (author_name.title != '') : \n",
    "        final_author_name = new_author_name\n",
    "    else:\n",
    "        if len(author_names) == 1:\n",
    "            final_author_name = new_author_name\n",
    "        elif len(author_names) == 2:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        elif len(author_names) == 3:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[2]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[2]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\") & (author_names[2][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        elif len(author_names) == 4:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[2]) == 1) & (len(author_names[3]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[3]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[2]) == 2) & (len(author_names[3]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\") & (author_names[2][1]==\".\") & (author_names[3][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[3]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        else:\n",
    "            final_author_name = new_author_name\n",
    "    return final_author_name\n",
    "\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def transform_coauthors(coauthors):\n",
    "    return [transform_author_name_reg(x) for x in coauthors]\n",
    "\n",
    "def transform_author_name_reg(author):\n",
    "    if author.startswith(\"None \"):\n",
    "        author = author.replace(\"None \", \"\")\n",
    "    elif author.startswith(\"Array \"):\n",
    "        author = author.replace(\"Array \", \"\")\n",
    "\n",
    "    author = unicodedata.normalize('NFKC', author)\n",
    "    \n",
    "    author_name = HumanName(\" \".join(author.split()))\n",
    "\n",
    "    if (author_name.title == 'Dr.') | (author_name.title == ''):\n",
    "        temp_new_author_name = f\"{author_name.first} {author_name.middle} {author_name.last}\"\n",
    "    else:\n",
    "        temp_new_author_name = f\"{author_name.title} {author_name.first} {author_name.middle} {author_name.last}\"\n",
    "\n",
    "    new_author_name = \" \".join(temp_new_author_name.split())\n",
    "\n",
    "    author_names = new_author_name.split(\" \")\n",
    "    \n",
    "    if (author_name.title != '') : \n",
    "        final_author_name = new_author_name\n",
    "    else:\n",
    "        if len(author_names) == 1:\n",
    "            final_author_name = new_author_name\n",
    "        elif len(author_names) == 2:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        elif len(author_names) == 3:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[2]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[2]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\") & (author_names[2][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        elif len(author_names) == 4:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[2]) == 1) & (len(author_names[3]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[3]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[2]) == 2) & (len(author_names[3]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\") & (author_names[2][1]==\".\") & (author_names[3][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[3]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        else:\n",
    "            final_author_name = new_author_name\n",
    "    return final_author_name\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))  \n",
    "def remove_current_author(author, coauthors):\n",
    "    return [x for x in coauthors if x!=author][:250]\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def transform_list_col_for_nulls_string(col_with_nulls):\n",
    "    if isinstance(col_with_nulls, list):\n",
    "        return col_with_nulls\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "@udf(returnType=ArrayType(LongType()))\n",
    "def transform_list_col_for_nulls_long(col_with_nulls):\n",
    "    if isinstance(col_with_nulls, list):\n",
    "        return col_with_nulls\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def remove_current_author(author, coauthors):\n",
    "    return [x for x in coauthors if x!=author][:250]\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def coauthor_transform(coauthors):\n",
    "    final_coauthors = []\n",
    "    skip_list = [\" \", \",\" ,\".\" ,\"-\" ,\":\" ,\"/\"]\n",
    "\n",
    "    for coauthor in coauthors:\n",
    "        split_coauthor = coauthor.split(\" \")\n",
    "        if len(split_coauthor) > 1:\n",
    "            temp_coauthor = f\"{split_coauthor[0][0]}_{split_coauthor[-1]}\".lower()\n",
    "            final_coauthors.append(\"\".join([i for i in temp_coauthor if i not in skip_list]))\n",
    "        else:\n",
    "            final_coauthors.append(\"\".join([i for i in coauthor if i not in skip_list]))\n",
    "\n",
    "    return list(set(final_coauthors))\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def get_orcid_from_list(orcid_list):\n",
    "    if isinstance(orcid_list, list):\n",
    "        if orcid_list:\n",
    "            orcid = orcid_list[0]\n",
    "        else:\n",
    "            orcid = ''\n",
    "    elif isinstance(orcid_list, set):\n",
    "        orcid_list = list(orcid_list)\n",
    "        if orcid_list:\n",
    "            orcid = orcid_list[0]\n",
    "        else:\n",
    "            orcid = ''\n",
    "    else:\n",
    "        orcid = ''\n",
    "    return orcid\n",
    "\n",
    "def length_greater_than_6(x):\n",
    "    return (F.length(x) > 6)\n",
    "\n",
    "def concept_L0_removed(x):\n",
    "    return ~x.isin([17744445,138885662,162324750,144133560,15744967,33923547,71924100,86803240,41008148,127313418,185592680,142362112,144024400,127413603,205649164,95457728,192562407,121332964,39432304])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1121695f-5b39-486f-85b0-f1b48803f462",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "curr_author_table = spark.read.parquet(f\"{prod_save_path}/current_authors_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "747a88e7-4d5a-426e-9004-2f70ce4e9f0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "authors_table_last_date = curr_author_table.select(F.max('modified_date')).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb187594-dcfb-4680-831b-1930bd80626e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "need to add the filter for new data above the last modified date of authors table (replace join with date filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02411847-77bf-4806-b086-2db9c7f861d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w1 = Window.partitionBy('work_author_id').orderBy(F.col('name_len').desc())\n",
    "\n",
    "(spark.read\n",
    "    .parquet(f\"<S3path>\")\n",
    "    .select('work_author_id', F.trim(F.col('original_author')).alias('original_author'), 'orcid', 'concepts', 'institutions', \n",
    "            'citations', 'coauthors', 'created_date', 'partition')\n",
    "    .filter(F.col('original_author').isNotNull())\n",
    "    .filter(F.col('original_author')!='')\n",
    "    .withColumn('name_len', F.length(F.col('original_author')))\n",
    "    .withColumn('rank', F.row_number().over(w1))\n",
    "    .filter(F.col('rank')==1)\n",
    "    .join(curr_author_table.select('work_author_id'), how='leftanti', on='work_author_id')\n",
    "    .withColumn('citations', transform_list_col_for_nulls_long(F.col('citations')))\n",
    "    .withColumn('coauthors', transform_list_col_for_nulls_string(F.col('coauthors')))\n",
    "    .withColumn('concepts', transform_list_col_for_nulls_long(F.col('concepts')))\n",
    "    .withColumn('institutions', transform_list_col_for_nulls_long(F.col('institutions')))\n",
    "    .withColumn('author', transform_author_name(F.col('original_author')))\n",
    "    .withColumn('coauthors', transform_coauthors(F.col('coauthors')))\n",
    "    .withColumn('coauthors', remove_current_author(F.col('author'),F.col('coauthors')))\n",
    "    .withColumn('coauthors', coauthor_transform(F.col('coauthors')))\n",
    "    .withColumn('orcid', F.when(F.col('orcid').isNull(), '').otherwise(F.col('orcid')))\n",
    "    .withColumn('paper_id', F.split(F.col('work_author_id'), \"_\").getItem(0).cast(LongType()))\n",
    "    .withColumn('concepts', F.array_distinct(F.col('concepts')))\n",
    "    .withColumn('concepts_shorter', F.filter(F.col('concepts'), concept_L0_removed))\n",
    "    .withColumn('coauthors_shorter', F.filter(F.col('coauthors'), length_greater_than_6))\n",
    "    .select('work_author_id','paper_id','original_author','author','orcid','coauthors_shorter','concepts_shorter',\n",
    "        'institutions','citations','created_date')\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/new_data_to_disambiguate/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93668424-b462-4915-8954-ca922adefa2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_data_size = spark.read.parquet(f\"{temp_save_path}/new_data_to_disambiguate/\").count()\n",
    "new_data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebba4c30-ee13-45a3-b46f-c157ae8e6407",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if new_data_size > 0:\n",
    "    print(\"NEW ROWS TO DISAMBIGUATE\")\n",
    "    pass\n",
    "else:\n",
    "    print(\"NO NEW DATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bbbe816-268e-4b49-b680-6114a1719d6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93dd9b7-27bb-4e5b-8516-91e82f7c1c92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=IntegerType())\n",
    "def get_random_int_udf(block_id):\n",
    "    return random.randint(0, 1000000)\n",
    "\n",
    "def length_greater_than_6(x):\n",
    "    return (F.length(x) > 6)\n",
    "\n",
    "def concept_L0_removed(x):\n",
    "    return ~x.isin([17744445,138885662,162324750,144133560,15744967,33923547,71924100,86803240,41008148,127313418,185592680,142362112,144024400,127413603,205649164,95457728,192562407,121332964,39432304])\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def only_get_last(all_names):\n",
    "    all_names = all_names.split(\" \")\n",
    "    if len(all_names) > 1:\n",
    "        return all_names[-1]\n",
    "    else:\n",
    "        return all_names[0]\n",
    "    \n",
    "@udf (returnType=ArrayType(ArrayType(StringType())))\n",
    "def score_data(full_arr):\n",
    "    full_arr = np.array(full_arr)\n",
    "    data_arr = full_arr[:,2:].astype('float')\n",
    "    block_arr = full_arr[:,0]\n",
    "    label_arr = full_arr[:,1]\n",
    "    model_preds = broadcast_disambiguator_model.value.predict_proba(data_arr)[:,1]\n",
    "    return np.vstack([block_arr[model_preds>0.05], label_arr[model_preds>0.05], model_preds[model_preds>0.05].astype('str')]).T.tolist()\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def get_starting_letter(names):\n",
    "    temp_letters = [x[0] for x in names.split(\" \") if x]\n",
    "    return temp_letters[0] if temp_letters else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066c3555-863d-45a3-a4c7-2e5c0ac56295",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(StringType()))\n",
    "def group_non_latin_characters(text):\n",
    "    groups = []\n",
    "    text = text.replace(\".\", \"\").replace(\" \", \"\")\n",
    "    for char in text:\n",
    "        try:\n",
    "            script = unicodedata.name(char).split(\" \")[0]\n",
    "            if script == 'LATIN':\n",
    "                pass\n",
    "            else:\n",
    "                if script not in groups:\n",
    "                    groups.append(script)\n",
    "        except:\n",
    "            if \"UNK\" not in groups:\n",
    "                groups.append(\"UNK\")\n",
    "    return groups\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def name_to_keep_ind(groups):\n",
    "    groups_to_skip = ['HIRAGANA', 'CJK', 'KATAKANA','ARABIC', 'HANGUL', 'THAI','DEVANAGARI','BENGALI',\n",
    "                      'THAANA','GUJARATI']\n",
    "    \n",
    "    if any(x in groups_to_skip for x in groups):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f93ab7f-deb6-4067-85ef-883d9d9663f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=IntegerType())\n",
    "def check_block_vs_block(block_1_names_list, block_2_names_list):\n",
    "    \n",
    "    # check first names\n",
    "    first_check, _ = match_block_names(block_1_names_list[0], block_1_names_list[1], block_2_names_list[0], \n",
    "                                    block_2_names_list[1])\n",
    "    # print(f\"FIRST {first_check}\")\n",
    "    \n",
    "    if first_check:\n",
    "        last_check, _ = match_block_names(block_1_names_list[-2], block_1_names_list[-1], block_2_names_list[-2], \n",
    "                                           block_2_names_list[-1])\n",
    "        # print(f\"LAST {last_check}\")\n",
    "        if last_check:\n",
    "            m1_check, more_to_go = match_block_names(block_1_names_list[2], block_1_names_list[3], block_2_names_list[2], \n",
    "                                           block_2_names_list[3])\n",
    "            if m1_check:\n",
    "                if not more_to_go:\n",
    "                    return 1\n",
    "                m2_check, more_to_go = match_block_names(block_1_names_list[4], block_1_names_list[5], block_2_names_list[4], \n",
    "                                                block_2_names_list[5])\n",
    "                \n",
    "                if m2_check:\n",
    "                    if not more_to_go:\n",
    "                        return 1\n",
    "                    m3_check, more_to_go = match_block_names(block_1_names_list[6], block_1_names_list[7], block_2_names_list[6], \n",
    "                                                block_2_names_list[7])\n",
    "                    if m3_check:\n",
    "                        if not more_to_go:\n",
    "                            return 1\n",
    "                        m4_check, more_to_go = match_block_names(block_1_names_list[8], block_1_names_list[8], block_2_names_list[8], \n",
    "                                                block_2_names_list[9])\n",
    "                        if m4_check:\n",
    "                            if not more_to_go:\n",
    "                                return 1\n",
    "                            m5_check, _ = match_block_names(block_1_names_list[10], block_1_names_list[11], block_2_names_list[10], \n",
    "                                                block_2_names_list[11])\n",
    "                            if m5_check:\n",
    "                                return 1\n",
    "                            else:\n",
    "                                return 0\n",
    "                        else:\n",
    "                            return 0\n",
    "                    else:\n",
    "                        return 0\n",
    "                else:\n",
    "                    return 0\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        swap_check = check_if_last_name_swapped_to_front_creates_match(block_1_names_list, block_2_names_list)\n",
    "        # print(f\"SWAP {swap_check}\")\n",
    "        if swap_check:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "def get_name_from_name_list(name_list):\n",
    "    name = []\n",
    "    for i in range(0,12,2):\n",
    "        if name_list[i]:\n",
    "            name.append(name_list[i][0])\n",
    "        elif name_list[i+1]:\n",
    "            name.append(name_list[i+1][0])\n",
    "        else:\n",
    "            break\n",
    "    if name_list[-2]:\n",
    "        name.append(name_list[-2][0])\n",
    "    elif name_list[-1]:\n",
    "        name.append(name_list[-1][0])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return name\n",
    "        \n",
    "def check_if_last_name_swapped_to_front_creates_match(block_1, block_2):\n",
    "    name_1 = get_name_from_name_list(block_1)\n",
    "    if len(name_1) != 2:\n",
    "        return False\n",
    "    else:\n",
    "        name_2 = get_name_from_name_list(block_2)\n",
    "        if len(name_2)==2:\n",
    "            if \" \".join(name_1) == \" \".join(name_2[-1:] + name_2[:-1]):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "def match_block_names(block_1_names, block_1_initials, block_2_names, block_2_initials):\n",
    "    if block_1_names and block_2_names:\n",
    "        if any(x in block_1_names for x in block_2_names):\n",
    "            return True, True\n",
    "        else:\n",
    "            return False, True\n",
    "    elif block_1_names and not block_2_names:\n",
    "        if block_2_initials:\n",
    "            if any(x in block_1_initials for x in block_2_initials):\n",
    "                return True, True\n",
    "            else:\n",
    "                return False, True\n",
    "        else:\n",
    "            return True, True\n",
    "    elif not block_1_names and block_2_names:\n",
    "        if block_1_initials:\n",
    "            if any(x in block_1_initials for x in block_2_initials):\n",
    "                return True, True\n",
    "            else:\n",
    "                return False, True\n",
    "        else:\n",
    "            return True, True\n",
    "    elif block_1_initials and block_2_initials:\n",
    "        if any(x in block_1_initials for x in block_2_initials):\n",
    "            return True, True\n",
    "        else:\n",
    "            return False, True\n",
    "    else:\n",
    "        return True, False\n",
    "\n",
    "@udf(returnType=ArrayType(ArrayType(StringType())))\n",
    "def get_name_match_list(name):\n",
    "    name_split_1 = name.replace(\"-\", \"\").split()\n",
    "    name_split_2 = \"\"\n",
    "    if \"-\" in name:\n",
    "        name_split_2 = name.replace(\"-\", \" \").split()\n",
    "\n",
    "    fn = []\n",
    "    fni = []\n",
    "    \n",
    "    m1 = []\n",
    "    m1i = []\n",
    "    m2 = []\n",
    "    m2i = []\n",
    "    m3 = []\n",
    "    m3i = []\n",
    "    m4 = []\n",
    "    m4i = []\n",
    "    m5 = []\n",
    "    m5i = []\n",
    "\n",
    "    ln = []\n",
    "    lni = []\n",
    "    for name_split in [name_split_1, name_split_2]:\n",
    "        if len(name_split) == 0:\n",
    "            pass\n",
    "        elif len(name_split) == 1:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[0]) > 1:\n",
    "                ln.append(name_split[0])\n",
    "                lni.append(name_split[0][0])\n",
    "            else:\n",
    "                lni.append(name_split[0][0])\n",
    "            \n",
    "        elif len(name_split) == 2:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 3:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 4:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 5:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "                \n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 6:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "            \n",
    "            if len(name_split[4]) > 1:\n",
    "                m4.append(name_split[4])\n",
    "                m4i.append(name_split[4][0])\n",
    "            else:\n",
    "                m4i.append(name_split[4][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 7:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "            \n",
    "            if len(name_split[4]) > 1:\n",
    "                m4.append(name_split[4])\n",
    "                m4i.append(name_split[4][0])\n",
    "            else:\n",
    "                m4i.append(name_split[4][0])\n",
    "\n",
    "            if len(name_split[5]) > 1:\n",
    "                m5.append(name_split[5])\n",
    "                m5i.append(name_split[5][0])\n",
    "            else:\n",
    "                m5i.append(name_split[5][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        else:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "                \n",
    "            if len(name_split[4]) > 1:\n",
    "                m4.append(name_split[4])\n",
    "                m4i.append(name_split[4][0])\n",
    "            else:\n",
    "                m4i.append(name_split[4][0])\n",
    "\n",
    "            joined_names = \" \".join(name_split[5:-1])\n",
    "            m5.append(joined_names)\n",
    "            m5i.append(joined_names[0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "            \n",
    "\n",
    "    return [list(set(x)) for x in [fn,fni,m1,m1i,m2,m2i,m3,m3i,m4,m4i,m5,m5i,ln,lni]]\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def transform_author_name(author):\n",
    "    if author.startswith(\"None \"):\n",
    "        author = author.replace(\"None \", \"\")\n",
    "    elif author.startswith(\"Array \"):\n",
    "        author = author.replace(\"Array \", \"\")\n",
    "\n",
    "    author = unicodedata.normalize('NFKC', author)\n",
    "    \n",
    "    author_name = HumanName(\" \".join(author.split()))\n",
    "\n",
    "    if (author_name.title == 'Dr.') | (author_name.title == ''):\n",
    "        temp_new_author_name = f\"{author_name.first} {author_name.middle} {author_name.last}\"\n",
    "    else:\n",
    "        temp_new_author_name = f\"{author_name.title} {author_name.first} {author_name.middle} {author_name.last}\"\n",
    "\n",
    "    new_author_name = \" \".join(temp_new_author_name.split())\n",
    "\n",
    "    author_names = new_author_name.split(\" \")\n",
    "    \n",
    "    if (author_name.title != '') : \n",
    "        final_author_name = new_author_name\n",
    "    else:\n",
    "        if len(author_names) == 1:\n",
    "            final_author_name = new_author_name\n",
    "        elif len(author_names) == 2:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        elif len(author_names) == 3:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[2]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[2]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\") & (author_names[2][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        elif len(author_names) == 4:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[2]) == 1) & (len(author_names[3]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[3]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[2]) == 2) & (len(author_names[3]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\") & (author_names[2][1]==\".\") & (author_names[3][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[3]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        else:\n",
    "            final_author_name = new_author_name\n",
    "    return final_author_name\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))  \n",
    "def remove_current_author(author, coauthors):\n",
    "    return [x for x in coauthors if x!=author][:250]\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def transform_name_for_search(name):\n",
    "    name = unidecode.unidecode(unicodedata.normalize('NFKC', name))\n",
    "    name = name.lower().replace(\" \", \" \").replace(\".\", \" \").replace(\",\", \" \").replace(\"|\", \" \").replace(\")\", \"\").replace(\"(\", \"\")\\\n",
    "        .replace(\"-\", \"\").replace(\"&\", \"\").replace(\"$\", \"\").replace(\"#\", \"\").replace(\"@\", \"\").replace(\"%\", \"\").replace(\"0\", \"\") \\\n",
    "        .replace(\"1\", \"\").replace(\"2\", \"\").replace(\"3\", \"\").replace(\"4\", \"\").replace(\"5\", \"\").replace(\"6\", \"\").replace(\"7\", \"\") \\\n",
    "        .replace(\"8\", \"\").replace(\"9\", \"\").replace(\"*\", \"\").replace(\"^\", \"\").replace(\"{\", \"\").replace(\"}\", \"\").replace(\"+\", \"\") \\\n",
    "        .replace(\"=\", \"\").replace(\"_\", \"\").replace(\"~\", \"\").replace(\"`\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\\\\", \"\") \\\n",
    "        .replace(\"<\", \"\").replace(\">\", \"\").replace(\"?\", \"\").replace(\"/\", \"\").replace(\";\", \"\").replace(\":\", \"\").replace(\"\\'\", \"\") \\\n",
    "        .replace(\"\\\"\", \"\")\n",
    "    name = \" \".join(name.split())\n",
    "    return name\n",
    "\n",
    "@udf(returnType=ArrayType(ArrayType(StringType())))\n",
    "def create_author_name_list_from_list(name_lists):\n",
    "    if not isinstance(name_lists, list):\n",
    "        name_lists = name_lists.tolist()\n",
    "    \n",
    "    name_list_len = len(name_lists[0])\n",
    "    \n",
    "    temp_name_list = [[j[i] for j in name_lists] for i in range(name_list_len)]\n",
    "    temp_name_list_2 = [[j[0] for j in i if j] for i in temp_name_list]\n",
    "    \n",
    "    return [list(set(x)) for x in temp_name_list_2]\n",
    "\n",
    "@udf(returnType=ArrayType(ArrayType(StringType())))\n",
    "def get_name_match_from_alternate_names(alt_names):\n",
    "    trans_names = list(set([transform_name_for_search_reg(x) for x in alt_names]))\n",
    "    name_lists = [get_name_match_list_reg(x) for x in trans_names]\n",
    "    return create_author_name_list_from_list_reg(name_lists)\n",
    "\n",
    "def create_author_name_list_from_list_reg(name_lists):\n",
    "    if not isinstance(name_lists, list):\n",
    "        name_lists = name_lists.tolist()\n",
    "    \n",
    "    name_list_len = len(name_lists[0])\n",
    "    \n",
    "    temp_name_list = [[j[i] for j in name_lists] for i in range(name_list_len)]\n",
    "    temp_name_list_2 = [[j[0] for j in i if j] for i in temp_name_list]\n",
    "    \n",
    "    return [list(set(x)) for x in temp_name_list_2]\n",
    "\n",
    "def transform_name_for_search_reg(name):\n",
    "    name = unidecode.unidecode(unicodedata.normalize('NFKC', name))\n",
    "    name = name.lower().replace(\" \", \" \").replace(\".\", \" \").replace(\",\", \" \").replace(\"|\", \" \").replace(\")\", \"\").replace(\"(\", \"\")\\\n",
    "        .replace(\"-\", \"\").replace(\"&\", \"\").replace(\"$\", \"\").replace(\"#\", \"\").replace(\"@\", \"\").replace(\"%\", \"\").replace(\"0\", \"\") \\\n",
    "        .replace(\"1\", \"\").replace(\"2\", \"\").replace(\"3\", \"\").replace(\"4\", \"\").replace(\"5\", \"\").replace(\"6\", \"\").replace(\"7\", \"\") \\\n",
    "        .replace(\"8\", \"\").replace(\"9\", \"\").replace(\"*\", \"\").replace(\"^\", \"\").replace(\"{\", \"\").replace(\"}\", \"\").replace(\"+\", \"\") \\\n",
    "        .replace(\"=\", \"\").replace(\"_\", \"\").replace(\"~\", \"\").replace(\"`\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\\\\", \"\") \\\n",
    "        .replace(\"<\", \"\").replace(\">\", \"\").replace(\"?\", \"\").replace(\"/\", \"\").replace(\";\", \"\").replace(\":\", \"\").replace(\"\\'\", \"\") \\\n",
    "        .replace(\"\\\"\", \"\")\n",
    "    name = \" \".join(name.split())\n",
    "    return name\n",
    "\n",
    "def get_name_match_list_reg(name):\n",
    "    name_split_1 = name.replace(\"-\", \"\").split()\n",
    "    name_split_2 = \"\"\n",
    "    if \"-\" in name:\n",
    "        name_split_2 = name.replace(\"-\", \" \").split()\n",
    "\n",
    "    fn = []\n",
    "    fni = []\n",
    "    \n",
    "    m1 = []\n",
    "    m1i = []\n",
    "    m2 = []\n",
    "    m2i = []\n",
    "    m3 = []\n",
    "    m3i = []\n",
    "    m4 = []\n",
    "    m4i = []\n",
    "    m5 = []\n",
    "    m5i = []\n",
    "\n",
    "    ln = []\n",
    "    lni = []\n",
    "    for name_split in [name_split_1, name_split_2]:\n",
    "        if len(name_split) == 0:\n",
    "            pass\n",
    "        elif len(name_split) == 1:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[0]) > 1:\n",
    "                ln.append(name_split[0])\n",
    "                lni.append(name_split[0][0])\n",
    "            else:\n",
    "                lni.append(name_split[0][0])\n",
    "            \n",
    "        elif len(name_split) == 2:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 3:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 4:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 5:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "                \n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 6:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "            \n",
    "            if len(name_split[4]) > 1:\n",
    "                m4.append(name_split[4])\n",
    "                m4i.append(name_split[4][0])\n",
    "            else:\n",
    "                m4i.append(name_split[4][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 7:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "            \n",
    "            if len(name_split[4]) > 1:\n",
    "                m4.append(name_split[4])\n",
    "                m4i.append(name_split[4][0])\n",
    "            else:\n",
    "                m4i.append(name_split[4][0])\n",
    "\n",
    "            if len(name_split[5]) > 1:\n",
    "                m5.append(name_split[5])\n",
    "                m5i.append(name_split[5][0])\n",
    "            else:\n",
    "                m5i.append(name_split[5][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        else:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "                \n",
    "            if len(name_split[4]) > 1:\n",
    "                m4.append(name_split[4])\n",
    "                m4i.append(name_split[4][0])\n",
    "            else:\n",
    "                m4i.append(name_split[4][0])\n",
    "\n",
    "            joined_names = \" \".join(name_split[5:-1])\n",
    "            m5.append(joined_names)\n",
    "            m5i.append(joined_names[0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "            \n",
    "\n",
    "    return [list(set(x)) for x in [fn,fni,m1,m1i,m2,m2i,m3,m3i,m4,m4i,m5,m5i,ln,lni]]\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def get_most_frequent_name(x):\n",
    "    return mode(x)\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def get_unique_orcid_for_author_table(list_of_orcids):\n",
    "    if not isinstance(list_of_orcids, list):\n",
    "        try:\n",
    "            list_of_orcids = list_of_orcids.tolist()\n",
    "        except:\n",
    "            list_of_orcids = list(list_of_orcids)\n",
    "        \n",
    "    orcids = [x for x in list_of_orcids if x]\n",
    "    \n",
    "    if orcids:\n",
    "        return orcids[0]\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "@udf(returnType=IntegerType())\n",
    "def check_for_unique_orcid_live_clustering(list_of_orcids):\n",
    "    if not isinstance(list_of_orcids, list):\n",
    "        try:\n",
    "            list_of_orcids = list_of_orcids.tolist()\n",
    "        except:\n",
    "            list_of_orcids = list(list_of_orcids)\n",
    "        \n",
    "    orcids = [x for x in list_of_orcids if x]\n",
    "    \n",
    "    if len(orcids) > 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d17c69a3-b2d9-4a27-8ac9-422fcbaf3818",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_data_features_scored(df, prefix):\n",
    "    df \\\n",
    "        .withColumn('row_label', F.concat_ws(\"|\", F.col('work_author_id'), F.col('work_author_id_2'))) \\\n",
    "        .withColumn('work_in_citations_2', F.array_contains(F.col('citations_2'), F.col('paper_id')).cast(IntegerType())) \\\n",
    "        .withColumn('work_2_in_citations', F.array_contains(F.col('citations'), F.col('paper_id_2')).cast(IntegerType())) \\\n",
    "        .withColumn('citation_work_match', F.when((F.col('work_2_in_citations')==1) | \n",
    "                                                  (F.col('work_in_citations_2')==1), 1).otherwise(0)) \\\n",
    "        .withColumn('insts_inter', F.size(F.array_intersect(F.col('institutions'), F.col('institutions_2')))) \\\n",
    "        .withColumn('coauths_inter', F.size(F.array_intersect(F.col('coauthors_shorter'), F.col('coauthors_shorter_2')))) \\\n",
    "        .withColumn('concps_inter', F.size(F.array_intersect(F.col('concepts_shorter'), F.col('concepts_shorter_2')))) \\\n",
    "        .withColumn('cites_inter', F.size(F.array_intersect(F.col('citations'), F.col('citations_2')))) \\\n",
    "        .withColumn('coauths_union', F.size(F.array_union(F.col('coauthors_shorter'), F.col('coauthors_shorter_2')))) \\\n",
    "        .withColumn('concps_union', F.size(F.array_union(F.col('concepts_shorter'), F.col('concepts_shorter_2')))) \\\n",
    "        .withColumn('cites_union', F.size(F.array_union(F.col('citations'), F.col('citations_2')))) \\\n",
    "        .withColumn('inst_per', F.when(F.col('insts_inter')>0, 1).otherwise(0)) \\\n",
    "        .withColumn('coauthors_shorter_per', F.round(F.when(F.col('coauths_union')>0, \n",
    "                                                            F.col('coauths_inter')/F.col('coauths_union')).otherwise(0.0), 4)) \\\n",
    "        .withColumn('concepts_shorter_per', F.round(F.when(F.col('concps_union')>0, \n",
    "                                                           F.col('concps_inter')/F.col('concps_union')).otherwise(0.0), 4)) \\\n",
    "        .withColumn('citation_per', F.round(F.when(F.col('cites_union')>0, \n",
    "                                                   F.col('cites_inter')/F.col('cites_union')).otherwise(0.0), 4)) \\\n",
    "        .withColumn('exact_match', F.when(F.col('author')==F.col('author_2'), 1).otherwise(0)) \\\n",
    "        .withColumn('name_len', F.length(F.col('author'))) \\\n",
    "        .withColumn('name_spaces', F.size(F.split(F.col('author'), \" \"))) \\\n",
    "        .select(F.col('work_author_id').alias('block'),'row_label', 'inst_per','concepts_shorter_per', 'coauthors_shorter_per', \n",
    "            (F.col('exact_match')*F.col('name_len')).alias('exact_match_len'),\n",
    "            (F.col('exact_match')*F.col('name_spaces')).alias('exact_match_spaces'), 'citation_per', 'citation_work_match') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}{prefix}all_features/\")\n",
    "\n",
    "    print('features saved: ', spark.read.parquet(f\"{temp_save_path}{prefix}all_features/\").count())\n",
    "        \n",
    "    spark.read.parquet(f\"{temp_save_path}{prefix}all_features/\")\\\n",
    "        .withColumn('random_int', get_random_int_udf(F.col('block'))) \\\n",
    "        .withColumn('concat_cols', F.array(F.col('block'), F.col('row_label').cast(StringType()), \n",
    "                                            F.col('inst_per').cast(StringType()), \n",
    "                                            F.col('concepts_shorter_per').cast(StringType()), \n",
    "                                            F.col('coauthors_shorter_per').cast(StringType()), \n",
    "                                            F.col('exact_match_len').cast(StringType()), \n",
    "                                            F.col('exact_match_spaces').cast(StringType()), \n",
    "                                            F.col('citation_per').cast(StringType()), \n",
    "                                            F.col('citation_work_match').cast(StringType()))) \\\n",
    "        .groupby('random_int') \\\n",
    "        .agg(F.collect_list(F.col('concat_cols')).alias('data_to_score')) \\\n",
    "        .withColumn('scored_data', score_data(F.col('data_to_score'))) \\\n",
    "        .select('scored_data') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}{prefix}data_scored/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74472631-352f-4096-9bb7-ed0ebe0d4001",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def live_clustering_algorithm(scored_data_prefix):\n",
    "    w1 = Window.partitionBy('work_author_id').orderBy(F.col('score').desc())\n",
    "    w2 = Window.partitionBy('author_id').orderBy(F.col('score').desc())\n",
    "\n",
    "    \n",
    "    spark.read.parquet(f\"{temp_save_path}{scored_data_prefix}data_scored/\") \\\n",
    "        .select(F.explode('scored_data').alias('scored_data')) \\\n",
    "        .select(F.col('scored_data').getItem(0).alias('work_author_id'),\n",
    "                F.col('scored_data').getItem(1).alias('pairs'), \n",
    "                F.col('scored_data').getItem(2).alias('score').cast(FloatType())) \\\n",
    "        .dropDuplicates(subset=['pairs']) \\\n",
    "        .select('work_author_id', \n",
    "                F.split(F.col('pairs'), \"\\|\")[1].alias('work_author_id_2'), \n",
    "                'score') \\\n",
    "        .repartition(250) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}{scored_data_prefix}flat_scored_data/\")\n",
    "    \n",
    "    spark.read.parquet(f\"{temp_save_path}{scored_data_prefix}flat_scored_data/\") \\\n",
    "        .join(all_new_data.select('work_author_id','orcid','author'), \n",
    "              how='inner', on='work_author_id') \\\n",
    "        .join(temp_authors_table.select('work_author_id_2','author_id','orcid_2').distinct(), \n",
    "              how='inner',on='work_author_id_2') \\\n",
    "        .filter((F.col('orcid')==F.col('orcid_2')) | \n",
    "        (F.col('orcid')=='') | \n",
    "        (F.col('orcid_2')=='')) \\\n",
    "        .withColumn('rank', F.row_number().over(w1)) \\\n",
    "        .filter(F.col('rank')==1) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}{scored_data_prefix}potential_cluster_matches/\")\n",
    "\n",
    "    pot_cluster_matches = spark.read.parquet(f\"{temp_save_path}{scored_data_prefix}potential_cluster_matches/\")\n",
    "\n",
    "    orcids_check = pot_cluster_matches\\\n",
    "        .groupby('author_id')\\\n",
    "        .agg(F.collect_set(F.col('orcid')).alias('orcids')) \\\n",
    "        .withColumn('orcid_good', check_for_unique_orcid_live_clustering('orcids')) \\\n",
    "        .select('author_id','orcid_good') \\\n",
    "        .alias('orcids_check')\n",
    "\n",
    "    pot_cluster_matches \\\n",
    "        .join(orcids_check.filter(F.col('orcid_good')==1).select('author_id').distinct(), how='inner', on='author_id')\\\n",
    "        .select('work_author_id', 'author_id') \\\n",
    "        .dropDuplicates(subset=['work_author_id']) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}{scored_data_prefix}matched_to_cluster/orcids_good/\")\n",
    "\n",
    "    pot_cluster_matches \\\n",
    "        .join(orcids_check.filter(F.col('orcid_good')==0).select('author_id').distinct(), how='inner', on='author_id')\\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}{scored_data_prefix}orcids_not_good/\")\n",
    "\n",
    "    spark.read.parquet(f\"{temp_save_path}{scored_data_prefix}orcids_not_good/\") \\\n",
    "        .withColumn('rank', F.row_number().over(w2)) \\\n",
    "        .filter(F.col('rank')==1) \\\n",
    "        .select('work_author_id', 'author_id') \\\n",
    "        .dropDuplicates(subset=['work_author_id']) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}{scored_data_prefix}matched_to_cluster/orcids_not_good/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb982ef3-8ea0-40c6-bf43-647ec28551ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_new_features_table(new_rows_location):\n",
    "    new_rows = spark.read.parquet(f\"{temp_save_path}/new_rows_for_author_table/{new_rows_location}/\") \\\n",
    "        .dropDuplicates()\n",
    "\n",
    "    temp_features_table \\\n",
    "        .union(all_new_data.join(new_rows.select('work_author_id').dropDuplicates(), how='inner', on='work_author_id') \\\n",
    "                .select(F.col('work_author_id').alias('work_author_id_2'), \n",
    "                        F.col('orcid').alias('orcid_2'),\n",
    "                        F.col('citations').alias('citations_2'),\n",
    "                        F.col('institutions').alias('institutions_2'),\n",
    "                        F.col('author').alias('author_2'),\n",
    "                        F.col('paper_id').alias('paper_id_2'),\n",
    "                        'original_author',\n",
    "                        F.col('concepts_shorter').alias('concepts_shorter_2'),\n",
    "                        F.col('coauthors_shorter').alias('coauthors_shorter_2'))) \\\n",
    "        .dropDuplicates(subset=['work_author_id_2']) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/temp_features_table/{new_rows_location}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6981c380-f863-4222-96dc-6a14da916cc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_new_author_table(new_rows_location):\n",
    "    new_rows = spark.read.parquet(f\"{temp_save_path}/new_rows_for_author_table/{new_rows_location}/\")\n",
    "\n",
    "    cluster_df = new_rows.union(temp_authors_table.select(F.col('work_author_id_2').alias('work_author_id'), 'author_id'))\n",
    "\n",
    "    # need to join new rows with features table\n",
    "    temp_features_table \\\n",
    "        .select(F.col('work_author_id_2').alias('work_author_id'),F.col('orcid_2').alias('orcid'),\n",
    "                'original_author',F.col('author_2').alias('author')) \\\n",
    "        .join(cluster_df, how='inner', on='work_author_id') \\\n",
    "        .filter(F.col('original_author')!=\"\") \\\n",
    "        .filter(F.col('original_author').isNotNull()) \\\n",
    "        .groupby('author_id') \\\n",
    "        .agg(F.collect_set(F.col('orcid')).alias('orcid'), \n",
    "            F.collect_set(F.col('work_author_id')).alias('work_author_id'),\n",
    "            F.collect_set(F.col('author')).alias('alternate_names'),\n",
    "            F.collect_set(F.col('author')).alias('names_for_list'),\n",
    "            F.collect_list(F.col('original_author')).alias('names')) \\\n",
    "        .withColumn('orcid', get_unique_orcid_for_author_table(F.col('orcid'))) \\\n",
    "        .withColumn('display_name', get_most_frequent_name(F.col('names'))) \\\n",
    "        .withColumn('name_match_list', get_name_match_from_alternate_names('names_for_list')) \\\n",
    "        .select(F.explode('work_author_id').alias('work_author_id_2'), \n",
    "                'author_id',\n",
    "                F.col('orcid').alias('orcid_2'), \n",
    "                'display_name',\n",
    "                'alternate_names',\n",
    "                'name_match_list') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/temp_authors_table/{new_rows_location}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07806f6d-4366-42ae-b5c6-42ab70224263",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Load Transformed New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71870967-54f3-4c0b-8078-05bbc1e29abf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_new_data = spark.read.parquet(f\"{temp_save_path}/new_data_to_disambiguate/\") \\\n",
    "    .dropDuplicates(subset=['work_author_id'])\n",
    "all_new_data.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4289dd5e-5bd9-4a71-b935-0e9b5d55d422",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "init_cluster_df = spark.read.parquet(f\"{prod_save_path}/current_authors_table/\")\\\n",
    "    .select('work_author_id',('author_id')) \\\n",
    "    .join(all_new_data.select('work_author_id'), how='leftanti', on='work_author_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ded6f42-8765-41c2-9fe0-9b94405fab9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Get current features table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abc034bd-644e-42a6-be91-b31242d0467a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{prod_save_path}/current_features_table/\") \\\n",
    "    .join(init_cluster_df.select(F.col('work_author_id').alias('work_author_id_2')), how='inner', on='work_author_id_2') \\\n",
    "    .select('work_author_id_2', 'orcid_2', F.col('citations_2').cast(ArrayType(LongType())), \n",
    "            'institutions_2', 'author_2', F.col('paper_id_2').cast(LongType()), 'original_author',\n",
    "            F.col('concepts_shorter_2').cast(ArrayType(LongType())), 'coauthors_shorter_2') \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/temp_features_table/init/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb524a3a-29aa-4902-9f66-626f10965099",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_features_table = spark.read.parquet(f\"{temp_save_path}/temp_features_table/init/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50b98b7b-05f3-4aeb-8b12-3bb6a2f197eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Get current authors table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e6bbb60-81c7-4e7c-b085-869a9790eaf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_features_table \\\n",
    "    .select(F.col('work_author_id_2').alias('work_author_id'),F.col('orcid_2').alias('orcid'),\n",
    "            'original_author',F.col('author_2').alias('author')) \\\n",
    "    .join(init_cluster_df, how='inner', on='work_author_id') \\\n",
    "    .filter(F.col('original_author')!=\"\") \\\n",
    "    .filter(F.col('original_author').isNotNull()) \\\n",
    "    .groupby('author_id') \\\n",
    "    .agg(F.collect_set(F.col('orcid')).alias('orcid'), \n",
    "         F.collect_set(F.col('work_author_id')).alias('work_author_id'),\n",
    "         F.collect_set(F.col('author')).alias('alternate_names'),\n",
    "         F.collect_set(F.col('author')).alias('names_for_list'),\n",
    "         F.collect_list(F.col('original_author')).alias('names')) \\\n",
    "    .withColumn('orcid', get_unique_orcid_for_author_table(F.col('orcid'))) \\\n",
    "    .withColumn('display_name', get_most_frequent_name(F.col('names'))) \\\n",
    "    .withColumn('name_match_list', get_name_match_from_alternate_names('names_for_list')) \\\n",
    "    .select(F.explode('work_author_id').alias('work_author_id_2'), \n",
    "            'author_id',\n",
    "            F.col('orcid').alias('orcid_2'), \n",
    "            'display_name',\n",
    "            'alternate_names',\n",
    "            'name_match_list') \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/temp_authors_table/init/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdcadccc-0645-489c-b4f8-7d09c7c70439",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_authors_table = spark.read.parquet(f\"{temp_save_path}/temp_authors_table/init/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bbbea19-f7c9-460d-a635-643b637846e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Get current names matching table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "655d5c84-761d-4fe6-97f0-bcdcfb3d13f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "author_names_match = spark.read.parquet(f\"{prod_save_path}/current_author_names_match/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddda3b1e-dd0c-4383-ada9-dabdf96813b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Check if work_author_id has been disambiguated before (shows up in final authors table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c1960ce-34bb-4e76-9f2c-3c08c7a82631",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{prod_save_path}/current_authors_table/\")\\\n",
    "    .select('work_author_id','author_id') \\\n",
    "    .join(all_new_data.select('work_author_id'), how='inner', on='work_author_id') \\\n",
    "    .select('work_author_id',F.col('author_id').alias('author_id_old')) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/temp_authors_to_change_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23b9fba3-3db1-459a-96b9-be4eaffa4dee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{temp_save_path}/temp_authors_to_change_table/\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93b5e1c7-6da5-4aa6-a82b-14a51ef423bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Need to add logic later on which will perform a set of actions if a work_author is getting disambiguated again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3043dfbf-037e-408b-8595-967c2e1c06fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Check for merges/changes to the final author table since last update (author merges, work removals/adds)\n",
    "\n",
    "* for work_author_ids that were removed and need a new cluster, keep a record of clusters it should not match to\n",
    "* need to make sure that when it goes through disambiguation again, it can't match back up to the same cluster\n",
    "* for removals, work will be taken away from the author quickly but it will go through the full disambiguation process during the next round (won't be a separate process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43d82d25-f4db-4e97-a89e-be65188f285c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Checking if ORCID matches to a cluster with an ORCID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8143f76e-7143-47b8-b566-b1c9765dffd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_new_data.filter(F.col('orcid')!='')\\\n",
    "    .join(temp_authors_table.select(F.col('orcid_2').alias('orcid'),'author_id'),how='inner', on='orcid') \\\n",
    "    .select('work_author_id', \n",
    "            'author_id') \\\n",
    "    .dropDuplicates(subset=['work_author_id']) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/new_rows_for_author_table/orcid_rows_for_author_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c328da99-8358-48e2-b0da-6689d0febfaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_loc = 'orcid_rows_for_author_table'\n",
    "_ = create_new_features_table(new_loc)\n",
    "print(\"New features table created\")\n",
    "temp_features_table = spark.read.parquet(f\"{temp_save_path}/temp_features_table/{new_loc}/\")\n",
    "\n",
    "_ = create_new_author_table(new_loc)\n",
    "print(\"New authors table created\")\n",
    "temp_authors_table = spark.read.parquet(f\"{temp_save_path}/temp_authors_table/{new_loc}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db70c1ba-6c60-4414-b6b4-2e13abf9d2cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{temp_save_path}/new_rows_for_author_table/orcid_rows_for_author_table/\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e293d57c-80a1-4375-b4b6-0947e0549f8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_new_data \\\n",
    "    .join(temp_authors_table.select(F.col('work_author_id_2').alias('work_author_id')).distinct(), \n",
    "                  how='leftanti', on='work_author_id') \\\n",
    "    .select('work_author_id','paper_id','original_author','author','orcid','coauthors_shorter','concepts_shorter',\n",
    "        'institutions','citations','created_date') \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/round_2_of_clustering/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "695bc213-4cdc-43e8-b979-dc6a7c94dccd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Check if name has been previously parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "551db773-da0c-41b0-ba9c-4f75da8a82d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    work_to_clusters_removed = spark.read.parquet(f\"{prod_save_path}/works_removed_from_clusters/\")\n",
    "except:\n",
    "    work_to_clusters_removed = spark.sparkContext.emptyRDD().toDF(schema=StructType([StructField(\"work_author_id\", StringType()),\n",
    "                                                                                     StructField(\"author_id\", LongType())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78700070-d20b-4608-917f-8211ba336a3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "round_2_new_data = spark.read.parquet(f\"{temp_save_path}/round_2_of_clustering/\")\n",
    "\n",
    "names_match = round_2_new_data \\\n",
    "    .withColumn('paper_id', F.split(F.col('work_author_id'), \"_\").getItem(0).cast(LongType())) \\\n",
    "    .join(temp_authors_table.select('work_author_id_2', \n",
    "                                    'orcid_2', \n",
    "                                    'author_id',\n",
    "                                    F.explode(F.col('alternate_names')).alias('author')),\n",
    "          how='inner', on='author') \\\n",
    "    .join(work_to_clusters_removed, how='leftanti', on=['work_author_id','author_id']) \\\n",
    "    .filter((F.col('orcid')==F.col('orcid_2')) | \n",
    "            (F.col('orcid')=='') | \n",
    "            (F.col('orcid_2')=='')) \\\n",
    "    .join(temp_features_table.drop(\"orcid_2\"), how='inner', on='work_author_id_2')\n",
    "\n",
    "# prepare data for model scoring and score\n",
    "_ = get_data_features_scored(names_match, \"/names_match/\")\n",
    "\n",
    "# send through clustering/matching algorithm\n",
    "_ = live_clustering_algorithm(\"/names_match/\")\n",
    "\n",
    "# save new author table rows to file\n",
    "spark.read.parquet(f\"{temp_save_path}/names_match/matched_to_cluster/*\") \\\n",
    "    .select('work_author_id', \n",
    "            'author_id') \\\n",
    "    .dropDuplicates(subset=['work_author_id']) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/new_rows_for_author_table/name_match_rows_for_author_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c355fe13-f5c9-4b5f-b2a2-69027fce5b93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_authors_table.groupBy('author_id')\\\n",
    "        .agg(F.collect_set(F.col('orcid_2')).alias('orcids')) \\\n",
    "        .withColumn('orcid_good', check_for_unique_orcid_live_clustering('orcids')) \\\n",
    "        .filter(F.col('orcid_good')==0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07dfc822-8257-40e5-aea4-14a7d337b5c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_loc = 'name_match_rows_for_author_table'\n",
    "_ = create_new_features_table(new_loc)\n",
    "print(\"New features table created\")\n",
    "temp_features_table = spark.read.parquet(f\"{temp_save_path}/temp_features_table/{new_loc}/\")\n",
    "\n",
    "_ = create_new_author_table(new_loc)\n",
    "print(\"New authors table created\")\n",
    "temp_authors_table = spark.read.parquet(f\"{temp_save_path}/temp_authors_table/{new_loc}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e959e19f-5c2e-4533-b929-e55a725191e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_authors_table.groupBy('author_id')\\\n",
    "        .agg(F.collect_set(F.col('orcid_2')).alias('orcids')) \\\n",
    "        .withColumn('orcid_good', check_for_unique_orcid_live_clustering('orcids')) \\\n",
    "        .filter(F.col('orcid_good')==0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8009660-2abe-44b4-b47f-ce5aef752332",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_new_data \\\n",
    "    .join(temp_authors_table.select(F.col('work_author_id_2').alias('work_author_id')).distinct(), \n",
    "                  how='leftanti', on='work_author_id') \\\n",
    "    .select('work_author_id','paper_id','original_author','author','orcid','coauthors_shorter','concepts_shorter',\n",
    "        'institutions','citations','created_date') \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/round_3_of_clustering/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06a460eb-9f24-4d6b-bced-8b3d5753c9e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Run \"previously-parsed\" code one more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1650511c-dc68-4db3-a6ca-6f68fcd4458f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "round_3_new_data = spark.read.parquet(f\"{temp_save_path}/round_3_of_clustering/\")\n",
    "\n",
    "print(round_3_new_data.count())\n",
    "\n",
    "names_match_2 = round_3_new_data \\\n",
    "    .withColumn('paper_id', F.split(F.col('work_author_id'), \"_\").getItem(0).cast(LongType())) \\\n",
    "    .join(temp_authors_table.select('work_author_id_2', \n",
    "                                    'orcid_2', \n",
    "                                    'author_id',\n",
    "                                    F.explode(F.col('alternate_names')).alias('author')),\n",
    "          how='inner', on='author') \\\n",
    "    .join(work_to_clusters_removed, how='leftanti', on=['work_author_id','author_id']) \\\n",
    "    .filter((F.col('orcid')==F.col('orcid_2')) | \n",
    "            (F.col('orcid')=='') | \n",
    "            (F.col('orcid_2')=='')) \\\n",
    "    .join(temp_features_table.drop(\"orcid_2\"), how='inner', on='work_author_id_2')\n",
    "\n",
    "# prepare data for model scoring and score\n",
    "_ = get_data_features_scored(names_match_2, \"/names_match_2/\")\n",
    "\n",
    "# send through clustering/matching algorithm\n",
    "_ = live_clustering_algorithm(\"/names_match_2/\")\n",
    "\n",
    "# save new author table rows to file\n",
    "spark.read.parquet(f\"{temp_save_path}/names_match_2/matched_to_cluster/*\") \\\n",
    "    .select('work_author_id', \n",
    "            'author_id') \\\n",
    "    .dropDuplicates(subset=['work_author_id']) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/new_rows_for_author_table/name_match_rows_for_author_table_2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2398f23e-48de-4804-93b9-e979ecd1a9e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{temp_save_path}/names_match_2/matched_to_cluster/*\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb4967bd-1554-4e09-bd37-8dad86790c88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_loc = 'name_match_rows_for_author_table_2'\n",
    "_ = create_new_features_table(new_loc)\n",
    "print(\"New features table created\")\n",
    "temp_features_table = spark.read.parquet(f\"{temp_save_path}/temp_features_table/{new_loc}/\")\n",
    "\n",
    "_ = create_new_author_table(new_loc)\n",
    "print(\"New authors table created\")\n",
    "temp_authors_table = spark.read.parquet(f\"{temp_save_path}/temp_authors_table/{new_loc}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77387202-b27c-4f57-b34c-8b9d20a617ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_features_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d5021e1-4b9d-4fc1-9a24-f264d8bc4c33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_authors_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57ef1e58-a16c-44ef-af85-b4b40d117afa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_authors_table.groupBy('author_id')\\\n",
    "        .agg(F.collect_set(F.col('orcid_2')).alias('orcids')) \\\n",
    "        .withColumn('orcid_good', check_for_unique_orcid_live_clustering('orcids')) \\\n",
    "        .filter(F.col('orcid_good')==0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1a157cb-019d-4a5d-aa31-1cf1b1ee88ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_new_data \\\n",
    "    .join(temp_authors_table.select(F.col('work_author_id_2').alias('work_author_id')).distinct(), \n",
    "                  how='leftanti', on='work_author_id') \\\n",
    "    .select('work_author_id','paper_id','original_author','author','orcid','coauthors_shorter','concepts_shorter',\n",
    "        'institutions','citations','created_date') \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/round_4_of_clustering/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5553143c-2f71-424a-9cd9-d77c5aa954f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Not previously parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ad66f58-f9e1-45df-beef-7e792444fb49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{temp_save_path}/round_4_of_clustering/\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceb2988e-3d74-4f15-ac04-bad049fb2578",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "round_4_new_data = spark.read.parquet(f\"{temp_save_path}/round_4_of_clustering/\") \\\n",
    "    .filter(F.col('author').isNotNull()) \\\n",
    "    .filter(F.col('author')!='') \\\n",
    "    .withColumn('non_latin_groups', group_non_latin_characters(F.col('author'))) \\\n",
    "    .withColumn('name_to_keep_ind', name_to_keep_ind('non_latin_groups'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30dbd914-1bee-4e72-aa42-39f8b41f262e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "round_4_new_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba77ca0-0912-461b-84dc-9b02e14196dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "round_4_new_data \\\n",
    "    .filter(F.col('name_to_keep_ind')==1) \\\n",
    "    .withColumn('transformed_search_name', transform_name_for_search(F.col('author'))) \\\n",
    "    .withColumn('name_len', F.length(F.col('transformed_search_name'))) \\\n",
    "    .filter(F.col('name_len')>1) \\\n",
    "    .withColumn('name_match_list', get_name_match_list(F.col('transformed_search_name'))) \\\n",
    "    .withColumn('block', only_get_last(F.col('transformed_search_name'))) \\\n",
    "    .select('work_author_id','orcid','name_match_list','transformed_search_name', 'block') \\\n",
    "    .withColumn('block_removed', F.expr(\"regexp_replace(transformed_search_name, block, '')\")) \\\n",
    "    .withColumn('new_block_removed', F.trim(F.expr(\"regexp_replace(block_removed, '  ', ' ')\"))) \\\n",
    "    .withColumn('letter', get_starting_letter(F.col('new_block_removed'))) \\\n",
    "    .select('work_author_id','orcid','name_match_list','transformed_search_name','letter', 'block') \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/for_new_authors_table/names_to_blocks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "736a483a-f957-429f-ba56-c1b53e384ded",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "no_names_match = spark.read.parquet(f\"{temp_save_path}/for_new_authors_table/names_to_blocks/\")\n",
    "no_names_match.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d894d5b5-0bba-4bd3-b194-527e21fc35ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# join those names to authors table alternate names to get work_author_ids to check\n",
    "full_no_names_match_table = no_names_match \\\n",
    "    .join(author_names_match, how='inner', on=['block','letter']) \\\n",
    "    .withColumn('matched_names', check_block_vs_block(F.col('name_match_list'), F.col('name_match_list_2'))) \\\n",
    "    .filter(F.col('matched_names')==1) \\\n",
    "    .select('work_author_id', 'work_author_id_2') \\\n",
    "    .dropDuplicates() \\\n",
    "    .join(round_4_new_data, how='inner', on='work_author_id') \\\n",
    "    .join(temp_features_table, how='inner', on='work_author_id_2') \\\n",
    "    .filter((F.col('orcid')==F.col('orcid_2')) | \n",
    "        (F.col('orcid')=='') | \n",
    "        (F.col('orcid_2')==''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f25c460-11da-4813-8461-f2a0595e4b28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prepare data for model scoring and score\n",
    "_ = get_data_features_scored(full_no_names_match_table, \"/no_names_match/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398177eb-5027-474e-9717-6fd41a83a179",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# send through clustering/matching algorithm\n",
    "_ = live_clustering_algorithm(\"/no_names_match/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fae29ce3-d143-45cd-a075-ed07564ec331",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save new author table rows to file\n",
    "spark.read.parquet(f\"{temp_save_path}/no_names_match/matched_to_cluster/*\") \\\n",
    "    .select('work_author_id', \n",
    "            'author_id') \\\n",
    "    .dropDuplicates(subset=['work_author_id']) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/new_rows_for_author_table/no_name_match_rows_for_author_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d1e195-22bf-44d5-bea9-faedef4b7673",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{temp_save_path}/new_rows_for_author_table/no_name_match_rows_for_author_table/\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7af98d1f-4aa2-48b0-9be1-2477150b307d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_loc = 'no_name_match_rows_for_author_table'\n",
    "_ = create_new_features_table(new_loc)\n",
    "print(\"New features table created\")\n",
    "temp_features_table = spark.read.parquet(f\"{temp_save_path}/temp_features_table/{new_loc}/\")\n",
    "\n",
    "_ = create_new_author_table(new_loc)\n",
    "print(\"New authors table created\")\n",
    "temp_authors_table = spark.read.parquet(f\"{temp_save_path}/temp_authors_table/{new_loc}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d6ff21a-38d3-49b7-9fe9-641de35f6888",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_features_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e7abe46-e3f5-4bc1-81e2-944062ade8d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_authors_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1866e13-afdf-42c5-a3ce-40cd6c1d6675",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_authors_table.groupBy('author_id')\\\n",
    "        .agg(F.collect_set(F.col('orcid_2')).alias('orcids')) \\\n",
    "        .withColumn('orcid_good', check_for_unique_orcid_live_clustering('orcids')) \\\n",
    "        .filter(F.col('orcid_good')==0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b7587e0-5921-4f43-8973-4da0818ab781",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Save all non-clustered data to new location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48dce7c7-9c75-4669-97df-3dc0517c85d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_new_data \\\n",
    "    .join(temp_authors_table.select(F.col('work_author_id_2').alias('work_author_id')).distinct(), \n",
    "                  how='leftanti', on='work_author_id') \\\n",
    "    .select('work_author_id','paper_id','original_author','author','orcid','coauthors_shorter','concepts_shorter',\n",
    "        'institutions','citations','created_date') \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/end_of_clustering_leftovers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ba0d18-d8ed-4207-89d9-9c8400b3963a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{temp_save_path}/end_of_clustering_leftovers/\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f531326-5964-465a-a41b-1f1af284d795",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_id = int(temp_authors_table.select(F.max(F.col('author_id'))).collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc58ea0f-36e6-43db-a6b0-20cbce45ed66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f8cf5b1-e380-461c-9f56-5e1d2b83bd94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Give non-clustered data an author ID (new single cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4786d448-843f-4be3-b890-8121dc014e58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w1 = Window.orderBy(F.col('work_author_id'))\n",
    "\n",
    "spark.read.parquet(f\"{temp_save_path}/end_of_clustering_leftovers/\") \\\n",
    "    .select('work_author_id').distinct() \\\n",
    "    .withColumn('temp_cluster_num', F.row_number().over(w1)) \\\n",
    "    .withColumn('author_id', F.lit(max_id) + F.col('temp_cluster_num')) \\\n",
    "    .select('work_author_id','author_id') \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/new_rows_for_author_table/new_author_clusters/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "517ac229-7295-4ab5-9311-ac8b98e4fb5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{temp_save_path}/new_rows_for_author_table/new_author_clusters/\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a802f4c9-6dda-4703-8777-19a1a8a25d76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_loc = 'new_author_clusters'\n",
    "_ = create_new_features_table(new_loc)\n",
    "print(\"New features table created\")\n",
    "temp_features_table = spark.read.parquet(f\"{temp_save_path}/temp_features_table/{new_loc}/\")\n",
    "\n",
    "_ = create_new_author_table(new_loc)\n",
    "print(\"New authors table created\")\n",
    "temp_authors_table = spark.read.parquet(f\"{temp_save_path}/temp_authors_table/{new_loc}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c48857bb-b9aa-44a5-9947-196a79eff331",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_authors_table.groupBy('author_id')\\\n",
    "        .agg(F.collect_set(F.col('orcid_2')).alias('orcids')) \\\n",
    "        .withColumn('orcid_good', check_for_unique_orcid_live_clustering('orcids')) \\\n",
    "        .filter(F.col('orcid_good')==0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f79e36-05eb-408a-a4c3-8f44f41e6afa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_features_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cd40597-2768-4728-a41b-a660cf85e15c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_authors_table.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9de20706-8114-43e4-8298-0972eb9e70fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Generate final authors table to write to S3 and postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd543378-1377-4150-88ab-08b5b603ed93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=IntegerType())\n",
    "def check_list_vs_list(list_1, list_2):\n",
    "    set_1 = set(list_1)\n",
    "    set_2 = set(list_2)\n",
    "    if set_1 == set_2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "298337a0-f21c-4528-8cd9-451d35afcccd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "init_author_table = spark.read.parquet(f\"{prod_save_path}/current_authors_table/\") \\\n",
    "    .select('work_author_id', \n",
    "            F.col('author_id').alias('author_id_1'),\n",
    "            F.col('display_name').alias('display_name_1'),\n",
    "            F.col('alternate_names').alias('alternate_names_1'), \n",
    "            F.col('orcid').alias('orcid_1'),\n",
    "            'created_date',\n",
    "            'modified_date')\n",
    "\n",
    "final_author_table = spark.read.parquet(f\"{temp_save_path}/temp_authors_table/new_author_clusters/\") \\\n",
    "        .select(F.col('work_author_id_2').alias('work_author_id'), \n",
    "            F.col('author_id').alias('author_id_2'),\n",
    "            F.col('display_name').alias('display_name_2'),\n",
    "            F.col('alternate_names').alias('alternate_names_2'), \n",
    "            'orcid_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c26c8d5e-9ac0-44d9-920e-04724c6bd42e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "init_author_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b9c5e4-8aa7-4493-80c9-5012c226ec18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "init_author_table.dropDuplicates(subset=['work_author_id']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf207215-1ba2-41b2-be94-202000137ec4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_author_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b3431c7-81fb-47cf-8a3a-15010ab2f8be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_author_table.dropDuplicates(subset=['work_author_id']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0404bdb-8ca8-43e2-9335-47a6f08e78b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# take final author table, compare to init table (all columns) to see if anything has been changed\n",
    "compare_tables = final_author_table.join(init_author_table, how='inner', on='work_author_id') \\\n",
    "    .withColumn('orcid_compare', F.when(F.col('orcid_1')==F.col('orcid_2'), 0).otherwise(1)) \\\n",
    "    .withColumn('display_name_compare', F.when(F.col('display_name_1')==F.col('display_name_2'), 0).otherwise(1)) \\\n",
    "    .withColumn('author_id_compare', F.when(F.col('author_id_1')==F.col('author_id_2'), 0).otherwise(1)) \\\n",
    "    .withColumn('alternate_names_compare', check_list_vs_list(F.col('alternate_names_1'), F.col('alternate_names_2'))) \\\n",
    "    .withColumn('total_changes', F.col('orcid_compare') + F.col('display_name_compare') + \n",
    "                            F.col('author_id_compare') + F.col('alternate_names_compare'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b23c7a3-bc17-4ec4-9631-be6950ecd1e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if not, write out those rows to a folder\n",
    "compare_tables.filter(F.col('total_changes')==0) \\\n",
    "    .select('work_author_id', \n",
    "            F.col('author_id_1').alias('author_id'),\n",
    "            F.col('display_name_1').alias('display_name'),\n",
    "            F.col('alternate_names_1').alias('alternate_names'), \n",
    "            F.col('orcid_1').alias('orcid'),\n",
    "            'created_date',\n",
    "            'modified_date') \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/final_author_table_part/no_changes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8271b893-5093-49c2-bab5-2fb18ea11bff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if modified but not created, write out to different folder\n",
    "compare_tables.filter(F.col('total_changes')>0) \\\n",
    "    .select('work_author_id', \n",
    "            F.col('author_id_2').alias('author_id'),\n",
    "            F.col('display_name_2').alias('display_name'),\n",
    "            F.col('alternate_names_2').alias('alternate_names'), \n",
    "            F.col('orcid_2').alias('orcid'),\n",
    "            'created_date') \\\n",
    "    .withColumn(\"modified_date\", F.current_timestamp()) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/final_author_table_part/modified/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db743501-9f7a-4f3e-a1a1-1dc407c9a139",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if created, write out to different folder\n",
    "final_author_table.join(init_author_table.select('work_author_id'), how='leftanti', on='work_author_id') \\\n",
    "    .select('work_author_id', \n",
    "            F.col('author_id_2').alias('author_id'),\n",
    "            F.col('display_name_2').alias('display_name'),\n",
    "            F.col('alternate_names_2').alias('alternate_names'), \n",
    "            F.col('orcid_2').alias('orcid')) \\\n",
    "    .withColumn(\"created_date\", F.current_timestamp()) \\\n",
    "    .withColumn(\"modified_date\", F.current_timestamp()) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/final_author_table_part/created/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf68d2b-8066-47e6-ad2d-79c8728b9af9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"No row changes: \", spark.read.parquet(f\"{temp_save_path}/final_author_table_part/no_changes/\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "385a7c39-a259-48e8-aa4a-9a13c7eac23c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Modified rows: \", spark.read.parquet(f\"{temp_save_path}/final_author_table_part/modified/\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac712355-53e4-4d9a-a63f-c8bd2309864b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"New cluster rows: \", spark.read.parquet(f\"{temp_save_path}/final_author_table_part/created/\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4393ab87-97d8-4e8b-b937-618eff9b2f2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Total rows: \", spark.read.parquet(f\"{temp_save_path}/final_author_table_part/*\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d9da4c6-4e50-4229-8b0d-ffd080093165",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Save all final tables to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d513c1d6-2daa-4199-8a84-b4e496b0cea2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{temp_save_path}/final_author_table_part/*\") \\\n",
    "    .repartition(250) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{prod_save_path}/current_authors_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85b81ca0-b4cb-483c-809b-b1c146759180",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{temp_save_path}/temp_features_table/new_author_clusters/\") \\\n",
    "    .repartition(250) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{prod_save_path}/current_features_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f970686-e122-4137-bae2-4a461f00c250",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{prod_save_path}/current_features_table/\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69c4e35d-74a0-401d-b403-3d02dd35ad32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{temp_save_path}/temp_features_table/new_author_clusters/\") \\\n",
    "    .select('work_author_id_2', 'orcid_2', F.col('author_2').alias('transformed_name')) \\\n",
    "    .filter(F.col('transformed_name')!=\"\") \\\n",
    "    .filter(F.col('transformed_name').isNotNull()) \\\n",
    "    .withColumn('transformed_search_name', transform_name_for_search(F.col('transformed_name'))) \\\n",
    "    .withColumn('name_len', F.length(F.col('transformed_search_name'))) \\\n",
    "    .filter(F.col('name_len')>1) \\\n",
    "    .withColumn('name_match_list_2', get_name_match_list(F.col('transformed_search_name'))) \\\n",
    "    .withColumn('block', only_get_last(F.col('transformed_search_name'))) \\\n",
    "    .select('work_author_id_2','name_match_list_2', 'orcid_2', 'transformed_search_name', 'block')\\\n",
    "    .withColumn('block_removed', F.expr(\"regexp_replace(transformed_search_name, block, '')\")) \\\n",
    "    .withColumn('new_block_removed', F.trim(F.expr(\"regexp_replace(block_removed, '  ', ' ')\"))) \\\n",
    "    .withColumn('letter', get_starting_letter(F.col('new_block_removed'))) \\\n",
    "    .select('work_author_id_2','orcid_2','name_match_list_2', 'block', 'letter') \\\n",
    "    .dropDuplicates() \\\n",
    "    .repartition(250) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{prod_save_path}/current_author_names_match/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "802c1bf8-e698-4951-871d-cc56cf600846",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AND_System_v3_live",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
