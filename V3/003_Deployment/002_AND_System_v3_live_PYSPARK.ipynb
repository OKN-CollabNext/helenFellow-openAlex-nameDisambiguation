{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44920190-9120-4ae8-858f-14f52b073315",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "import datetime\n",
    "from statistics import mode\n",
    "from nameparser import HumanName\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba961a2-7e64-4844-8d78-c47e5f47c752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, ArrayType, DoubleType, StructType, StructField, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f29191d-1059-4a1a-a42b-0a11e8599c1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "curr_date = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "prod_save_path = \"<S3path>\"\n",
    "temp_save_path = f\"<S3path>/{curr_date}\"\n",
    "name_of_stats_to_track = []\n",
    "stats_to_track = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48537f80-c03a-4170-b5b6-e50abb2306ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Load Disambiguator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cd221af-a5eb-4e1a-8a0b-a49784900d1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(\"<path-to-model>/model.pkl\", \"rb\") as f:\n",
    "    disambiguator_model = pickle.load(f)\n",
    "\n",
    "broadcast_disambiguator_model = spark.sparkContext.broadcast(disambiguator_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a6936b8-e68a-4b21-bf74-b52e2123411e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Get Latest Data to Disambiguate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9aa07445-3a65-40d2-b491-2dacad7fc07e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_secret():\n",
    "    ### code for getting AWS secrets ###\n",
    "    \n",
    "    return secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcfc90cc-a80f-4196-9ed9-e8fa2993431a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "secret = get_secret()\n",
    "\n",
    "df = (spark.read\n",
    "    .format(\"postgresql\")\n",
    "    .option(\"dbtable\", \"<input-data-postgres-table>\")\n",
    "    .option(\"host\", secret['host'])\n",
    "    .option(\"port\", secret['port'])\n",
    "    .option(\"database\", secret['dbname'])\n",
    "    .option(\"user\", secret['username'])\n",
    "    .option(\"password\", secret['password'])\n",
    "    .option(\"partitionColumn\", \"partition\")\n",
    "    .option(\"lowerBound\", 0)\n",
    "    .option(\"upperBound\", 21)\n",
    "    .option(\"numPartitions\", 6)\n",
    "    .option(\"fetchSize\", \"15\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "df.write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/raw_data_to_disambiguate/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff0b8457-b001-4a2a-8a18-694727b4a65e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_count = spark.read.parquet(f\"{temp_save_path}/raw_data_to_disambiguate/\").count()\n",
    "name_of_stats_to_track.append('raw_data_count')\n",
    "stats_to_track.append(raw_count)\n",
    "print(raw_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "322b2469-676b-411c-a711-99993200785a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ac143a42-2ad6-4ec6-9222-c2ac11411e2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def transform_author_name(author):\n",
    "    if author.startswith(\"None \"):\n",
    "        author = author.replace(\"None \", \"\")\n",
    "    elif author.startswith(\"Array \"):\n",
    "        author = author.replace(\"Array \", \"\")\n",
    "\n",
    "    author = unicodedata.normalize('NFKC', author)\n",
    "    \n",
    "    author_name = HumanName(\" \".join(author.split()))\n",
    "\n",
    "    if (author_name.title == 'Dr.') | (author_name.title == ''):\n",
    "        temp_new_author_name = f\"{author_name.first} {author_name.middle} {author_name.last}\"\n",
    "    else:\n",
    "        temp_new_author_name = f\"{author_name.title} {author_name.first} {author_name.middle} {author_name.last}\"\n",
    "\n",
    "    new_author_name = \" \".join(temp_new_author_name.split())\n",
    "\n",
    "    author_names = new_author_name.split(\" \")\n",
    "    \n",
    "    if (author_name.title != '') : \n",
    "        final_author_name = new_author_name\n",
    "    else:\n",
    "        if len(author_names) == 1:\n",
    "            final_author_name = new_author_name\n",
    "        elif len(author_names) == 2:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        elif len(author_names) == 3:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[2]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[2]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\") & (author_names[2][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        elif len(author_names) == 4:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[2]) == 1) & (len(author_names[3]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[3]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[2]) == 2) & (len(author_names[3]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\") & (author_names[2][1]==\".\") & (author_names[3][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[3]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        else:\n",
    "            final_author_name = new_author_name\n",
    "    return final_author_name\n",
    "\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def transform_coauthors(coauthors):\n",
    "    return [transform_author_name_reg(x) for x in coauthors]\n",
    "\n",
    "def transform_author_name_reg(author):\n",
    "    if author.startswith(\"None \"):\n",
    "        author = author.replace(\"None \", \"\")\n",
    "    elif author.startswith(\"Array \"):\n",
    "        author = author.replace(\"Array \", \"\")\n",
    "\n",
    "    author = unicodedata.normalize('NFKC', author)\n",
    "    \n",
    "    author_name = HumanName(\" \".join(author.split()))\n",
    "\n",
    "    if (author_name.title == 'Dr.') | (author_name.title == ''):\n",
    "        temp_new_author_name = f\"{author_name.first} {author_name.middle} {author_name.last}\"\n",
    "    else:\n",
    "        temp_new_author_name = f\"{author_name.title} {author_name.first} {author_name.middle} {author_name.last}\"\n",
    "\n",
    "    new_author_name = \" \".join(temp_new_author_name.split())\n",
    "\n",
    "    author_names = new_author_name.split(\" \")\n",
    "    \n",
    "    if (author_name.title != '') : \n",
    "        final_author_name = new_author_name\n",
    "    else:\n",
    "        if len(author_names) == 1:\n",
    "            final_author_name = new_author_name\n",
    "        elif len(author_names) == 2:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        elif len(author_names) == 3:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[2]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[2]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\") & (author_names[2][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        elif len(author_names) == 4:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[2]) == 1) & (len(author_names[3]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[3]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[2]) == 2) & (len(author_names[3]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\") & (author_names[2][1]==\".\") & (author_names[3][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[3]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        else:\n",
    "            final_author_name = new_author_name\n",
    "    return final_author_name\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))  \n",
    "def remove_current_author(author, coauthors):\n",
    "    return [x for x in coauthors if x!=author][:250]\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def transform_list_col_for_nulls_string(col_with_nulls):\n",
    "    if isinstance(col_with_nulls, list):\n",
    "        return col_with_nulls\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "@udf(returnType=ArrayType(LongType()))\n",
    "def transform_list_col_for_nulls_long(col_with_nulls):\n",
    "    if isinstance(col_with_nulls, list):\n",
    "        return col_with_nulls\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def remove_current_author(author, coauthors):\n",
    "    return [x for x in coauthors if x!=author][:250]\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def coauthor_transform(coauthors):\n",
    "    final_coauthors = []\n",
    "    skip_list = [\" \", \",\" ,\".\" ,\"-\" ,\":\" ,\"/\"]\n",
    "\n",
    "    for coauthor in coauthors:\n",
    "        split_coauthor = coauthor.split(\" \")\n",
    "        if len(split_coauthor) > 1:\n",
    "            temp_coauthor = f\"{split_coauthor[0][0]}_{split_coauthor[-1]}\".lower()\n",
    "            final_coauthors.append(\"\".join([i for i in temp_coauthor if i not in skip_list]))\n",
    "        else:\n",
    "            final_coauthors.append(\"\".join([i for i in coauthor if i not in skip_list]))\n",
    "\n",
    "    return list(set(final_coauthors))\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def get_orcid_from_list(orcid_list):\n",
    "    if isinstance(orcid_list, list):\n",
    "        if orcid_list:\n",
    "            orcid = orcid_list[0]\n",
    "        else:\n",
    "            orcid = ''\n",
    "    elif isinstance(orcid_list, set):\n",
    "        orcid_list = list(orcid_list)\n",
    "        if orcid_list:\n",
    "            orcid = orcid_list[0]\n",
    "        else:\n",
    "            orcid = ''\n",
    "    else:\n",
    "        orcid = ''\n",
    "    return orcid\n",
    "\n",
    "def length_greater_than_6(x):\n",
    "    return (F.length(x) > 6)\n",
    "\n",
    "def concept_L0_removed(x):\n",
    "    return ~x.isin([17744445,138885662,162324750,144133560,15744967,33923547,71924100,86803240,41008148,127313418,185592680,142362112,144024400,127413603,205649164,95457728,192562407,121332964,39432304])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c93dd9b7-27bb-4e5b-8516-91e82f7c1c92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=IntegerType())\n",
    "def get_random_int_udf(block_id):\n",
    "    return random.randint(0, 1000000)\n",
    "\n",
    "def length_greater_than_6(x):\n",
    "    return (F.length(x) > 6)\n",
    "\n",
    "def concept_L0_removed(x):\n",
    "    return ~x.isin([17744445,138885662,162324750,144133560,15744967,33923547,71924100,86803240,41008148,127313418,185592680,142362112,144024400,127413603,205649164,95457728,192562407,121332964,39432304])\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def only_get_last(all_names):\n",
    "    all_names = all_names.split(\" \")\n",
    "    if len(all_names) > 1:\n",
    "        return all_names[-1]\n",
    "    else:\n",
    "        return all_names[0]\n",
    "    \n",
    "@udf (returnType=ArrayType(ArrayType(StringType())))\n",
    "def score_data(full_arr):\n",
    "    full_arr = np.array(full_arr)\n",
    "    data_arr = full_arr[:,2:].astype('float')\n",
    "    block_arr = full_arr[:,0]\n",
    "    label_arr = full_arr[:,1]\n",
    "    model_preds = broadcast_disambiguator_model.value.predict_proba(data_arr)[:,1]\n",
    "    return np.vstack([block_arr[model_preds>0.05], label_arr[model_preds>0.05], model_preds[model_preds>0.05].astype('str')]).T.tolist()\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def get_starting_letter(names):\n",
    "    temp_letters = [x[0] for x in names.split(\" \") if x]\n",
    "    return temp_letters[0] if temp_letters else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "066c3555-863d-45a3-a4c7-2e5c0ac56295",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(StringType()))\n",
    "def group_non_latin_characters(text):\n",
    "    groups = []\n",
    "    text = text.replace(\".\", \"\").replace(\" \", \"\")\n",
    "    for char in text:\n",
    "        try:\n",
    "            script = unicodedata.name(char).split(\" \")[0]\n",
    "            if script == 'LATIN':\n",
    "                pass\n",
    "            else:\n",
    "                if script not in groups:\n",
    "                    groups.append(script)\n",
    "        except:\n",
    "            if \"UNK\" not in groups:\n",
    "                groups.append(\"UNK\")\n",
    "    return groups\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def name_to_keep_ind(groups):\n",
    "    groups_to_skip = ['HIRAGANA', 'CJK', 'KATAKANA','ARABIC', 'HANGUL', 'THAI','DEVANAGARI','BENGALI',\n",
    "                      'THAANA','GUJARATI']\n",
    "    \n",
    "    if any(x in groups_to_skip for x in groups):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1f93ab7f-deb6-4067-85ef-883d9d9663f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=IntegerType())\n",
    "def check_block_vs_block(block_1_names_list, block_2_names_list):\n",
    "    \n",
    "    # check first names\n",
    "    first_check, _ = match_block_names(block_1_names_list[0], block_1_names_list[1], block_2_names_list[0], \n",
    "                                    block_2_names_list[1])\n",
    "    # print(f\"FIRST {first_check}\")\n",
    "    \n",
    "    if first_check:\n",
    "        last_check, _ = match_block_names(block_1_names_list[-2], block_1_names_list[-1], block_2_names_list[-2], \n",
    "                                           block_2_names_list[-1])\n",
    "        # print(f\"LAST {last_check}\")\n",
    "        if last_check:\n",
    "            m1_check, more_to_go = match_block_names(block_1_names_list[2], block_1_names_list[3], block_2_names_list[2], \n",
    "                                           block_2_names_list[3])\n",
    "            if m1_check:\n",
    "                if not more_to_go:\n",
    "                    return 1\n",
    "                m2_check, more_to_go = match_block_names(block_1_names_list[4], block_1_names_list[5], block_2_names_list[4], \n",
    "                                                block_2_names_list[5])\n",
    "                \n",
    "                if m2_check:\n",
    "                    if not more_to_go:\n",
    "                        return 1\n",
    "                    m3_check, more_to_go = match_block_names(block_1_names_list[6], block_1_names_list[7], block_2_names_list[6], \n",
    "                                                block_2_names_list[7])\n",
    "                    if m3_check:\n",
    "                        if not more_to_go:\n",
    "                            return 1\n",
    "                        m4_check, more_to_go = match_block_names(block_1_names_list[8], block_1_names_list[8], block_2_names_list[8], \n",
    "                                                block_2_names_list[9])\n",
    "                        if m4_check:\n",
    "                            if not more_to_go:\n",
    "                                return 1\n",
    "                            m5_check, _ = match_block_names(block_1_names_list[10], block_1_names_list[11], block_2_names_list[10], \n",
    "                                                block_2_names_list[11])\n",
    "                            if m5_check:\n",
    "                                return 1\n",
    "                            else:\n",
    "                                return 0\n",
    "                        else:\n",
    "                            return 0\n",
    "                    else:\n",
    "                        return 0\n",
    "                else:\n",
    "                    return 0\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        swap_check = check_if_last_name_swapped_to_front_creates_match(block_1_names_list, block_2_names_list)\n",
    "        # print(f\"SWAP {swap_check}\")\n",
    "        if swap_check:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "def get_name_from_name_list(name_list):\n",
    "    name = []\n",
    "    for i in range(0,12,2):\n",
    "        if name_list[i]:\n",
    "            name.append(name_list[i][0])\n",
    "        elif name_list[i+1]:\n",
    "            name.append(name_list[i+1][0])\n",
    "        else:\n",
    "            break\n",
    "    if name_list[-2]:\n",
    "        name.append(name_list[-2][0])\n",
    "    elif name_list[-1]:\n",
    "        name.append(name_list[-1][0])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return name\n",
    "        \n",
    "def check_if_last_name_swapped_to_front_creates_match(block_1, block_2):\n",
    "    name_1 = get_name_from_name_list(block_1)\n",
    "    if len(name_1) != 2:\n",
    "        return False\n",
    "    else:\n",
    "        name_2 = get_name_from_name_list(block_2)\n",
    "        if len(name_2)==2:\n",
    "            if \" \".join(name_1) == \" \".join(name_2[-1:] + name_2[:-1]):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "def match_block_names(block_1_names, block_1_initials, block_2_names, block_2_initials):\n",
    "    if block_1_names and block_2_names:\n",
    "        if any(x in block_1_names for x in block_2_names):\n",
    "            return True, True\n",
    "        else:\n",
    "            return False, True\n",
    "    elif block_1_names and not block_2_names:\n",
    "        if block_2_initials:\n",
    "            if any(x in block_1_initials for x in block_2_initials):\n",
    "                return True, True\n",
    "            else:\n",
    "                return False, True\n",
    "        else:\n",
    "            return True, True\n",
    "    elif not block_1_names and block_2_names:\n",
    "        if block_1_initials:\n",
    "            if any(x in block_1_initials for x in block_2_initials):\n",
    "                return True, True\n",
    "            else:\n",
    "                return False, True\n",
    "        else:\n",
    "            return True, True\n",
    "    elif block_1_initials and block_2_initials:\n",
    "        if any(x in block_1_initials for x in block_2_initials):\n",
    "            return True, True\n",
    "        else:\n",
    "            return False, True\n",
    "    else:\n",
    "        return True, False\n",
    "\n",
    "@udf(returnType=ArrayType(ArrayType(StringType())))\n",
    "def get_name_match_list(name):\n",
    "    name_split_1 = name.replace(\"-\", \"\").split()\n",
    "    name_split_2 = \"\"\n",
    "    if \"-\" in name:\n",
    "        name_split_2 = name.replace(\"-\", \" \").split()\n",
    "\n",
    "    fn = []\n",
    "    fni = []\n",
    "    \n",
    "    m1 = []\n",
    "    m1i = []\n",
    "    m2 = []\n",
    "    m2i = []\n",
    "    m3 = []\n",
    "    m3i = []\n",
    "    m4 = []\n",
    "    m4i = []\n",
    "    m5 = []\n",
    "    m5i = []\n",
    "\n",
    "    ln = []\n",
    "    lni = []\n",
    "    for name_split in [name_split_1, name_split_2]:\n",
    "        if len(name_split) == 0:\n",
    "            pass\n",
    "        elif len(name_split) == 1:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[0]) > 1:\n",
    "                ln.append(name_split[0])\n",
    "                lni.append(name_split[0][0])\n",
    "            else:\n",
    "                lni.append(name_split[0][0])\n",
    "            \n",
    "        elif len(name_split) == 2:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 3:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 4:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 5:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "                \n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 6:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "            \n",
    "            if len(name_split[4]) > 1:\n",
    "                m4.append(name_split[4])\n",
    "                m4i.append(name_split[4][0])\n",
    "            else:\n",
    "                m4i.append(name_split[4][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 7:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "            \n",
    "            if len(name_split[4]) > 1:\n",
    "                m4.append(name_split[4])\n",
    "                m4i.append(name_split[4][0])\n",
    "            else:\n",
    "                m4i.append(name_split[4][0])\n",
    "\n",
    "            if len(name_split[5]) > 1:\n",
    "                m5.append(name_split[5])\n",
    "                m5i.append(name_split[5][0])\n",
    "            else:\n",
    "                m5i.append(name_split[5][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        else:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "                \n",
    "            if len(name_split[4]) > 1:\n",
    "                m4.append(name_split[4])\n",
    "                m4i.append(name_split[4][0])\n",
    "            else:\n",
    "                m4i.append(name_split[4][0])\n",
    "\n",
    "            joined_names = \" \".join(name_split[5:-1])\n",
    "            m5.append(joined_names)\n",
    "            m5i.append(joined_names[0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "            \n",
    "\n",
    "    return [list(set(x)) for x in [fn,fni,m1,m1i,m2,m2i,m3,m3i,m4,m4i,m5,m5i,ln,lni]]\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def transform_author_name(author):\n",
    "    if author.startswith(\"None \"):\n",
    "        author = author.replace(\"None \", \"\")\n",
    "    elif author.startswith(\"Array \"):\n",
    "        author = author.replace(\"Array \", \"\")\n",
    "\n",
    "    author = unicodedata.normalize('NFKC', author)\n",
    "    \n",
    "    author_name = HumanName(\" \".join(author.split()))\n",
    "\n",
    "    if (author_name.title == 'Dr.') | (author_name.title == ''):\n",
    "        temp_new_author_name = f\"{author_name.first} {author_name.middle} {author_name.last}\"\n",
    "    else:\n",
    "        temp_new_author_name = f\"{author_name.title} {author_name.first} {author_name.middle} {author_name.last}\"\n",
    "\n",
    "    new_author_name = \" \".join(temp_new_author_name.split())\n",
    "\n",
    "    author_names = new_author_name.split(\" \")\n",
    "    \n",
    "    if (author_name.title != '') : \n",
    "        final_author_name = new_author_name\n",
    "    else:\n",
    "        if len(author_names) == 1:\n",
    "            final_author_name = new_author_name\n",
    "        elif len(author_names) == 2:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        elif len(author_names) == 3:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[2]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[2]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\") & (author_names[2][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        elif len(author_names) == 4:\n",
    "            if (len(author_names[1]) == 1) & (len(author_names[2]) == 1) & (len(author_names[3]) == 1) & (len(author_names[0]) > 3):\n",
    "                final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[3]} {author_names[0]}\"\n",
    "            elif (len(author_names[1]) == 2) & (len(author_names[2]) == 2) & (len(author_names[3]) == 2) & (len(author_names[0]) > 3):\n",
    "                if (author_names[1][1]==\".\") & (author_names[2][1]==\".\") & (author_names[3][1]==\".\"):\n",
    "                    final_author_name = f\"{author_names[1]} {author_names[2]} {author_names[3]} {author_names[0]}\"\n",
    "                else:\n",
    "                    final_author_name = new_author_name\n",
    "            else:\n",
    "                final_author_name = new_author_name\n",
    "        else:\n",
    "            final_author_name = new_author_name\n",
    "    return final_author_name\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))  \n",
    "def remove_current_author(author, coauthors):\n",
    "    return [x for x in coauthors if x!=author][:250]\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def transform_name_for_search(name):\n",
    "    name = unidecode(unicodedata.normalize('NFKC', name))\n",
    "    name = name.lower().replace(\" \", \" \").replace(\".\", \" \").replace(\",\", \" \").replace(\"|\", \" \").replace(\")\", \"\").replace(\"(\", \"\")\\\n",
    "        .replace(\"-\", \"\").replace(\"&\", \"\").replace(\"$\", \"\").replace(\"#\", \"\").replace(\"@\", \"\").replace(\"%\", \"\").replace(\"0\", \"\") \\\n",
    "        .replace(\"1\", \"\").replace(\"2\", \"\").replace(\"3\", \"\").replace(\"4\", \"\").replace(\"5\", \"\").replace(\"6\", \"\").replace(\"7\", \"\") \\\n",
    "        .replace(\"8\", \"\").replace(\"9\", \"\").replace(\"*\", \"\").replace(\"^\", \"\").replace(\"{\", \"\").replace(\"}\", \"\").replace(\"+\", \"\") \\\n",
    "        .replace(\"=\", \"\").replace(\"_\", \"\").replace(\"~\", \"\").replace(\"`\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\\\\", \"\") \\\n",
    "        .replace(\"<\", \"\").replace(\">\", \"\").replace(\"?\", \"\").replace(\"/\", \"\").replace(\";\", \"\").replace(\":\", \"\").replace(\"\\'\", \"\") \\\n",
    "        .replace(\"\\\"\", \"\")\n",
    "    name = \" \".join(name.split())\n",
    "    return name\n",
    "\n",
    "@udf(returnType=ArrayType(ArrayType(StringType())))\n",
    "def create_author_name_list_from_list(name_lists):\n",
    "    if not isinstance(name_lists, list):\n",
    "        name_lists = name_lists.tolist()\n",
    "    \n",
    "    name_list_len = len(name_lists[0])\n",
    "    \n",
    "    temp_name_list = [[j[i] for j in name_lists] for i in range(name_list_len)]\n",
    "    temp_name_list_2 = [[j[0] for j in i if j] for i in temp_name_list]\n",
    "    \n",
    "    return [list(set(x)) for x in temp_name_list_2]\n",
    "\n",
    "@udf(returnType=ArrayType(ArrayType(StringType())))\n",
    "def get_name_match_from_alternate_names(alt_names):\n",
    "    trans_names = list(set([transform_name_for_search_reg(x) for x in alt_names]))\n",
    "    name_lists = [get_name_match_list_reg(x) for x in trans_names]\n",
    "    return create_author_name_list_from_list_reg(name_lists)\n",
    "\n",
    "def create_author_name_list_from_list_reg(name_lists):\n",
    "    if not isinstance(name_lists, list):\n",
    "        name_lists = name_lists.tolist()\n",
    "    \n",
    "    name_list_len = len(name_lists[0])\n",
    "    \n",
    "    temp_name_list = [[j[i] for j in name_lists] for i in range(name_list_len)]\n",
    "    temp_name_list_2 = [[j[0] for j in i if j] for i in temp_name_list]\n",
    "    \n",
    "    return [list(set(x)) for x in temp_name_list_2]\n",
    "\n",
    "def transform_name_for_search_reg(name):\n",
    "    name = unidecode(unicodedata.normalize('NFKC', name))\n",
    "    name = name.lower().replace(\" \", \" \").replace(\".\", \" \").replace(\",\", \" \").replace(\"|\", \" \").replace(\")\", \"\").replace(\"(\", \"\")\\\n",
    "        .replace(\"-\", \"\").replace(\"&\", \"\").replace(\"$\", \"\").replace(\"#\", \"\").replace(\"@\", \"\").replace(\"%\", \"\").replace(\"0\", \"\") \\\n",
    "        .replace(\"1\", \"\").replace(\"2\", \"\").replace(\"3\", \"\").replace(\"4\", \"\").replace(\"5\", \"\").replace(\"6\", \"\").replace(\"7\", \"\") \\\n",
    "        .replace(\"8\", \"\").replace(\"9\", \"\").replace(\"*\", \"\").replace(\"^\", \"\").replace(\"{\", \"\").replace(\"}\", \"\").replace(\"+\", \"\") \\\n",
    "        .replace(\"=\", \"\").replace(\"_\", \"\").replace(\"~\", \"\").replace(\"`\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\\\\", \"\") \\\n",
    "        .replace(\"<\", \"\").replace(\">\", \"\").replace(\"?\", \"\").replace(\"/\", \"\").replace(\";\", \"\").replace(\":\", \"\").replace(\"\\'\", \"\") \\\n",
    "        .replace(\"\\\"\", \"\")\n",
    "    name = \" \".join(name.split())\n",
    "    return name\n",
    "\n",
    "def get_name_match_list_reg(name):\n",
    "    name_split_1 = name.replace(\"-\", \"\").split()\n",
    "    name_split_2 = \"\"\n",
    "    if \"-\" in name:\n",
    "        name_split_2 = name.replace(\"-\", \" \").split()\n",
    "\n",
    "    fn = []\n",
    "    fni = []\n",
    "    \n",
    "    m1 = []\n",
    "    m1i = []\n",
    "    m2 = []\n",
    "    m2i = []\n",
    "    m3 = []\n",
    "    m3i = []\n",
    "    m4 = []\n",
    "    m4i = []\n",
    "    m5 = []\n",
    "    m5i = []\n",
    "\n",
    "    ln = []\n",
    "    lni = []\n",
    "    for name_split in [name_split_1, name_split_2]:\n",
    "        if len(name_split) == 0:\n",
    "            pass\n",
    "        elif len(name_split) == 1:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[0]) > 1:\n",
    "                ln.append(name_split[0])\n",
    "                lni.append(name_split[0][0])\n",
    "            else:\n",
    "                lni.append(name_split[0][0])\n",
    "            \n",
    "        elif len(name_split) == 2:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 3:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 4:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 5:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "                \n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 6:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "            \n",
    "            if len(name_split[4]) > 1:\n",
    "                m4.append(name_split[4])\n",
    "                m4i.append(name_split[4][0])\n",
    "            else:\n",
    "                m4i.append(name_split[4][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        elif len(name_split) == 7:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "            \n",
    "            if len(name_split[4]) > 1:\n",
    "                m4.append(name_split[4])\n",
    "                m4i.append(name_split[4][0])\n",
    "            else:\n",
    "                m4i.append(name_split[4][0])\n",
    "\n",
    "            if len(name_split[5]) > 1:\n",
    "                m5.append(name_split[5])\n",
    "                m5i.append(name_split[5][0])\n",
    "            else:\n",
    "                m5i.append(name_split[5][0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "        else:\n",
    "            if len(name_split[0]) > 1:\n",
    "                fn.append(name_split[0])\n",
    "                fni.append(name_split[0][0])\n",
    "            else:\n",
    "                fni.append(name_split[0][0])\n",
    "\n",
    "            if len(name_split[1]) > 1:\n",
    "                m1.append(name_split[1])\n",
    "                m1i.append(name_split[1][0])\n",
    "            else:\n",
    "                m1i.append(name_split[1][0])\n",
    "\n",
    "            if len(name_split[2]) > 1:\n",
    "                m2.append(name_split[2])\n",
    "                m2i.append(name_split[2][0])\n",
    "            else:\n",
    "                m2i.append(name_split[2][0])\n",
    "\n",
    "            if len(name_split[3]) > 1:\n",
    "                m3.append(name_split[3])\n",
    "                m3i.append(name_split[3][0])\n",
    "            else:\n",
    "                m3i.append(name_split[3][0])\n",
    "                \n",
    "            if len(name_split[4]) > 1:\n",
    "                m4.append(name_split[4])\n",
    "                m4i.append(name_split[4][0])\n",
    "            else:\n",
    "                m4i.append(name_split[4][0])\n",
    "\n",
    "            joined_names = \" \".join(name_split[5:-1])\n",
    "            m5.append(joined_names)\n",
    "            m5i.append(joined_names[0])\n",
    "\n",
    "            if len(name_split[-1]) > 1:\n",
    "                ln.append(name_split[-1])\n",
    "                lni.append(name_split[-1][0])\n",
    "            else:\n",
    "                lni.append(name_split[-1][0])\n",
    "            \n",
    "\n",
    "    return [list(set(x)) for x in [fn,fni,m1,m1i,m2,m2i,m3,m3i,m4,m4i,m5,m5i,ln,lni]]\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def get_most_frequent_name(x):\n",
    "    return mode(x)\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def get_unique_orcid_for_author_table(list_of_orcids):\n",
    "    if not isinstance(list_of_orcids, list):\n",
    "        try:\n",
    "            list_of_orcids = list_of_orcids.tolist()\n",
    "        except:\n",
    "            list_of_orcids = list(list_of_orcids)\n",
    "        \n",
    "    orcids = [x for x in list_of_orcids if x]\n",
    "    \n",
    "    if orcids:\n",
    "        return orcids[0]\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "@udf(returnType=IntegerType())\n",
    "def check_for_unique_orcid_live_clustering(list_of_orcids):\n",
    "    if not isinstance(list_of_orcids, list):\n",
    "        try:\n",
    "            list_of_orcids = list_of_orcids.tolist()\n",
    "        except:\n",
    "            list_of_orcids = list(list_of_orcids)\n",
    "        \n",
    "    orcids = [x for x in list_of_orcids if x]\n",
    "    \n",
    "    if len(orcids) > 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d17c69a3-b2d9-4a27-8ac9-422fcbaf3818",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_data_features_scored(df, prefix):\n",
    "    df \\\n",
    "        .withColumn('row_label', F.concat_ws(\"|\", F.col('work_author_id'), F.col('work_author_id_2'))) \\\n",
    "        .withColumn('work_in_citations_2', F.array_contains(F.col('citations_2'), F.col('paper_id')).cast(IntegerType())) \\\n",
    "        .withColumn('work_2_in_citations', F.array_contains(F.col('citations'), F.col('paper_id_2')).cast(IntegerType())) \\\n",
    "        .withColumn('citation_work_match', F.when((F.col('work_2_in_citations')==1) | \n",
    "                                                  (F.col('work_in_citations_2')==1), 1).otherwise(0)) \\\n",
    "        .withColumn('insts_inter', F.size(F.array_intersect(F.col('institutions'), F.col('institutions_2')))) \\\n",
    "        .withColumn('coauths_inter', F.size(F.array_intersect(F.col('coauthors_shorter'), F.col('coauthors_shorter_2')))) \\\n",
    "        .withColumn('concps_inter', F.size(F.array_intersect(F.col('concepts_shorter'), F.col('concepts_shorter_2')))) \\\n",
    "        .withColumn('cites_inter', F.size(F.array_intersect(F.col('citations'), F.col('citations_2')))) \\\n",
    "        .withColumn('coauths_union', F.size(F.array_union(F.col('coauthors_shorter'), F.col('coauthors_shorter_2')))) \\\n",
    "        .withColumn('concps_union', F.size(F.array_union(F.col('concepts_shorter'), F.col('concepts_shorter_2')))) \\\n",
    "        .withColumn('cites_union', F.size(F.array_union(F.col('citations'), F.col('citations_2')))) \\\n",
    "        .withColumn('inst_per', F.when(F.col('insts_inter')>0, 1).otherwise(0)) \\\n",
    "        .withColumn('coauthors_shorter_per', F.round(F.when(F.col('coauths_union')>0, \n",
    "                                                            F.col('coauths_inter')/F.col('coauths_union')).otherwise(0.0), 4)) \\\n",
    "        .withColumn('concepts_shorter_per', F.round(F.when(F.col('concps_union')>0, \n",
    "                                                           F.col('concps_inter')/F.col('concps_union')).otherwise(0.0), 4)) \\\n",
    "        .withColumn('citation_per', F.round(F.when(F.col('cites_union')>0, \n",
    "                                                   F.col('cites_inter')/F.col('cites_union')).otherwise(0.0), 4)) \\\n",
    "        .withColumn('exact_match', F.when(F.col('author')==F.col('author_2'), 1).otherwise(0)) \\\n",
    "        .withColumn('name_len', F.length(F.col('author'))) \\\n",
    "        .withColumn('name_spaces', F.size(F.split(F.col('author'), \" \"))) \\\n",
    "        .select(F.col('work_author_id').alias('block'),'row_label', 'inst_per','concepts_shorter_per', 'coauthors_shorter_per', \n",
    "            (F.col('exact_match')*F.col('name_len')).alias('exact_match_len'),\n",
    "            (F.col('exact_match')*F.col('name_spaces')).alias('exact_match_spaces'), 'citation_per', 'citation_work_match') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}{prefix}all_features/\")\n",
    "\n",
    "    print('features saved: ', spark.read.parquet(f\"{temp_save_path}{prefix}all_features/\").count())\n",
    "        \n",
    "    spark.read.parquet(f\"{temp_save_path}{prefix}all_features/\")\\\n",
    "        .withColumn('random_int', get_random_int_udf(F.col('block'))) \\\n",
    "        .withColumn('concat_cols', F.array(F.col('block'), F.col('row_label').cast(StringType()), \n",
    "                                            F.col('inst_per').cast(StringType()), \n",
    "                                            F.col('concepts_shorter_per').cast(StringType()), \n",
    "                                            F.col('coauthors_shorter_per').cast(StringType()), \n",
    "                                            F.col('exact_match_len').cast(StringType()), \n",
    "                                            F.col('exact_match_spaces').cast(StringType()), \n",
    "                                            F.col('citation_per').cast(StringType()), \n",
    "                                            F.col('citation_work_match').cast(StringType()))) \\\n",
    "        .groupby('random_int') \\\n",
    "        .agg(F.collect_list(F.col('concat_cols')).alias('data_to_score')) \\\n",
    "        .withColumn('scored_data', score_data(F.col('data_to_score'))) \\\n",
    "        .select('scored_data') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}{prefix}data_scored/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "74472631-352f-4096-9bb7-ed0ebe0d4001",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def live_clustering_algorithm(scored_data_prefix):\n",
    "    w1 = Window.partitionBy('work_author_id').orderBy(F.col('score').desc())\n",
    "    w2 = Window.partitionBy('author_id').orderBy(F.col('score').desc())\n",
    "\n",
    "    \n",
    "    spark.read.parquet(f\"{temp_save_path}{scored_data_prefix}data_scored/\") \\\n",
    "        .select(F.explode('scored_data').alias('scored_data')) \\\n",
    "        .select(F.col('scored_data').getItem(0).alias('work_author_id'),\n",
    "                F.col('scored_data').getItem(1).alias('pairs'), \n",
    "                F.col('scored_data').getItem(2).alias('score').cast(FloatType())) \\\n",
    "        .dropDuplicates(subset=['pairs']) \\\n",
    "        .select('work_author_id', \n",
    "                F.split(F.col('pairs'), \"\\|\")[1].alias('work_author_id_2'), \n",
    "                'score') \\\n",
    "        .repartition(250) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}{scored_data_prefix}flat_scored_data/\")\n",
    "    \n",
    "    spark.read.parquet(f\"{temp_save_path}{scored_data_prefix}flat_scored_data/\") \\\n",
    "        .join(all_new_data.select('work_author_id','orcid','author'), \n",
    "              how='inner', on='work_author_id') \\\n",
    "        .join(temp_authors_table.select('work_author_id_2','author_id','orcid_2').distinct(), \n",
    "              how='inner',on='work_author_id_2') \\\n",
    "        .filter((F.col('orcid')==F.col('orcid_2')) | \n",
    "        (F.col('orcid')=='') | \n",
    "        (F.col('orcid_2')=='')) \\\n",
    "        .withColumn('rank', F.row_number().over(w1)) \\\n",
    "        .filter(F.col('rank')==1) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}{scored_data_prefix}potential_cluster_matches/\")\n",
    "\n",
    "    pot_cluster_matches = spark.read.parquet(f\"{temp_save_path}{scored_data_prefix}potential_cluster_matches/\")\n",
    "\n",
    "    orcids_check = pot_cluster_matches\\\n",
    "        .groupby('author_id')\\\n",
    "        .agg(F.collect_set(F.col('orcid')).alias('orcids')) \\\n",
    "        .withColumn('orcid_good', check_for_unique_orcid_live_clustering('orcids')) \\\n",
    "        .select('author_id','orcid_good') \\\n",
    "        .alias('orcids_check')\n",
    "\n",
    "    pot_cluster_matches \\\n",
    "        .join(orcids_check.filter(F.col('orcid_good')==1).select('author_id').distinct(), how='inner', on='author_id')\\\n",
    "        .select('work_author_id', 'author_id') \\\n",
    "        .dropDuplicates(subset=['work_author_id']) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}{scored_data_prefix}matched_to_cluster/orcids_good/\")\n",
    "\n",
    "    pot_cluster_matches \\\n",
    "        .join(orcids_check.filter(F.col('orcid_good')==0).select('author_id').distinct(), how='inner', on='author_id')\\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}{scored_data_prefix}orcids_not_good/\")\n",
    "\n",
    "    spark.read.parquet(f\"{temp_save_path}{scored_data_prefix}orcids_not_good/\") \\\n",
    "        .withColumn('rank', F.row_number().over(w2)) \\\n",
    "        .filter(F.col('rank')==1) \\\n",
    "        .select('work_author_id', 'author_id') \\\n",
    "        .dropDuplicates(subset=['work_author_id']) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}{scored_data_prefix}matched_to_cluster/orcids_not_good/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cb982ef3-8ea0-40c6-bf43-647ec28551ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_new_features_table(new_rows_location):\n",
    "    new_rows = spark.read.parquet(f\"{temp_save_path}/new_rows_for_author_table/{new_rows_location}/\") \\\n",
    "        .dropDuplicates()\n",
    "\n",
    "    temp_features_table \\\n",
    "        .union(all_new_data.join(new_rows.select('work_author_id').dropDuplicates(), how='inner', on='work_author_id') \\\n",
    "                .select(F.col('work_author_id').alias('work_author_id_2'), \n",
    "                        F.col('orcid').alias('orcid_2'),\n",
    "                        F.col('citations').alias('citations_2'),\n",
    "                        F.col('institutions').alias('institutions_2'),\n",
    "                        F.col('author').alias('author_2'),\n",
    "                        F.col('paper_id').alias('paper_id_2'),\n",
    "                        'original_author',\n",
    "                        F.col('concepts_shorter').alias('concepts_shorter_2'),\n",
    "                        F.col('coauthors_shorter').alias('coauthors_shorter_2'))) \\\n",
    "        .dropDuplicates(subset=['work_author_id_2']) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/temp_features_table/{new_rows_location}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d4ec17ca-4a56-4c38-a4a1-0a4a04028cd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def get_author_display_name(list_of_all_names):\n",
    "    # Get counts of all unique, transformed names\n",
    "    name_counts = Counter(list_of_all_names)\n",
    "    \n",
    "    # Check to see if there are names with spaces (preferred)\n",
    "    has_space = [x for x in name_counts.most_common() if len(x[0].split(\" \"))>1]\n",
    "\n",
    "    # Logic for if there is at least one name with a space\n",
    "    if has_space:\n",
    "        count_to_check = has_space[0][1]\n",
    "        match_count = [x for x in has_space if x[1]==count_to_check]\n",
    "        if len(match_count) == 1:\n",
    "            display_name = match_count[0][0]\n",
    "        else:\n",
    "            name_len = 0\n",
    "            display_name = ''\n",
    "            for match in match_count:\n",
    "                if len(match[0]) > name_len:\n",
    "                    display_name = match[0]\n",
    "                    name_len = len(match[0])\n",
    "                elif len(match[0]) == name_len:\n",
    "                    if match[0] > display_name:\n",
    "                        display_name = match[0]\n",
    "                        name_len = len(match[0])\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "    # Logic for if there are no names with a space\n",
    "    else:\n",
    "        no_space = name_counts.most_common()\n",
    "        count_to_check = no_space[0][1]\n",
    "        match_count = [x for x in no_space if x[1]==count_to_check]\n",
    "        if len(match_count) == 1:\n",
    "            display_name = match_count[0][0]\n",
    "        else:\n",
    "            name_len = 0\n",
    "            display_name = ''\n",
    "            for match in match_count:\n",
    "                if len(match[0]) > name_len:\n",
    "                    display_name = match[0]\n",
    "                    name_len = len(match[0])\n",
    "                elif len(match[0]) == name_len:\n",
    "                    if match[0] > display_name:\n",
    "                        display_name = match[0]\n",
    "                        name_len = len(match[0])\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "    # check to see if there are other variations of the name\n",
    "    possible_replacements = [x for x in list_of_all_names if ((unidecode(display_name)==unidecode(x)) & \n",
    "                                                              (unidecode(x) != x))]\n",
    "    if possible_replacements:\n",
    "        if len(possible_replacements)==1:\n",
    "            if (len(set(possible_replacements[0]+unidecode(possible_replacements[0]))) > \n",
    "                len(set(display_name+unidecode(display_name)))):\n",
    "                display_name = possible_replacements[0]\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            replace_lens = [len(set(x+unidecode(x))) for x in possible_replacements]\n",
    "            max_len = max(replace_lens)\n",
    "            for name in possible_replacements:\n",
    "                if len(set(name+unidecode(name))) == max_len:\n",
    "                    display_name = name\n",
    "\n",
    "                \n",
    "    return display_name\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def get_author_alternate_names(init_alt_names_list):\n",
    "    temp_list = [x.replace(\"-\",\"‐\") for x in init_alt_names_list]\n",
    "    temp_list = [x[:-1] if x.endswith(\"(\") else x for x in temp_list]\n",
    "    \n",
    "    final_list = temp_list.copy()\n",
    "    \n",
    "    for name in temp_list:\n",
    "        if (name.replace(\".\",\"\") in temp_list) & (name.replace(\".\",\"\") != name):\n",
    "            try:\n",
    "                final_list.remove(name.replace('.',''))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        if ((name.replace(\".\",\" \").replace(\"  \", \" \") in temp_list) & \n",
    "            (name.replace(\".\",\" \").replace(\"  \", \" \") != name)):\n",
    "            try:\n",
    "                final_list.remove(name.replace('.', ' ').replace('  ', ' '))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        if ((name.replace(\".\",\". \").replace(\"  \", \" \") in temp_list) & \n",
    "            (name.replace(\".\",\". \").replace(\"  \", \" \") != name)):\n",
    "            try:\n",
    "                final_list.remove(name)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        if (name.title() in temp_list) & (name.title() != name):\n",
    "            try:\n",
    "                final_list.remove(name)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return list(set(final_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6981c380-f863-4222-96dc-6a14da916cc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_new_author_table(new_rows_location):\n",
    "    new_rows = spark.read.parquet(f\"{temp_save_path}/new_rows_for_author_table/{new_rows_location}/\")\n",
    "\n",
    "    cluster_df = new_rows.union(temp_authors_table.select(F.col('work_author_id_2').alias('work_author_id'), 'author_id'))\n",
    "\n",
    "    # need to join new rows with features table\n",
    "    temp_features_table \\\n",
    "        .select(F.col('work_author_id_2').alias('work_author_id'),F.col('orcid_2').alias('orcid'),\n",
    "                'original_author',F.col('author_2').alias('author')) \\\n",
    "        .join(cluster_df, how='inner', on='work_author_id') \\\n",
    "        .filter(F.col('original_author')!=\"\") \\\n",
    "        .filter(F.col('original_author').isNotNull()) \\\n",
    "        .groupby('author_id') \\\n",
    "        .agg(F.collect_set(F.col('orcid')).alias('orcid'), \n",
    "            F.collect_set(F.col('work_author_id')).alias('work_author_id'),\n",
    "            F.collect_set(F.col('author')).alias('alternate_names'),\n",
    "            F.collect_set(F.col('author')).alias('names_for_list'),\n",
    "            F.collect_list(F.col('author')).alias('names')) \\\n",
    "        .withColumn('orcid', get_unique_orcid_for_author_table(F.col('orcid'))) \\\n",
    "        .withColumn('display_name', get_author_display_name(F.col('names'))) \\\n",
    "        .withColumn('author_alternate_names', get_author_alternate_names(F.col('alternate_names'))) \\\n",
    "        .withColumn('name_match_list', get_name_match_from_alternate_names('names_for_list')) \\\n",
    "        .select(F.explode('work_author_id').alias('work_author_id_2'), \n",
    "                'author_id',\n",
    "                F.col('orcid').alias('orcid_2'), \n",
    "                'display_name',\n",
    "                'alternate_names',\n",
    "                'author_alternate_names',\n",
    "                'name_match_list') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/temp_authors_table/{new_rows_location}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d4f16eef-d84e-4a06-958e-623c19c0dad6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=IntegerType())\n",
    "def check_list_vs_list(list_1, list_2):\n",
    "    set_1 = set(list_1)\n",
    "    set_2 = set(list_2)\n",
    "    if set_1 == set_2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "@udf(returnType=LongType())\n",
    "def get_paper_id_from_work_author(work_author_id):\n",
    "    return int(work_author_id.split(\"_\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "19ee63e8-219a-4c9d-9c6e-49854816e530",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bad_author_names = ['None Anonymous','Not specified','&NA; &NA;','Creator','None Unknown',\n",
    "                    'Unknown Unknown','Unknown Author','Unknown','Author Unknown',\n",
    "                    'None, None','None','None None','None No authorship indicated',\n",
    "                    'No authorship indicated','None &NA;','&Na; &Na;','&Na;']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f69e9ab4-5858-470d-a436-13107ca5b62a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save prod features table\n",
    "spark.read.parquet(f\"{prod_save_path}/current_features_table/\") \\\n",
    "    .filter(~F.col('original_author').isin(bad_author_names)) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/current_features_table/\")\n",
    "\n",
    "spark.read.parquet(f\"{prod_save_path}/current_features_table/\")\\\n",
    "    .filter(F.col('original_author').isin(bad_author_names)) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/null_author_rows_to_filter_out/\")\n",
    "\n",
    "spark.read.parquet(f\"{temp_save_path}/current_features_table/\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1121695f-5b39-486f-85b0-f1b48803f462",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "curr_author_table = spark.read.parquet(f\"{prod_save_path}/current_authors_table/\")\n",
    "\n",
    "null_author_rows = spark.read.parquet(f\"{temp_save_path}/null_author_rows_to_filter_out/\")\\\n",
    "    .select(F.col('work_author_id_2').alias('work_author_id')).distinct()\n",
    "\n",
    "# save prod authors table\n",
    "curr_author_table\\\n",
    "    .join(null_author_rows, how='leftanti', on='work_author_id') \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/current_authors_table/\")\n",
    "\n",
    "spark.read.parquet(f\"{temp_save_path}/current_authors_table/\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd1f4f54-380e-4744-9c57-922fa0877555",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "authors_table_last_date = curr_author_table.select(F.max('modified_date')).collect()[0][0]\n",
    "\n",
    "authors_table_last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "280a036b-b452-4353-93f1-3fb622ebb35a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read\\\n",
    "    .parquet(f\"{temp_save_path}/raw_data_to_disambiguate/\").select(F.min('created_date'), F.max('created_date')) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4af16d83-e355-49ae-b10c-38bb578b54b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# saving null authors table\n",
    "spark.read.parquet(f\"{prod_save_path}/current_null_authors_table/\")\\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/current_null_authors_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02411847-77bf-4806-b086-2db9c7f861d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w1 = Window.partitionBy('work_author_id').orderBy(F.col('name_len').desc())\n",
    "\n",
    "(spark.read\n",
    "    .parquet(f\"{temp_save_path}/raw_data_to_disambiguate/\")\n",
    "    .select('work_author_id', F.trim(F.col('original_author')).alias('original_author'), 'orcid', 'concepts', \n",
    "            'institutions', 'citations', 'coauthors', 'created_date', 'partition')\n",
    "    .filter(F.col('original_author').isNotNull())\n",
    "    .filter(F.col('original_author')!='')\n",
    "    .filter(~F.col('original_author').isin(bad_author_names))\n",
    "    .withColumn('name_len', F.length(F.col('original_author')))\n",
    "    .withColumn('rank', F.row_number().over(w1))\n",
    "    .filter(F.col('rank')==1)\n",
    "    .withColumn('citations', transform_list_col_for_nulls_long(F.col('citations')))\n",
    "    .withColumn('coauthors', transform_list_col_for_nulls_string(F.col('coauthors')))\n",
    "    .withColumn('concepts', transform_list_col_for_nulls_long(F.col('concepts')))\n",
    "    .withColumn('institutions', transform_list_col_for_nulls_long(F.col('institutions')))\n",
    "    .withColumn('author', transform_author_name(F.col('original_author')))\n",
    "    .withColumn('coauthors', transform_coauthors(F.col('coauthors')))\n",
    "    .withColumn('coauthors', remove_current_author(F.col('author'),F.col('coauthors')))\n",
    "    .withColumn('coauthors', coauthor_transform(F.col('coauthors')))\n",
    "    .withColumn('orcid', F.when(F.col('orcid').isNull(), '').otherwise(F.col('orcid')))\n",
    "    .withColumn('paper_id', F.split(F.col('work_author_id'), \"_\").getItem(0).cast(LongType()))\n",
    "    .withColumn('concepts', F.array_distinct(F.col('concepts')))\n",
    "    .withColumn('concepts_shorter', F.filter(F.col('concepts'), concept_L0_removed))\n",
    "    .withColumn('coauthors_shorter', F.filter(F.col('coauthors'), length_greater_than_6))\n",
    "    .select('work_author_id','paper_id','original_author','author','orcid','coauthors_shorter','concepts_shorter',\n",
    "        'institutions','citations','created_date')\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/new_data_to_disambiguate/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93668424-b462-4915-8954-ca922adefa2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_new_data = spark.read.parquet(f\"{temp_save_path}/new_data_to_disambiguate/\")\n",
    "name_of_stats_to_track.append('temp_data_count')\n",
    "stats_to_track.append(temp_new_data.count())\n",
    "temp_new_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6b99e7b-efeb-40fe-8747-58effe1ece84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_new_data = temp_new_data.groupby(['original_author', 'author', 'orcid', 'coauthors_shorter', 'concepts_shorter', \n",
    "                       'institutions', 'citations']) \\\n",
    "    .agg(F.collect_set(F.col('work_author_id')).alias('work_author_ids'), \n",
    "         F.max(F.col('created_date')).alias('created_date'), \n",
    "         F.max(F.col('work_author_id')).alias('work_author_id')) \\\n",
    "    .withColumn('works_len', F.size(F.col('work_author_ids'))) \\\n",
    "    .withColumn('paper_id', get_paper_id_from_work_author('work_author_id')) \\\n",
    "    .select('work_author_ids','work_author_id', 'paper_id', 'original_author', 'author', 'orcid', 'coauthors_shorter', \n",
    "            'concepts_shorter', 'institutions', 'citations', 'created_date','works_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e72ec7be-625c-4003-91b6-622ae7794510",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_new_data.filter(F.col('works_len')>1) \\\n",
    "    .select('work_author_ids','work_author_id','original_author', 'author', 'orcid', 'coauthors_shorter', \n",
    "            'concepts_shorter', 'institutions', 'citations') \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/duplicate_work_entries/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e633e54e-416b-458c-b6e5-32d2f6232657",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "name_of_stats_to_track.append('dup_work_entries_count')\n",
    "stats_to_track.append(spark.read.parquet(f\"{temp_save_path}/duplicate_work_entries/\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "999ed5f5-13b0-490d-9974-4a2122de7e0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_new_data \\\n",
    "    .select('work_author_id', 'paper_id', 'original_author', 'author', 'orcid', 'coauthors_shorter', 'concepts_shorter', 'institutions', 'citations', 'created_date') \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{temp_save_path}/final_data_to_disambiguate/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bcefed8-f99e-4d8b-89d1-84bac54fd9a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_new_data.sample(0.001).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff224bb-55cd-438a-bf28-e4d0a33b8200",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for_stats = spark.read.parquet(f\"{temp_save_path}/final_data_to_disambiguate/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cf869f0-1889-4aba-af62-cb19f96e33e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# getting input data stats for tracking purposes\n",
    "print(\"Getting stats for tracking\")\n",
    "name_of_stats_to_track.append('input_data_citations_null_count')\n",
    "stats_to_track.append(for_stats.filter(F.size(F.col('citations'))==0).count())\n",
    "\n",
    "name_of_stats_to_track.append('input_data_concepts_shorter_null_count')\n",
    "stats_to_track.append(for_stats.filter(F.size(F.col('concepts_shorter'))==0).count())\n",
    "\n",
    "name_of_stats_to_track.append('input_data_coauthors_shorter_null_count')\n",
    "stats_to_track.append(for_stats.filter(F.size(F.col('coauthors_shorter'))==0).count())\n",
    "\n",
    "name_of_stats_to_track.append('input_data_institutions_null_count')\n",
    "stats_to_track.append(for_stats.filter(F.size(F.col('institutions'))==0).count())\n",
    "\n",
    "name_of_stats_to_track.append('input_data_orcid_null_count')\n",
    "stats_to_track.append(for_stats.filter(F.col('orcid')=='').count())\n",
    "\n",
    "print(\"Done getting stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4a1774a-d191-453c-929b-8961b0570171",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### AND Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f87ea69-0fe4-49f6-87d1-6d67a23f9654",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if temp_new_data.filter(F.col('created_date')>authors_table_last_date).count() <= 0:\n",
    "    print(\"NO NEW DATA\")\n",
    "    pass\n",
    "else:\n",
    "    ########################################### INITIAL DATA PREP #####################################################\n",
    "\n",
    "    # Read add_works table\n",
    "    df = (spark.read\n",
    "        .format(\"postgresql\")\n",
    "        .option(\"dbtable\", \"<postgres-table>\")\n",
    "        .option(\"host\", secret['host'])\n",
    "        .option(\"port\", secret['port'])\n",
    "        .option(\"database\", secret['dbname'])\n",
    "        .option(\"user\", secret['username'])\n",
    "        .option(\"password\", secret['password'])\n",
    "        .option(\"fetchSize\", \"15\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    df.write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/current_add_works_table/\")\n",
    "\n",
    "    # Read author_id_merges table\n",
    "    df = (spark.read\n",
    "        .format(\"postgresql\")\n",
    "        .option(\"dbtable\", \"<postgres-table>\")\n",
    "        .option(\"host\", secret['host'])\n",
    "        .option(\"port\", secret['port'])\n",
    "        .option(\"database\", secret['dbname'])\n",
    "        .option(\"user\", secret['username'])\n",
    "        .option(\"password\", secret['password'])\n",
    "        .option(\"fetchSize\", \"15\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    df.write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/current_author_id_merges_table/\")\n",
    "\n",
    "    merge_authors_df = spark.read.parquet(f\"{temp_save_path}/current_author_id_merges_table/\") \\\n",
    "        .dropDuplicates(subset=['merge_from_id'])\n",
    "    print(f\"Rows in author_ids_merges table: {merge_authors_df.cache().count()}\")\n",
    "\n",
    "    add_works_df = spark.read.parquet(f\"{temp_save_path}/current_add_works_table/\") \\\n",
    "        .select('work_author_id', F.col('new_author_id').alias('author_id')) \\\n",
    "        .join(merge_authors_df.select(F.col('merge_from_id').alias('author_id'), 'merge_to_id'), \n",
    "              how='left', on='author_id') \\\n",
    "        .withColumn('final_author_id', F.when(F.col('merge_to_id').isNull(), \n",
    "                                              F.col('author_id')).otherwise(F.col('merge_to_id'))) \\\n",
    "        .select('work_author_id', F.col('final_author_id').alias('author_id')) \\\n",
    "        .dropDuplicates(subset=['work_author_id'])\n",
    "    \n",
    "    print(f\"Rows in add_works table: {add_works_df.cache().count()}\")\n",
    "\n",
    "    all_new_data = spark.read.parquet(f\"{temp_save_path}/final_data_to_disambiguate/\") \\\n",
    "        .dropDuplicates(subset=['work_author_id']) \\\n",
    "        .join(add_works_df, how='leftanti', on='work_author_id')\n",
    "    new_data_size = all_new_data.count()\n",
    "\n",
    "    print(f\"{new_data_size} NEW ROWS TO DISAMBIGUATE\")\n",
    "    name_of_stats_to_track.append('new_rows_count')\n",
    "    stats_to_track.append(new_data_size)\n",
    "    all_new_data.cache().count()\n",
    "\n",
    "    init_cluster_df = spark.read.parquet(f\"{temp_save_path}/current_authors_table/\")\\\n",
    "        .select('work_author_id','author_id') \\\n",
    "        .join(all_new_data.select('work_author_id'), how='leftanti', on='work_author_id') \\\n",
    "        .join(add_works_df.select('work_author_id', F.col('author_id').alias('new_author_id')), \n",
    "              how='left', on='work_author_id') \\\n",
    "        .withColumn('final_author_id', F.when(F.col('new_author_id').isNull(), \n",
    "                                              F.col('author_id')).otherwise(F.col('new_author_id'))) \\\n",
    "        .select('work_author_id', F.col('final_author_id').alias('author_id')) \\\n",
    "        .join(merge_authors_df.select(F.col('merge_from_id').alias('author_id'), 'merge_to_id'), \n",
    "              how='left', on='author_id') \\\n",
    "        .withColumn('final_author_id', F.when(F.col('merge_to_id').isNull(), \n",
    "                                              F.col('author_id')).otherwise(F.col('merge_to_id'))) \\\n",
    "        .select('work_author_id', F.col('final_author_id').alias('author_id')) \\\n",
    "\n",
    "    print(\"Init cluster table created\")\n",
    "\n",
    "    # Create init features table\n",
    "    spark.read.parquet(f\"{temp_save_path}/current_features_table/\") \\\n",
    "        .join(init_cluster_df.select(F.col('work_author_id').alias('work_author_id_2')), how='inner', on='work_author_id_2') \\\n",
    "        .select('work_author_id_2', 'orcid_2', F.col('citations_2').cast(ArrayType(LongType())), \n",
    "                'institutions_2', 'author_2', F.col('paper_id_2').cast(LongType()), 'original_author',\n",
    "                F.col('concepts_shorter_2').cast(ArrayType(LongType())), 'coauthors_shorter_2') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/temp_features_table/init/\")\n",
    "\n",
    "    temp_features_table = spark.read.parquet(f\"{temp_save_path}/temp_features_table/init/\")\n",
    "\n",
    "    print(\"Init features table created\")\n",
    "\n",
    "    # Create init authors table\n",
    "    temp_features_table \\\n",
    "        .select(F.col('work_author_id_2').alias('work_author_id'),F.col('orcid_2').alias('orcid'),\n",
    "                'original_author',F.col('author_2').alias('author')) \\\n",
    "        .join(init_cluster_df, how='inner', on='work_author_id') \\\n",
    "        .filter(F.col('original_author')!=\"\") \\\n",
    "        .filter(F.col('original_author').isNotNull()) \\\n",
    "        .groupby('author_id') \\\n",
    "        .agg(F.collect_set(F.col('orcid')).alias('orcid'), \n",
    "            F.collect_set(F.col('work_author_id')).alias('work_author_id'),\n",
    "            F.collect_set(F.col('author')).alias('alternate_names'),\n",
    "            F.collect_set(F.col('author')).alias('names_for_list'),\n",
    "            F.collect_list(F.col('author')).alias('names')) \\\n",
    "        .withColumn('orcid', get_unique_orcid_for_author_table(F.col('orcid'))) \\\n",
    "        .withColumn('display_name', get_author_display_name(F.col('names'))) \\\n",
    "        .withColumn('name_match_list', get_name_match_from_alternate_names('names_for_list')) \\\n",
    "        .withColumn('author_alternate_names', get_author_alternate_names(F.col('alternate_names'))) \\\n",
    "        .select(F.explode('work_author_id').alias('work_author_id_2'), \n",
    "                'author_id',\n",
    "                F.col('orcid').alias('orcid_2'), \n",
    "                'display_name',\n",
    "                'alternate_names',\n",
    "                'author_alternate_names',\n",
    "                'name_match_list') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/temp_authors_table/init/\")\n",
    "\n",
    "    temp_authors_table = spark.read.parquet(f\"{temp_save_path}/temp_authors_table/init/\")\n",
    "\n",
    "    print(\"Init authors table created\")\n",
    "\n",
    "    # Get author names match data\n",
    "    author_names_match = spark.read.parquet(f\"{prod_save_path}/current_author_names_match/\")\n",
    "\n",
    "    # Checking for works that have already been disambiguated\n",
    "    spark.read.parquet(f\"{temp_save_path}/current_authors_table/\")\\\n",
    "        .select('work_author_id','author_id') \\\n",
    "        .join(all_new_data.select('work_author_id'), how='inner', on='work_author_id') \\\n",
    "        .select('work_author_id') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/temp_authors_to_change_table/\")\n",
    "\n",
    "    if spark.read.parquet(f\"{temp_save_path}/temp_authors_to_change_table/\").count() == 0:\n",
    "        print(\"No authors have been previously disambiguated\")\n",
    "        pass\n",
    "    else:\n",
    "        # Need to remove all data from tables\n",
    "        works_authors_to_remove = spark.read.parquet(f\"{temp_save_path}/temp_authors_to_change_table/\")\n",
    "\n",
    "        print(f\"Authors have been previously disambiguated: {works_authors_to_remove.count()}\")\n",
    "        name_of_stats_to_track.append('previously_disambiguated_count')\n",
    "        stats_to_track.append(works_authors_to_remove.count())\n",
    "\n",
    "        temp_features_table \\\n",
    "            .join(works_authors_to_remove.select(F.col('work_author_id').alias('work_author_id_2')), \n",
    "                how='leftanti', on='work_author_id_2') \\\n",
    "            .write.mode('overwrite') \\\n",
    "            .parquet(f\"{temp_save_path}/temp_features_table/init_2/\")\n",
    "\n",
    "        temp_features_table = spark.read.parquet(f\"{temp_save_path}/temp_features_table/init_2/\")\n",
    "\n",
    "        temp_authors_table \\\n",
    "            .join(works_authors_to_remove.select(F.col('work_author_id').alias('work_author_id_2')), \n",
    "                how='leftanti', on='work_author_id_2') \\\n",
    "            .write.mode('overwrite') \\\n",
    "            .parquet(f\"{temp_save_path}/temp_authors_table/init_2/\")\n",
    "\n",
    "        temp_authors_table = spark.read.parquet(f\"{temp_save_path}/temp_authors_table/init_2/\")\n",
    "\n",
    "        author_names_match \\\n",
    "            .join(works_authors_to_remove.select(F.col('work_author_id').alias('work_author_id_2')), \n",
    "                how='leftanti', on='work_author_id_2') \\\n",
    "            .write.mode('overwrite') \\\n",
    "            .parquet(f\"{temp_save_path}/temp_author_names_match/init_2/\")\n",
    "\n",
    "        author_names_match = spark.read.parquet(f\"{temp_save_path}/temp_author_names_match/init_2/\")\n",
    "\n",
    "    ########################################### ORCID MATCH #####################################################\n",
    "\n",
    "    # Getting ORCID matches\n",
    "    all_new_data.filter(F.col('orcid')!='')\\\n",
    "        .join(temp_authors_table.select(F.col('orcid_2').alias('orcid'),'author_id'),how='inner', on='orcid') \\\n",
    "        .select('work_author_id', \n",
    "                'author_id') \\\n",
    "        .dropDuplicates(subset=['work_author_id']) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/new_rows_for_author_table/orcid_rows_for_author_table/\")\n",
    "\n",
    "    orcid_add_count = spark.read\\\n",
    "        .parquet(f\"{temp_save_path}/new_rows_for_author_table/orcid_rows_for_author_table/\").count()\n",
    "    print(f\"ORCID added: {orcid_add_count}\")\n",
    "    name_of_stats_to_track.append('orcid_matched_count')\n",
    "    stats_to_track.append(orcid_add_count)\n",
    "\n",
    "    # Making all tables current\n",
    "    new_loc = 'orcid_rows_for_author_table'\n",
    "    _ = create_new_features_table(new_loc)\n",
    "    print(\"New features table created\")\n",
    "    temp_features_table = spark.read.parquet(f\"{temp_save_path}/temp_features_table/{new_loc}/\")\n",
    "\n",
    "    _ = create_new_author_table(new_loc)\n",
    "    print(\"New authors table created\")\n",
    "    temp_authors_table = spark.read.parquet(f\"{temp_save_path}/temp_authors_table/{new_loc}/\")\n",
    "\n",
    "    # Creating table for next round\n",
    "    all_new_data \\\n",
    "        .join(temp_authors_table.select(F.col('work_author_id_2').alias('work_author_id')).distinct(), \n",
    "                    how='leftanti', on='work_author_id') \\\n",
    "        .select('work_author_id','paper_id','original_author','author','orcid','coauthors_shorter','concepts_shorter',\n",
    "            'institutions','citations','created_date') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/round_2_of_clustering/\")\n",
    "\n",
    "    # Removing connections to clusters for user input\n",
    "    try:\n",
    "        work_to_clusters_removed = spark.read.parquet(f\"{prod_save_path}/works_removed_from_clusters/\")\n",
    "    except:\n",
    "        work_to_clusters_removed = spark.sparkContext.emptyRDD()\\\n",
    "            .toDF(schema=StructType([StructField(\"work_author_id\", StringType()),\n",
    "                                     StructField(\"author_id\", LongType())]))\n",
    "            \n",
    "    ####################################### NAME MATCH ROUND 1 ################################################\n",
    "            \n",
    "    round_2_new_data = spark.read.parquet(f\"{temp_save_path}/round_2_of_clustering/\")\n",
    "\n",
    "    names_match = round_2_new_data \\\n",
    "        .withColumn('paper_id', F.split(F.col('work_author_id'), \"_\").getItem(0).cast(LongType())) \\\n",
    "        .join(temp_authors_table.select('work_author_id_2', \n",
    "                                        'orcid_2', \n",
    "                                        'author_id',\n",
    "                                        F.explode(F.col('alternate_names')).alias('author')),\n",
    "            how='inner', on='author') \\\n",
    "        .join(work_to_clusters_removed, how='leftanti', on=['work_author_id','author_id']) \\\n",
    "        .filter((F.col('orcid')==F.col('orcid_2')) | \n",
    "                (F.col('orcid')=='') | \n",
    "                (F.col('orcid_2')=='')) \\\n",
    "        .join(temp_features_table.drop(\"orcid_2\"), how='inner', on='work_author_id_2') \\\n",
    "        .repartition(250)\n",
    "\n",
    "    # prepare data for model scoring and score\n",
    "    _ = get_data_features_scored(names_match, \"/names_match/\")\n",
    "\n",
    "    # send through clustering/matching algorithm\n",
    "    _ = live_clustering_algorithm(\"/names_match/\")\n",
    "\n",
    "    # save new author table rows to file\n",
    "    spark.read.parquet(f\"{temp_save_path}/names_match/matched_to_cluster/*\") \\\n",
    "        .select('work_author_id', \n",
    "                'author_id') \\\n",
    "        .dropDuplicates(subset=['work_author_id']) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/new_rows_for_author_table/name_match_rows_for_author_table/\")\n",
    "\n",
    "    name_match_add_count = spark.read\\\n",
    "        .parquet(f\"{temp_save_path}/new_rows_for_author_table/name_match_rows_for_author_table/\").count()\n",
    "    print(f\"Name match rows added: {name_match_add_count}\")\n",
    "    name_of_stats_to_track.append('name_match_1_count')\n",
    "    stats_to_track.append(name_match_add_count)\n",
    "\n",
    "    # Making all tables current\n",
    "    new_loc = 'name_match_rows_for_author_table'\n",
    "    _ = create_new_features_table(new_loc)\n",
    "    print(\"New features table created\")\n",
    "    temp_features_table = spark.read.parquet(f\"{temp_save_path}/temp_features_table/{new_loc}/\")\n",
    "\n",
    "    _ = create_new_author_table(new_loc)\n",
    "    print(\"New authors table created\")\n",
    "    temp_authors_table = spark.read.parquet(f\"{temp_save_path}/temp_authors_table/{new_loc}/\")\n",
    "\n",
    "    # Creating table for next round\n",
    "    all_new_data \\\n",
    "        .join(temp_authors_table.select(F.col('work_author_id_2').alias('work_author_id')).distinct(), \n",
    "                    how='leftanti', on='work_author_id') \\\n",
    "        .select('work_author_id','paper_id','original_author','author','orcid','coauthors_shorter','concepts_shorter',\n",
    "            'institutions','citations','created_date') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/round_3_of_clustering/\")\n",
    "\n",
    "    ####################################### NAME MATCH ROUND 2 ###############################################\n",
    "\n",
    "    round_3_new_data = spark.read.parquet(f\"{temp_save_path}/round_3_of_clustering/\")\n",
    "\n",
    "    names_match_2 = round_3_new_data \\\n",
    "        .withColumn('paper_id', F.split(F.col('work_author_id'), \"_\").getItem(0).cast(LongType())) \\\n",
    "        .join(temp_authors_table.select('work_author_id_2', \n",
    "                                        'orcid_2', \n",
    "                                        'author_id',\n",
    "                                        F.explode(F.col('alternate_names')).alias('author')),\n",
    "            how='inner', on='author') \\\n",
    "        .join(work_to_clusters_removed, how='leftanti', on=['work_author_id','author_id']) \\\n",
    "        .filter((F.col('orcid')==F.col('orcid_2')) | \n",
    "                (F.col('orcid')=='') | \n",
    "                (F.col('orcid_2')=='')) \\\n",
    "        .join(temp_features_table.drop(\"orcid_2\"), how='inner', on='work_author_id_2')\n",
    "\n",
    "    # prepare data for model scoring and score\n",
    "    _ = get_data_features_scored(names_match_2, \"/names_match_2/\")\n",
    "\n",
    "    # send through clustering/matching algorithm\n",
    "    _ = live_clustering_algorithm(\"/names_match_2/\")\n",
    "\n",
    "    # save new author table rows to file\n",
    "    spark.read.parquet(f\"{temp_save_path}/names_match_2/matched_to_cluster/*\") \\\n",
    "        .select('work_author_id', \n",
    "                'author_id') \\\n",
    "        .dropDuplicates(subset=['work_author_id']) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/new_rows_for_author_table/name_match_rows_for_author_table_2/\")\n",
    "\n",
    "    name_match_add_count = spark.read\\\n",
    "        .parquet(f\"{temp_save_path}/new_rows_for_author_table/name_match_rows_for_author_table_2/\").count()\n",
    "    print(f\"Name match rows added: {name_match_add_count}\")\n",
    "    name_of_stats_to_track.append('name_match_2_count')\n",
    "    stats_to_track.append(name_match_add_count)\n",
    "\n",
    "    # Making all tables current\n",
    "    new_loc = 'name_match_rows_for_author_table_2'\n",
    "    _ = create_new_features_table(new_loc)\n",
    "    print(\"New features table created\")\n",
    "    temp_features_table = spark.read.parquet(f\"{temp_save_path}/temp_features_table/{new_loc}/\")\n",
    "\n",
    "    _ = create_new_author_table(new_loc)\n",
    "    print(\"New authors table created\")\n",
    "    temp_authors_table = spark.read.parquet(f\"{temp_save_path}/temp_authors_table/{new_loc}/\")\n",
    "\n",
    "    # Creating table for next round\n",
    "    all_new_data \\\n",
    "        .join(temp_authors_table.select(F.col('work_author_id_2').alias('work_author_id')).distinct(), \n",
    "                    how='leftanti', on='work_author_id') \\\n",
    "        .select('work_author_id','paper_id','original_author','author','orcid','coauthors_shorter','concepts_shorter',\n",
    "            'institutions','citations','created_date') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/round_4_of_clustering/\")\n",
    "    \n",
    "    ######################################### NO NAME MATCH ###############################################\n",
    "\n",
    "    round_4_new_data = spark.read.parquet(f\"{temp_save_path}/round_4_of_clustering/\") \\\n",
    "        .filter(F.col('author').isNotNull()) \\\n",
    "        .filter(F.col('author')!='') \\\n",
    "        .withColumn('non_latin_groups', group_non_latin_characters(F.col('author'))) \\\n",
    "        .withColumn('name_to_keep_ind', name_to_keep_ind('non_latin_groups'))\n",
    "\n",
    "    round_4_new_data \\\n",
    "        .filter(F.col('name_to_keep_ind')==1) \\\n",
    "        .withColumn('transformed_search_name', transform_name_for_search(F.col('author'))) \\\n",
    "        .withColumn('name_len', F.length(F.col('transformed_search_name'))) \\\n",
    "        .filter(F.col('name_len')>1) \\\n",
    "        .withColumn('name_match_list', get_name_match_list(F.col('transformed_search_name'))) \\\n",
    "        .withColumn('block', only_get_last(F.col('transformed_search_name'))) \\\n",
    "        .select('work_author_id','orcid','name_match_list','transformed_search_name', 'block') \\\n",
    "        .withColumn('block_removed', F.expr(\"regexp_replace(transformed_search_name, block, '')\")) \\\n",
    "        .withColumn('new_block_removed', F.trim(F.expr(\"regexp_replace(block_removed, '  ', ' ')\"))) \\\n",
    "        .withColumn('letter', get_starting_letter(F.col('new_block_removed'))) \\\n",
    "        .select('work_author_id','orcid','name_match_list','transformed_search_name','letter', 'block') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/for_new_authors_table/names_to_blocks/\")\n",
    "\n",
    "    no_names_match = spark.read.parquet(f\"{temp_save_path}/for_new_authors_table/names_to_blocks/\")\n",
    "    no_names_match.cache().count()\n",
    "\n",
    "    # join those names to authors table alternate names to get work_author_ids to check\n",
    "    full_no_names_match_table = no_names_match \\\n",
    "        .join(author_names_match, how='inner', on=['block','letter']) \\\n",
    "        .withColumn('matched_names', check_block_vs_block(F.col('name_match_list'), F.col('name_match_list_2'))) \\\n",
    "        .filter(F.col('matched_names')==1) \\\n",
    "        .select('work_author_id', 'work_author_id_2') \\\n",
    "        .dropDuplicates() \\\n",
    "        .join(round_4_new_data, how='inner', on='work_author_id') \\\n",
    "        .join(temp_features_table, how='inner', on='work_author_id_2') \\\n",
    "        .filter((F.col('orcid')==F.col('orcid_2')) | \n",
    "            (F.col('orcid')=='') | \n",
    "            (F.col('orcid_2')==''))\n",
    "        \n",
    "    # prepare data for model scoring and score\n",
    "    _ = get_data_features_scored(full_no_names_match_table, \"/no_names_match/\")\n",
    "\n",
    "    # send through clustering/matching algorithm\n",
    "    _ = live_clustering_algorithm(\"/no_names_match/\")\n",
    "\n",
    "    # save new author table rows to file\n",
    "    spark.read.parquet(f\"{temp_save_path}/no_names_match/matched_to_cluster/*\") \\\n",
    "        .select('work_author_id', \n",
    "                'author_id') \\\n",
    "        .dropDuplicates(subset=['work_author_id']) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/new_rows_for_author_table/no_name_match_rows_for_author_table/\")\n",
    "\n",
    "    no_name_match_add_count = spark.read\\\n",
    "        .parquet(f\"{temp_save_path}/new_rows_for_author_table/no_name_match_rows_for_author_table/\").count()\n",
    "    print(f\"No name match rows added: {no_name_match_add_count}\")\n",
    "    name_of_stats_to_track.append('no_name_match_count')\n",
    "    stats_to_track.append(no_name_match_add_count)\n",
    "\n",
    "    # Making all tables current\n",
    "    new_loc = 'no_name_match_rows_for_author_table'\n",
    "    _ = create_new_features_table(new_loc)\n",
    "    print(\"New features table created\")\n",
    "    temp_features_table = spark.read.parquet(f\"{temp_save_path}/temp_features_table/{new_loc}/\")\n",
    "\n",
    "    _ = create_new_author_table(new_loc)\n",
    "    print(\"New authors table created\")\n",
    "    temp_authors_table = spark.read.parquet(f\"{temp_save_path}/temp_authors_table/{new_loc}/\")\n",
    "\n",
    "    #################################### NO MATCH: NEW CLUSTER ##########################################\n",
    "\n",
    "    # Creating table for work_author_ids that need new cluster\n",
    "    all_new_data \\\n",
    "        .join(temp_authors_table.select(F.col('work_author_id_2').alias('work_author_id')).distinct(), \n",
    "                    how='leftanti', on='work_author_id') \\\n",
    "        .select('work_author_id','paper_id','original_author','author','orcid','coauthors_shorter','concepts_shorter',\n",
    "            'institutions','citations','created_date') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/end_of_clustering_leftovers/\")\n",
    "\n",
    "    new_cluster_count = spark.read\\\n",
    "        .parquet(f\"{temp_save_path}/end_of_clustering_leftovers/\").count()\n",
    "    print(f\"New clusters added: {new_cluster_count}\")\n",
    "    name_of_stats_to_track.append('new_cluster_count')\n",
    "    stats_to_track.append(new_cluster_count)\n",
    "\n",
    "    # Getting max author_id to create new cluster nums\n",
    "    max_id = int(temp_authors_table.select(F.max(F.col('author_id'))).collect()[0][0])\n",
    "\n",
    "    # Create new clusters\n",
    "    w1 = Window.orderBy(F.col('work_author_id'))\n",
    "\n",
    "    spark.read.parquet(f\"{temp_save_path}/end_of_clustering_leftovers/\") \\\n",
    "        .select('work_author_id').distinct() \\\n",
    "        .withColumn('temp_cluster_num', F.row_number().over(w1)) \\\n",
    "        .withColumn('author_id', F.lit(max_id) + F.col('temp_cluster_num')) \\\n",
    "        .select('work_author_id','author_id') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/new_rows_for_author_table/new_author_clusters/\")\n",
    "\n",
    "    # Making all tables current\n",
    "    new_loc = 'new_author_clusters'\n",
    "    _ = create_new_features_table(new_loc)\n",
    "    print(\"New features table created\")\n",
    "    temp_features_table = spark.read.parquet(f\"{temp_save_path}/temp_features_table/{new_loc}/\")\n",
    "\n",
    "    _ = create_new_author_table(new_loc)\n",
    "    print(\"New authors table created\")\n",
    "    temp_authors_table = spark.read.parquet(f\"{temp_save_path}/temp_authors_table/{new_loc}/\")\n",
    "\n",
    "    ################################### DUPLICATES/NULL AUTHORS ###########################################\n",
    "\n",
    "    # Taking care of duplicate entries (if necessary)\n",
    "    dup_work_entries = spark.read.parquet(f\"{temp_save_path}/duplicate_work_entries/\")\n",
    "    if dup_work_entries.count() > 0:\n",
    "        duplicate_rows_temp_authors = temp_authors_table \\\n",
    "            .join(dup_work_entries.select(F.col('work_author_id').alias('work_author_id_2'),\n",
    "                                        'work_author_ids'), how='inner', on='work_author_id_2') \\\n",
    "            .select(F.explode('work_author_ids').alias('work_author_id_2'),\n",
    "                    'author_id',\n",
    "                    'orcid_2', \n",
    "                    'display_name',\n",
    "                    'alternate_names',\n",
    "                    'author_alternate_names',\n",
    "                    'name_match_list').alias('dup_rows_temp_author_table')\n",
    "            \n",
    "        temp_authors_table.union(duplicate_rows_temp_authors.select(*temp_authors_table.columns)) \\\n",
    "            .dropDuplicates(subset=['work_author_id_2']) \\\n",
    "            .write.mode('overwrite') \\\n",
    "            .parquet(f\"{temp_save_path}/temp_authors_table/after_dup_entries/\")\n",
    "    \n",
    "        duplicate_rows_temp_features = temp_features_table \\\n",
    "            .join(dup_work_entries.select(F.col('work_author_id').alias('work_author_id_2'),\n",
    "                                        'work_author_ids'), how='inner', on='work_author_id_2') \\\n",
    "            .select(F.explode('work_author_ids').alias('work_author_id_2'),\n",
    "                    'orcid_2',\n",
    "                    'citations_2',\n",
    "                    'institutions_2',\n",
    "                    'author_2',\n",
    "                    'paper_id_2',\n",
    "                    'original_author',\n",
    "                    'concepts_shorter_2',\n",
    "                    'coauthors_shorter_2').alias('dup_rows_temp_feature_table')\n",
    "            \n",
    "        temp_features_table.union(duplicate_rows_temp_features.select(*temp_features_table.columns)) \\\n",
    "            .dropDuplicates(subset=['work_author_id_2']) \\\n",
    "            .write.mode('overwrite') \\\n",
    "            .parquet(f\"{temp_save_path}/temp_features_table/after_dup_entries/\")\n",
    "\n",
    "        print(\"New features table created\")\n",
    "        temp_features_table = spark.read.parquet(f\"{temp_save_path}/temp_features_table/after_dup_entries/\")\n",
    "\n",
    "        print(\"New authors table created\")\n",
    "        temp_authors_table = spark.read.parquet(f\"{temp_save_path}/temp_authors_table/after_dup_entries/\")\n",
    "\n",
    "    # Taking care of empty/null/bad author names\n",
    "    (spark.read\n",
    "        .parquet(f\"{temp_save_path}/raw_data_to_disambiguate/\")\n",
    "        .select('work_author_id', F.trim(F.col('original_author')).alias('original_author'), 'orcid', 'concepts',   \n",
    "                'institutions', 'citations', 'coauthors', 'created_date', 'partition')\n",
    "        .join(temp_authors_table.select(F.col('work_author_id_2').alias('work_author_id'))\n",
    "                .dropDuplicates(subset=['work_author_id']), how='leftanti', on='work_author_id') \\\n",
    "        .dropDuplicates(subset=['work_author_id'])\n",
    "        .select(F.col('work_author_id').alias('work_author_id_2'), \n",
    "                F.col('orcid').alias('orcid_2'), 'original_author')\n",
    "        .withColumn('author_id', F.lit(9999999999))\n",
    "        .withColumn('display_name', F.lit('NULL AUTHOR_ID'))\n",
    "        .withColumn('alternate_names', F.array(F.lit('NULL AUTHOR_ID')))\n",
    "        .select('work_author_id_2', \n",
    "                'original_author',\n",
    "                'author_id',\n",
    "                'orcid_2', \n",
    "                'display_name',\n",
    "                'alternate_names')\n",
    "        .write.mode('overwrite')\n",
    "        .parquet(f\"{temp_save_path}/new_data_to_be_given_null_value/\"))\n",
    "    \n",
    "    if spark.read.parquet(f\"{temp_save_path}/null_author_rows_to_filter_out/\").count() > 0:\n",
    "        (spark.read.parquet(f\"{temp_save_path}/null_author_rows_to_filter_out/\")\n",
    "            .select('work_author_id_2', 'orcid_2', 'original_author')\n",
    "            .withColumn('author_id', F.lit(9999999999))\n",
    "            .withColumn('display_name', F.lit('NULL AUTHOR_ID'))\n",
    "            .withColumn('alternate_names', F.array(F.lit('NULL AUTHOR_ID')))\n",
    "            .select('work_author_id_2', \n",
    "                    'original_author',\n",
    "                    'author_id',\n",
    "                    'orcid_2', \n",
    "                    'display_name',\n",
    "                    'alternate_names')\n",
    "            .write.mode('append')\n",
    "            .parquet(f\"{temp_save_path}/new_data_to_be_given_null_value/\"))\n",
    "    \n",
    "    ############################## COMPARING FINAL VS INIT AUTHOR TABLES ######################################\n",
    "\n",
    "    # Loading initial and final null author tables to compare against\n",
    "    new_null_author_data = spark.read.parquet(f\"{temp_save_path}/new_data_to_be_given_null_value/\")\n",
    "    old_null_author_data = spark.read.parquet(f\"{temp_save_path}/current_null_authors_table/\")\n",
    "\n",
    "    null_authors_diff = new_null_author_data\\\n",
    "        .join(old_null_author_data.select(F.col('work_author_id').alias('work_author_id_2')).distinct(), \n",
    "                how='leftanti', on='work_author_id_2') \\\n",
    "        .withColumn(\"created_date\", F.current_timestamp()) \\\n",
    "        .withColumn(\"modified_date\", F.current_timestamp()) \\\n",
    "        .withColumn('author_id_changed', F.lit(True)) \\\n",
    "        .dropDuplicates(subset=['work_author_id_2'])\n",
    "\n",
    "    # Loading initial and final tables to compare against\n",
    "    init_author_table = spark.read.parquet(f\"{prod_save_path}/current_authors_table/\") \\\n",
    "        .select('work_author_id', \n",
    "                F.col('author_id').alias('author_id_1'),\n",
    "                F.col('display_name').alias('display_name_1'),\n",
    "                F.col('alternate_names').alias('alternate_names_1'), \n",
    "                F.col('orcid').alias('orcid_1'),\n",
    "                'created_date',\n",
    "                'modified_date')\n",
    "\n",
    "    final_author_table = temp_authors_table \\\n",
    "            .select(F.col('work_author_id_2').alias('work_author_id'), \n",
    "                F.col('author_id').alias('author_id_2'),\n",
    "                F.col('display_name').alias('display_name_2'),\n",
    "                F.col('author_alternate_names').alias('alternate_names_2'), \n",
    "                'orcid_2')\n",
    "            \n",
    "    print(f\"INITIAL TABLE: {init_author_table.count()}\")\n",
    "    print(f\"FINAL TABLE: {final_author_table.count()}\")\n",
    "\n",
    "    name_of_stats_to_track.append('init_author_table_count')\n",
    "    stats_to_track.append(init_author_table.count())\n",
    "\n",
    "    name_of_stats_to_track.append('final_author_table_count')\n",
    "    stats_to_track.append(final_author_table.count())\n",
    "\n",
    "    # take final author table, compare to init table (all columns) to see if anything has been changed\n",
    "    compare_tables = final_author_table.join(init_author_table, how='inner', on='work_author_id') \\\n",
    "        .withColumn('orcid_compare', F.when(F.col('orcid_1')==F.col('orcid_2'), 0).otherwise(1)) \\\n",
    "        .withColumn('display_name_compare', F.when(F.col('display_name_1')==F.col('display_name_2'), 0).otherwise(1)) \\\n",
    "        .withColumn('author_id_compare', F.when(F.col('author_id_1')==F.col('author_id_2'), 0).otherwise(1)) \\\n",
    "        .withColumn('alternate_names_compare', check_list_vs_list(F.col('alternate_names_1'), \n",
    "                                                                    F.col('alternate_names_2'))) \\\n",
    "        .withColumn('total_changes', F.col('orcid_compare') + F.col('display_name_compare') + \n",
    "                                F.col('author_id_compare') + F.col('alternate_names_compare'))\n",
    "    \n",
    "    # if not, write out those rows to a folder\n",
    "    compare_tables.filter(F.col('total_changes')==0) \\\n",
    "        .select('work_author_id', \n",
    "                F.col('author_id_1').alias('author_id'),\n",
    "                F.col('display_name_1').alias('display_name'),\n",
    "                F.col('alternate_names_1').alias('alternate_names'), \n",
    "                F.col('orcid_1').alias('orcid'),\n",
    "                'created_date',\n",
    "                'modified_date') \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/final_author_table_part/no_changes/\")\n",
    "\n",
    "    # if modified but not created, write out to different folder\n",
    "    compare_tables.filter(F.col('total_changes')>0) \\\n",
    "        .select('work_author_id', \n",
    "                F.col('author_id_2').alias('author_id'),\n",
    "                F.col('display_name_2').alias('display_name'),\n",
    "                F.col('alternate_names_2').alias('alternate_names'), \n",
    "                F.col('orcid_2').alias('orcid'),\n",
    "                'created_date') \\\n",
    "        .withColumn(\"modified_date\", F.current_timestamp()) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/final_author_table_part/modified/\")\n",
    "\n",
    "    # if created, write out to different folder\n",
    "    final_author_table.join(init_author_table.select('work_author_id'), how='leftanti', on='work_author_id') \\\n",
    "        .select('work_author_id', \n",
    "                F.col('author_id_2').alias('author_id'),\n",
    "                F.col('display_name_2').alias('display_name'),\n",
    "                F.col('alternate_names_2').alias('alternate_names'), \n",
    "                F.col('orcid_2').alias('orcid')) \\\n",
    "        .withColumn(\"created_date\", F.current_timestamp()) \\\n",
    "        .withColumn(\"modified_date\", F.current_timestamp()) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{temp_save_path}/final_author_table_part/created/\")\n",
    "\n",
    "    # Writing out rows that have changed author IDs (merge table)\n",
    "    if spark.read.parquet(f\"{temp_save_path}/temp_authors_to_change_table/\").count() > 0:\n",
    "        # write out previously disambiguated rows to final locations\n",
    "        compare_tables.join(works_authors_to_remove, how='inner', on='work_author_id') \\\n",
    "            .filter(F.col('author_id_1')!=F.col('author_id_2')) \\\n",
    "            .select('work_author_id', \n",
    "                    F.col('author_id_1').alias('old_author_id'),\n",
    "                    F.col('author_id_2').alias('new_author_id')) \\\n",
    "            .withColumn(\"modified_date\", F.current_timestamp()) \\\n",
    "            .write.mode('overwrite') \\\n",
    "            .parquet(f\"{temp_save_path}/work_authors_changed_clusters/\")\n",
    "\n",
    "        print(\"Author cluster changes: \", spark.read.parquet(f\"{temp_save_path}/work_authors_changed_clusters/\").count())\n",
    "    \n",
    "    # Writing out rows that have changed author IDs (merge table)\n",
    "    compare_tables.join(works_authors_to_remove, how='inner', on='work_author_id') \\\n",
    "            .filter(F.col('author_id_1').isNotNull()) \\\n",
    "            .filter(F.col('author_id_1')!=F.col('author_id_2')) \\\n",
    "            .select('work_author_id', \n",
    "                    F.col('author_id_1').alias('old_author_id'),\n",
    "                    F.col('author_id_2').alias('new_author_id')) \\\n",
    "            .withColumn(\"modified_date\", F.current_timestamp()) \\\n",
    "            .write.mode('append') \\\n",
    "            .parquet(f\"{prod_save_path}/previously_disambiguated_author_merges/\")\n",
    "    \n",
    "    print(\"No row changes: \", spark.read.parquet(f\"{temp_save_path}/final_author_table_part/no_changes/\").count())\n",
    "    print(\"Modified rows: \", spark.read.parquet(f\"{temp_save_path}/final_author_table_part/modified/\").count())\n",
    "    print(\"New cluster rows: \", spark.read.parquet(f\"{temp_save_path}/final_author_table_part/created/\").count())\n",
    "    print(\"Total rows: \", spark.read.parquet(f\"{temp_save_path}/final_author_table_part/*\").count())\n",
    "\n",
    "    name_of_stats_to_track.append('new_rows_count')\n",
    "    stats_to_track.append(spark.read.parquet(f\"{temp_save_path}/final_author_table_part/created/\").count())\n",
    "\n",
    "    name_of_stats_to_track.append('modified_rows_count')\n",
    "    stats_to_track.append(spark.read.parquet(f\"{temp_save_path}/final_author_table_part/modified/\").count())\n",
    "\n",
    "    name_of_stats_to_track.append('total_rows_count')\n",
    "    stats_to_track.append(spark.read.parquet(f\"{temp_save_path}/final_author_table_part/*\").count())\n",
    "\n",
    "    ############### ALL ROWS BEING WRITTEN TO POSTGRES OUTPUT TABLE (authorships.authors_modified) #################\n",
    "\n",
    "    secret = get_secret()\n",
    "\n",
    "    # Getting new author IDs and writing to authorships.authors_modified\n",
    "    (final_author_table.join(init_author_table.select('work_author_id'), how='leftanti', on='work_author_id') \n",
    "        .select('work_author_id', F.col('author_id_2').alias('author_id'), \n",
    "                F.col('display_name_2').alias('display_name'), \n",
    "                F.col('alternate_names_2').alias('alternate_names'), \n",
    "                F.col('orcid_2').alias('orcid'))\n",
    "        .withColumn(\"created_date\", F.current_timestamp()) \n",
    "        .withColumn(\"modified_date\", F.current_timestamp())\n",
    "        .withColumn('author_id_changed', F.lit(True))\n",
    "        .repartition(6)\n",
    "        .write.format(\"jdbc\") \n",
    "        .option(\"url\", f\"jdbc:postgresql://{secret['host']}:{secret['port']}/{secret['dbname']}\") \n",
    "        .option(\"dbtable\", '<postgres-table>') \n",
    "        .option(\"user\", secret['username']) \n",
    "        .option(\"password\", secret['password']) \n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \n",
    "        .mode(\"overwrite\") \n",
    "        .save())\n",
    "    \n",
    "    print(\"authorships.authors_modified write 1 done\")\n",
    "\n",
    "    # Getting rows that changed author IDs and writing to authorships.authors_modified\n",
    "    (compare_tables.filter(F.col('author_id_compare')==1).filter(F.col('total_changes')>0)\n",
    "            .select('work_author_id', F.col('author_id_2').alias('author_id'), \n",
    "                    F.col('display_name_2').alias('display_name'), \n",
    "                    F.col('alternate_names_2').alias('alternate_names'), \n",
    "                    F.col('orcid_2').alias('orcid'), 'created_date') \n",
    "        .withColumn(\"modified_date\", F.current_timestamp())\n",
    "        .withColumn('author_id_changed', F.lit(True))\n",
    "        .repartition(6)\n",
    "        .write.format(\"jdbc\")\n",
    "        .option(\"url\", f\"jdbc:postgresql://{secret['host']}:{secret['port']}/{secret['dbname']}\") \n",
    "        .option(\"dbtable\", '<postgres-table>')\n",
    "        .option(\"user\", secret['username'])\n",
    "        .option(\"password\", secret['password'])\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\n",
    "        .mode(\"append\")\n",
    "        .save())\n",
    "    \n",
    "    print(\"authorships.authors_modified write 2 done\")\n",
    "\n",
    "    # Getting any new null authors and writing to authorships.authors_modified\n",
    "    if null_authors_diff.count() > 0:\n",
    "        (null_authors_diff\n",
    "            .select(F.col('work_author_id_2').alias('work_author_id'), 'author_id', \n",
    "                        'display_name', 'alternate_names', F.col('orcid_2').alias('orcid'), \n",
    "                        'created_date','modified_date','author_id_changed')\n",
    "            .repartition(6)\n",
    "            .write.format(\"jdbc\") \n",
    "            .option(\"url\", f\"jdbc:postgresql://{secret['host']}:{secret['port']}/{secret['dbname']}\") \n",
    "            .option(\"dbtable\", '<postgres-table>') \n",
    "            .option(\"user\", secret['username']) \n",
    "            .option(\"password\", secret['password']) \n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \n",
    "            .mode(\"append\") \n",
    "            .save())\n",
    "        \n",
    "    print(\"authorships.authors_modified write 3 done\")\n",
    "\n",
    "    # Getting any row changes to author ID metadata (but not author ID) and writing to authorships.authors_modified\n",
    "    (compare_tables.filter(F.col('author_id_compare')==0).filter(F.col('total_changes')>0)\n",
    "            .select('work_author_id', F.col('author_id_2').alias('author_id'), \n",
    "                    F.col('display_name_2').alias('display_name'), \n",
    "                    F.col('alternate_names_2').alias('alternate_names'), \n",
    "                    F.col('orcid_2').alias('orcid'), 'created_date') \n",
    "        .withColumn(\"modified_date\", F.current_timestamp()) \n",
    "        .withColumn('author_id_changed', F.lit(False))\n",
    "        .repartition(6)\n",
    "        .write.format(\"jdbc\")\n",
    "        .option(\"url\", f\"jdbc:postgresql://{secret['host']}:{secret['port']}/{secret['dbname']}\") \n",
    "        .option(\"dbtable\", '<postgres-table>')\n",
    "        .option(\"user\", secret['username'])\n",
    "        .option(\"password\", secret['password'])\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\n",
    "        .mode(\"append\")\n",
    "        .save())\n",
    "    \n",
    "    print(\"authorships.authors_modified write 4 done\")\n",
    "    \n",
    "    print(\"All postgres tables written\")\n",
    "\n",
    "    ########################################## FINAL TABLES TO S3 ##########################################\n",
    "\n",
    "    # Writing out final null authors table from this round\n",
    "    (new_null_author_data.select(F.col('work_author_id_2').alias('work_author_id')).distinct()\\\n",
    "        .join(old_null_author_data, how='inner', on ='work_author_id')\n",
    "        .dropDuplicates(subset=['work_author_id'])\n",
    "        .select('work_author_id', 'author_id', 'display_name', 'alternate_names', 'orcid',\n",
    "                'created_date','modified_date')\n",
    "        .write.mode('overwrite')\n",
    "        .parquet(f\"{prod_save_path}/current_null_authors_table/\"))\n",
    "\n",
    "    if null_authors_diff.count() > 0:\n",
    "        (null_authors_diff\n",
    "            .select(F.col('work_author_id_2').alias('work_author_id'), 'author_id', \n",
    "                        'display_name', 'alternate_names', F.col('orcid_2').alias('orcid'), \n",
    "                        'created_date','modified_date','author_id_changed')\n",
    "            .write.mode('append')\n",
    "            .parquet(f\"{prod_save_path}/current_null_authors_table/\"))\n",
    "    \n",
    "    print(\"Final null author table written to S3\")\n",
    "\n",
    "    # Writing out final authors table from this round\n",
    "    spark.read.parquet(f\"{temp_save_path}/final_author_table_part/*\") \\\n",
    "        .repartition(250) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{prod_save_path}/current_authors_table/\")\n",
    "\n",
    "    print(\"Final authors table written to S3\")\n",
    "\n",
    "    # Writing out final features table from this round\n",
    "    temp_features_table \\\n",
    "        .repartition(250) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{prod_save_path}/current_features_table/\")\n",
    "\n",
    "    print(\"Final features table written to S3\")\n",
    "\n",
    "    # Writing out new author name match table from this round\n",
    "    spark.read.parquet(f\"{temp_save_path}/temp_features_table/new_author_clusters/\") \\\n",
    "        .select('work_author_id_2', 'orcid_2', F.col('author_2').alias('transformed_name')) \\\n",
    "        .filter(F.col('transformed_name')!=\"\") \\\n",
    "        .filter(F.col('transformed_name').isNotNull()) \\\n",
    "        .withColumn('transformed_search_name', transform_name_for_search(F.col('transformed_name'))) \\\n",
    "        .withColumn('name_len', F.length(F.col('transformed_search_name'))) \\\n",
    "        .filter(F.col('name_len')>1) \\\n",
    "        .withColumn('name_match_list_2', get_name_match_list(F.col('transformed_search_name'))) \\\n",
    "        .withColumn('block', only_get_last(F.col('transformed_search_name'))) \\\n",
    "        .select('work_author_id_2','name_match_list_2', 'orcid_2', 'transformed_search_name', 'block')\\\n",
    "        .withColumn('block_removed', F.expr(\"regexp_replace(transformed_search_name, block, '')\")) \\\n",
    "        .withColumn('new_block_removed', F.trim(F.expr(\"regexp_replace(block_removed, '  ', ' ')\"))) \\\n",
    "        .withColumn('letter', get_starting_letter(F.col('new_block_removed'))) \\\n",
    "        .select('work_author_id_2','orcid_2','name_match_list_2', 'block', 'letter') \\\n",
    "        .dropDuplicates() \\\n",
    "        .repartition(250) \\\n",
    "        .write.mode('overwrite') \\\n",
    "        .parquet(f\"{prod_save_path}/current_author_names_match/\")\n",
    "\n",
    "    print(\"Final author name match table written to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ebf2ee7-e466-4079-ae4e-c37c874e6e01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i,j in zip(name_of_stats_to_track,stats_to_track):\n",
    "    print(f\"{curr_date} -- {i} -- {j}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AND_System_v3_live_actual_test_v4",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
