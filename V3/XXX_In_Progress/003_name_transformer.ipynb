{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f071eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 08:18:57.465242: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-18 08:18:58.386127: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib64:/opt/amazon/openmpi/lib64:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/lib\n",
      "2023-04-18 08:18:58.386249: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib64:/opt/amazon/openmpi/lib64:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/lib\n",
      "2023-04-18 08:18:58.386261: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# import logging\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "# import tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09cc4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ed5363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.models import WordPiece\n",
    "# from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a93c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "so = open(\"data.log\", 'w', 10)\n",
    "sys.stdout.echo = so\n",
    "sys.stderr.echo = so\n",
    "\n",
    "get_ipython().log.handlers[0].stream = so\n",
    "get_ipython().log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae7251cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tensorflow_datasets\n",
    "# !pip install -q -U tensorflow-text tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edcc5c4",
   "metadata": {},
   "source": [
    "### Load and save data as TF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a53d2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.system(\"rm author_text.txt\")\n",
    "    print(\"Done\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4566f77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘./training_data_text_files/train/*’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.system(\"rm -r ./training_data_text_files/train/*\")\n",
    "    print(\"Done\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d52f668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.system(\"rm -r ./training_data_text_files/validation/*\")\n",
    "    print(\"Done\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c6c6cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_file_to_directory(file_name, directory, col_name, output_file, type_of_data):\n",
    "    temp_df = pd.read_parquet(file_name)\n",
    "    temp_df = temp_df[(~temp_df['output_name'].isnull()) & \n",
    "                      (~temp_df['raw_input'].isnull())].copy()\n",
    "    temp_df['raw_input'] = temp_df['raw_input'].apply(lambda x: \" \".join(x.split()))\n",
    "    temp_df['output_name'] = temp_df['output_name'].apply(lambda x: \" \".join(x.split()))\n",
    "    temp_df['raw_input_len'] = temp_df['raw_input'].apply(len)\n",
    "    temp_df['output_name_len'] = temp_df['output_name'].apply(len)\n",
    "    temp_df = temp_df[(temp_df['output_name_len']>3) & \n",
    "                      (temp_df['raw_input_len']>3)].copy()\n",
    "    temp_df.to_parquet(f\"./training_data_processed/{type_of_data}/{file_name.split('/')[-1][:10]}.parquet\")\n",
    "\n",
    "    \n",
    "    if type_of_data == 'train':\n",
    "        with open(output_file, \"a\") as f:\n",
    "            for row_text in temp_df[col_name].tolist():\n",
    "                f.write(f\"{row_text}\\n\")\n",
    "            for row_text in temp_df['output_name'].tolist():\n",
    "                f.write(f\"{row_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eeca61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data_files(output_file, col_name='raw_input'):\n",
    "    final_df = pd.DataFrame()\n",
    "    for type_of_data in ['train','validation']:\n",
    "        for ith, file_name in enumerate(glob.glob(f\"./training_data/{type_of_data}/*\")[:40]):\n",
    "            print(f\"{type_of_data} - {ith} - {file_name}\")\n",
    "            _ = write_data_file_to_directory(file_name, f\"./training_data_text_files/{type_of_data}/\", \n",
    "                                             col_name, output_file, type_of_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0863728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train - 0 - ./training_data/train/part-00012-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1352-1-c000.snappy.parquet\n",
      "train - 1 - ./training_data/train/part-00021-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1361-1-c000.snappy.parquet\n",
      "train - 2 - ./training_data/train/part-00015-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1378-1-c000.snappy.parquet\n",
      "train - 3 - ./training_data/train/part-00024-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1326-1-c000.snappy.parquet\n",
      "train - 4 - ./training_data/train/part-00023-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1368-1-c000.snappy.parquet\n",
      "train - 5 - ./training_data/train/part-00003-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1302-1-c000.snappy.parquet\n",
      "train - 6 - ./training_data/train/part-00001-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1303-1-c000.snappy.parquet\n",
      "train - 7 - ./training_data/train/part-00002-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1293-1-c000.snappy.parquet\n",
      "train - 8 - ./training_data/train/part-00020-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1308-1-c000.snappy.parquet\n",
      "train - 9 - ./training_data/train/part-00051-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1292-1-c000.snappy.parquet\n",
      "train - 10 - ./training_data/train/part-00000-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1344-1-c000.snappy.parquet\n",
      "train - 11 - ./training_data/train/part-00033-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1317-1-c000.snappy.parquet\n",
      "train - 12 - ./training_data/train/part-00004-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1343-1-c000.snappy.parquet\n",
      "train - 13 - ./training_data/train/part-00017-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1331-1-c000.snappy.parquet\n",
      "train - 14 - ./training_data/train/part-00018-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1340-1-c000.snappy.parquet\n",
      "train - 15 - ./training_data/train/part-00005-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1354-1-c000.snappy.parquet\n",
      "train - 16 - ./training_data/train/part-00006-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1279-1-c000.snappy.parquet\n",
      "train - 17 - ./training_data/train/part-00022-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1325-1-c000.snappy.parquet\n",
      "train - 18 - ./training_data/train/part-00038-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1356-1-c000.snappy.parquet\n",
      "train - 19 - ./training_data/train/part-00007-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1298-1-c000.snappy.parquet\n",
      "train - 20 - ./training_data/train/part-00008-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1315-1-c000.snappy.parquet\n",
      "train - 21 - ./training_data/train/part-00025-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1362-1-c000.snappy.parquet\n",
      "train - 22 - ./training_data/train/part-00009-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1309-1-c000.snappy.parquet\n",
      "train - 23 - ./training_data/train/part-00026-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1334-1-c000.snappy.parquet\n",
      "train - 24 - ./training_data/train/part-00011-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1316-1-c000.snappy.parquet\n",
      "train - 25 - ./training_data/train/part-00010-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1345-1-c000.snappy.parquet\n",
      "train - 26 - ./training_data/train/part-00016-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1367-1-c000.snappy.parquet\n",
      "train - 27 - ./training_data/train/part-00019-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1301-1-c000.snappy.parquet\n",
      "train - 28 - ./training_data/train/part-00013-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1287-1-c000.snappy.parquet\n",
      "train - 29 - ./training_data/train/part-00030-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1355-1-c000.snappy.parquet\n",
      "train - 30 - ./training_data/train/part-00014-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1348-1-c000.snappy.parquet\n",
      "train - 31 - ./training_data/train/part-00036-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1335-1-c000.snappy.parquet\n",
      "train - 32 - ./training_data/train/part-00028-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1375-1-c000.snappy.parquet\n",
      "train - 33 - ./training_data/train/part-00027-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1311-1-c000.snappy.parquet\n",
      "train - 34 - ./training_data/train/part-00029-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1288-1-c000.snappy.parquet\n",
      "train - 35 - ./training_data/train/part-00031-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1327-1-c000.snappy.parquet\n",
      "train - 36 - ./training_data/train/part-00056-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1328-1-c000.snappy.parquet\n",
      "train - 37 - ./training_data/train/part-00032-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1360-1-c000.snappy.parquet\n",
      "train - 38 - ./training_data/train/part-00035-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1338-1-c000.snappy.parquet\n",
      "train - 39 - ./training_data/train/part-00034-tid-7029455405628506627-07caf35b-07ef-40c0-9792-454519d7ade9-1372-1-c000.snappy.parquet\n",
      "validation - 0 - ./training_data/validation/part-00000-tid-2698832604035103174-09a11065-a2c3-4bff-b407-12900bfc9566-1389-1-c000.snappy.parquet\n",
      "validation - 1 - ./training_data/validation/part-00001-tid-2698832604035103174-09a11065-a2c3-4bff-b407-12900bfc9566-1390-1-c000.snappy.parquet\n",
      "validation - 2 - ./training_data/validation/part-00002-tid-2698832604035103174-09a11065-a2c3-4bff-b407-12900bfc9566-1391-1-c000.snappy.parquet\n",
      "validation - 3 - ./training_data/validation/part-00003-tid-2698832604035103174-09a11065-a2c3-4bff-b407-12900bfc9566-1392-1-c000.snappy.parquet\n",
      "validation - 4 - ./training_data/validation/part-00004-tid-2698832604035103174-09a11065-a2c3-4bff-b407-12900bfc9566-1393-1-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "_ = get_all_data_files(\"author_text.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e02f2f",
   "metadata": {},
   "source": [
    "### Build Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b57cd9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.system(\"rm name_transformer_wordpiece_tokenizer\")\n",
    "    print(\"Done\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed3ce859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordpiece_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# NFD Unicode, lowercase, and getting rid of accents (to make sure text is as readable as possible)\n",
    "# wordpiece_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "\n",
    "# Splitting on whitespace\n",
    "wordpiece_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Training a tokenizer on the training dataset\n",
    "trainer = WordPieceTrainer(vocab_size=7632*2, special_tokens=[\"[UNK]\", \"[START]\", \"[END]\"])\n",
    "files = [\"author_text.txt\"]\n",
    "wordpiece_tokenizer.train(files, trainer)\n",
    "\n",
    "wordpiece_tokenizer.save(\"name_transformer_wordpiece_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef0b42fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"name_transformer_wordpiece_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d73b73",
   "metadata": {},
   "source": [
    "### Make TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72481934",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef9ec0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_serialize_example(f0, f1, f2):\n",
    "    \"\"\"\n",
    "    Serialization function.\n",
    "    \"\"\"\n",
    "    tf_string = tf.py_function(serialize_example, (f0, f1, f2), tf.string)\n",
    "    return tf.reshape(tf_string, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fb0bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(inputs, outputs, labels):\n",
    "    \"\"\"\n",
    "    Takes in features and outputs them to a serialized string that can be written to\n",
    "    a file using the TFRecord Writer.\n",
    "    \"\"\"\n",
    "    inputs_list = tf.train.Int64List(value=inputs.numpy().tolist())\n",
    "    outputs_list = tf.train.Int64List(value=outputs.numpy().tolist())\n",
    "    labels_list = tf.train.Int64List(value=labels.numpy().tolist())\n",
    "    \n",
    "    inputs_feature = tf.train.Feature(int64_list = inputs_list)\n",
    "    outputs_feature = tf.train.Feature(int64_list = outputs_list)\n",
    "    labels_feature = tf.train.Feature(int64_list = labels_list)\n",
    "    \n",
    "    features_for_example = {\n",
    "        'inputs': inputs_feature,\n",
    "        'outputs': outputs_feature,\n",
    "        'labels': labels_feature\n",
    "    }\n",
    "    \n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=features_for_example))\n",
    "    \n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "591291a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_raw_input(tok_input, start_tok, end_tok):\n",
    "    temp_tok_input = (start_tok + tok_input)[:(MAX_LEN-1)]+end_tok\n",
    "    final_tok_input = temp_tok_input + [0]*(MAX_LEN-len(temp_tok_input))\n",
    "    return np.asarray(final_tok_input, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfe293af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_outputs(tok_input, start_tok):\n",
    "    temp_tok_input = (start_tok + tok_input)[:MAX_LEN]\n",
    "    final_tok_input = temp_tok_input + [0]*(MAX_LEN-len(temp_tok_input))\n",
    "    return np.asarray(final_tok_input, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5786d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_outputs(tok_input, end_tok):\n",
    "    temp_tok_input = (tok_input + end_tok)[:MAX_LEN]\n",
    "    final_tok_input = temp_tok_input + [0]*(MAX_LEN-len(temp_tok_input))\n",
    "    return np.asarray(final_tok_input, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7215676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecords_dataset(filename, iter_num, dataset_type='train'):\n",
    "    \"\"\"\n",
    "    Creates a TF Dataset that can then be saved to a file to make it faster to read\n",
    "    data during training and allow for transferring of data between compute instances.\n",
    "    \"\"\"\n",
    "    data = pd.read_parquet(filename)\n",
    "    \n",
    "    # Getting tokenized data\n",
    "    start_token = name_tokenizer(\"[START]\")['input_ids']\n",
    "    end_token = name_tokenizer(\"[END]\")['input_ids']\n",
    "    \n",
    "    print(\"------------tokenizing input\")\n",
    "    data['tokenized_raw_input'] = name_tokenizer(data['raw_input'].tolist())['input_ids']\n",
    "    print(\"------------tokenizing output\")\n",
    "    data['tokenized_raw_output'] = name_tokenizer(data['output_name'].tolist())['input_ids']\n",
    "    \n",
    "    print(\"------------getting labeled data\")\n",
    "    data['inputs'] = data['tokenized_raw_input'].apply(lambda x: create_labeled_raw_input(x, start_token, end_token))\n",
    "    data['outputs'] = data['tokenized_raw_output'].apply(lambda x: create_outputs(x, start_token))\n",
    "    data['labels'] = data['tokenized_raw_output'].apply(lambda x: create_labeled_outputs(x, end_token))\n",
    "    \n",
    "    # Creating TF Dataset\n",
    "    ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(data['inputs'].to_list()),\n",
    "                              tf.data.Dataset.from_tensor_slices(data['outputs'].to_list()),\n",
    "                              tf.data.Dataset.from_tensor_slices(data['labels'].to_list())))\n",
    "    \n",
    "    serialized_features_dataset = ds.map(tf_serialize_example)\n",
    "    \n",
    "    print(\"------------writing to tfrecord\")\n",
    "    \n",
    "    filename = f\"./tfrecords/{dataset_type}/{str(iter_num).zfill(4)}.tfrecord\"\n",
    "    writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "    writer.write(serialized_features_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77eecd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training_data_processed/train/part-00012.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "WARNING:tensorflow:From /tmp/ipykernel_60592/3700649595.py:32: TFRecordWriter.__init__ (from tensorflow.python.data.experimental.ops.writers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To write TFRecords to disk, use `tf.io.TFRecordWriter`. To save and load the contents of a dataset, use `tf.data.experimental.save` and `tf.data.experimental.load`\n",
      "./training_data_processed/train/part-00021.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00015.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00024.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00023.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00003.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00001.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00002.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00020.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00051.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00000.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00033.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00004.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00017.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00018.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00005.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00006.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00022.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00038.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00007.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00008.parquet\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00025.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00009.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00026.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00011.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00010.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00016.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00019.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00013.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00030.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00014.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00036.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00028.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00027.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00029.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00031.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "./training_data_processed/train/part-00056.parquet\n",
      "------------tokenizing input\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in glob.glob(\"./training_data_processed/train/*\"):\n",
    "    print(f\"{i}\")\n",
    "    create_tfrecords_dataset(i, int(i.split(\"part\")[1][1:6]), 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d3acb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - ./training_data_processed/validation/part-00000.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 08:19:28.566248: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------writing to tfrecord\n",
      "WARNING:tensorflow:From /tmp/ipykernel_99895/3700649595.py:32: TFRecordWriter.__init__ (from tensorflow.python.data.experimental.ops.writers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To write TFRecords to disk, use `tf.io.TFRecordWriter`. To save and load the contents of a dataset, use `tf.data.experimental.save` and `tf.data.experimental.load`\n",
      "1 - ./training_data_processed/validation/part-00001.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "2 - ./training_data_processed/validation/part-00002.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "3 - ./training_data_processed/validation/part-00003.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "4 - ./training_data_processed/validation/part-00004.parquet\n",
      "------------tokenizing input\n",
      "------------tokenizing output\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "CPU times: user 1min 48s, sys: 12.1 s, total: 2min\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for ith, i in enumerate(glob.glob(\"./training_data_processed/validation/*\")):\n",
    "    print(f\"{ith} - {i}\")\n",
    "    create_tfrecords_dataset(i, ith, 'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d9465",
   "metadata": {},
   "source": [
    "### Read training data from tfrecord files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e541808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "    \"\"\"\n",
    "    Parses the TFRecord file.\n",
    "    \"\"\"\n",
    "    feature_description = {\n",
    "        'inputs': tf.io.FixedLenFeature((32,), tf.int64),\n",
    "        'outputs': tf.io.FixedLenFeature((32,), tf.int64),\n",
    "        'labels': tf.io.FixedLenFeature((32,), tf.int64)\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    inputs = tf.cast(example['inputs'], dtype=tf.int32)\n",
    "    outputs = tf.cast(example['outputs'], dtype=tf.int32)\n",
    "    labels = tf.cast(example['labels'], dtype=tf.int32)\n",
    "\n",
    "    return (inputs,outputs),labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c9fdae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path, data_type='train', batch_size=512):\n",
    "    \"\"\"\n",
    "    Takes in a path to the TFRecords and returns a TF Dataset to be used for training.\n",
    "    \"\"\"\n",
    "    if data_type=='train':\n",
    "        tfrecords = [f\"{path}{data_type}/{x}\" for x in os.listdir(f\"{path}{data_type}/\") \n",
    "                     if x.endswith('tfrecord')][:25]\n",
    "    else:\n",
    "        tfrecords = [f\"{path}{data_type}/{x}\" for x in os.listdir(f\"{path}{data_type}/\") \n",
    "                     if x.endswith('tfrecord')][:2]\n",
    "    tfrecords.sort()\n",
    "    \n",
    "    \n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecords, num_parallel_reads=tf.data.AUTOTUNE)\n",
    "    parsed_dataset = raw_dataset.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    parsed_dataset = parsed_dataset\\\n",
    "        .shuffle(batch_size)\\\n",
    "        .batch(batch_size,drop_remainder=True) \\\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return parsed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f8cc6",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b518395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "    angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1) \n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b728187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'vocab_size': int(self.vocab_size),\n",
    "            'd_model': int(self.d_model)\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9edf72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        \n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "    \n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask = True)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f043b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff, activation='relu'),\n",
    "          tf.keras.layers.Dense(d_model),\n",
    "          tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x) \n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'd_model': int(self.d_model),\n",
    "            'dff': int(self.dff),\n",
    "            'dropout_rate': float(self.dropout_rate)\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b685b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        self.context_data = x\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'd_model': int(self.d_model),\n",
    "            'num_heads': int(self.num_heads),\n",
    "            'dff': int(self.dff),\n",
    "            'dropout_rate': float(self.dropout_rate)\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cba5290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads,\n",
    "                 dff, vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(\n",
    "            vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # `x` is token-IDs shape: (batch, seq_len)\n",
    "        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "            \n",
    "        self.last_1of4_encoder_output = self.enc_layers[-4].context_data\n",
    "        self.last_2of4_encoder_output = self.enc_layers[-3].context_data\n",
    "        self.last_3of4_encoder_output = self.enc_layers[-2].context_data\n",
    "        self.last_4of4_encoder_output = self.enc_layers[-1].context_data\n",
    "        \n",
    "        self.encoder_context_data = x\n",
    "\n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_layers': int(self.num_layers),\n",
    "            'd_model': int(self.d_model),\n",
    "            'num_heads': int(self.num_heads),\n",
    "            'dff': int(self.dff),\n",
    "            'vocab_size': int(self.vocab_size),\n",
    "            'dropout_rate': float(self.dropout_rate)\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe2edf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context):\n",
    "        x = self.causal_self_attention(x=x)\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "        # Cache the last attention scores for plotting later\n",
    "        self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        self.context_data = x\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'd_model': int(self.d_model),\n",
    "            'num_heads': int(self.num_heads),\n",
    "            'dff': int(self.dff),\n",
    "            'dropout_rate': float(self.dropout_rate)\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "980bcd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                                 d_model=d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=self.d_model, num_heads=num_heads,\n",
    "                         dff=self.dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "        # `x` is token-IDs shape (batch, target_seq_len)\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x  = self.dec_layers[i](x, context)\n",
    "\n",
    "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "        self.last_1of4_decoder_output = self.dec_layers[-4].context_data\n",
    "        self.last_2of4_decoder_output = self.dec_layers[-3].context_data\n",
    "        self.last_3of4_decoder_output = self.dec_layers[-2].context_data\n",
    "        self.last_4of4_decoder_output = self.dec_layers[-1].context_data\n",
    "\n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        \n",
    "        self.decoder_context_data = x\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_layers': int(self.num_layers),\n",
    "            'd_model': int(self.d_model),\n",
    "            'num_heads': int(self.num_heads),\n",
    "            'dff': int(self.dff),\n",
    "            'vocab_size': int(self.vocab_size),\n",
    "            'dropout_rate': float(self.dropout_rate)\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "982e862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=input_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=target_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "        self.decoder_output = None\n",
    "        self.encoder_output = None\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "        # first argument.\n",
    "        context, x  = inputs\n",
    "\n",
    "        context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "        self.encoder_output = context\n",
    "\n",
    "        x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "        self.decoder_output = x\n",
    "\n",
    "        # Final linear layer output.\n",
    "        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "        try:\n",
    "          # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "          # b/250038731\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        # Return the final output and the attention weights.\n",
    "        return logits\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_layers': int(self.num_layers),\n",
    "            'd_model': int(self.d_model),\n",
    "            'num_heads': int(self.num_heads),\n",
    "            'dff': int(self.dff),\n",
    "            'input_vocab_size': int(self.input_vocab_size),\n",
    "            'target_vocab_size': int(self.target_vocab_size),\n",
    "            'dropout_rate': float(self.dropout_rate)\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2174d1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7945f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'd_model': int(self.d_model),\n",
    "            'warmup_steps': int(self.warmup_steps),\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4aacc3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b6241a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, curr_lr):\n",
    "    \"\"\"\n",
    "    Setting up a exponentially decaying learning rate.\n",
    "    \"\"\"\n",
    "    rampup_epochs = 2\n",
    "    exp_decay = 0.17\n",
    "    def lr(epoch, beg_lr, rampup_epochs, exp_decay):\n",
    "        if epoch < rampup_epochs:\n",
    "            return beg_lr*4*epoch\n",
    "        else:\n",
    "            return beg_lr * math.exp(-exp_decay * epoch)\n",
    "    return lr(epoch, start_lr, rampup_epochs, exp_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64021114",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 6\n",
    "d_model = 64\n",
    "dff = 128\n",
    "num_heads = 8\n",
    "dropout_rate = 0.15\n",
    "num_epochs = 12\n",
    "MAX_LEN=32\n",
    "BATCH_SIZE=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13eafc8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 20:11:20.313645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8500\n"
     ]
    }
   ],
   "source": [
    "# Allow for use of multiple GPUs\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    \n",
    "    transformer = Transformer(\n",
    "        num_layers=num_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dff=dff,\n",
    "        input_vocab_size=name_tokenizer.vocab_size,\n",
    "        target_vocab_size=name_tokenizer.vocab_size,\n",
    "        dropout_rate=dropout_rate)\n",
    "    \n",
    "    learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                         epsilon=1e-9)\n",
    "\n",
    "    transformer.compile(\n",
    "        loss=masked_loss,\n",
    "        optimizer=optimizer,\n",
    "        metrics=[masked_accuracy])\n",
    "\n",
    "    curr_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "    filepath_1 = f\"./models/{curr_date}_{num_heads}_{num_layers}_{d_model}_{dff}_{int(dropout_rate*100)}/\" \\\n",
    "\n",
    "\n",
    "    filepath = filepath_1 + \"model_epoch{epoch:02d}ckpt\"\n",
    "\n",
    "    # Adding in checkpointing\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', \n",
    "                                                          verbose=0, save_best_only=False,\n",
    "                                                          save_weights_only=False, mode='auto',\n",
    "                                                          save_freq='epoch')\n",
    "\n",
    "    # Adding in early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=4)\n",
    "\n",
    "#     lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "    callbacks = [model_checkpoint, early_stopping]\n",
    "    \n",
    "    train_batches = get_dataset(\"./tfrecords/\", data_type='train', batch_size=BATCH_SIZE)\n",
    "    val_batches = get_dataset(\"./tfrecords/\", data_type='validation', batch_size=BATCH_SIZE)\n",
    "    \n",
    "    for (raw_input_example, output_name), output_labels in train_batches.take(1):\n",
    "        pass\n",
    "        \n",
    "    output = transformer((raw_input_example, output_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7ccc8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_1 (Encoder)         multiple                  1873920   \n",
      "                                                                 \n",
      " decoder_1 (Decoder)         multiple                  2670720   \n",
      "                                                                 \n",
      " dense_49 (Dense)            multiple                  992160    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,536,800\n",
      "Trainable params: 5,536,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6455292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "INFO:tensorflow:batch_all_reduce: 254 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 254 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 20:55:55.054702: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8500\n",
      "2023-03-30 20:55:55.556292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8500\n",
      "2023-03-30 20:55:55.913813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8500\n",
      "2023-03-30 20:55:56.047107: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fa1d8003930 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-30 20:55:56.047150: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2023-03-30 20:55:56.047161: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2023-03-30 20:55:56.047168: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (2): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2023-03-30 20:55:56.047174: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (3): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2023-03-30 20:55:56.055012: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-03-30 20:55:56.229849: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20256/Unknown - 2702s 126ms/step - loss: 0.8727 - masked_accuracy: 0.8431"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19024/54329 [=========>....................] - ETA: 1:14:35 - loss: 0.3838 - masked_accuracy: 0.9097"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26598/54329 [=============>................] - ETA: 58:36 - loss: 0.3835 - masked_accuracy: 0.9098"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35518/54329 [==================>...........] - ETA: 39:45 - loss: 0.3831 - masked_accuracy: 0.9098"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42551/54329 [======================>.......] - ETA: 24:53 - loss: 0.3828 - masked_accuracy: 0.9099"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50248/54329 [==========================>...] - ETA: 8:37 - loss: 0.3824 - masked_accuracy: 0.9099"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54003/54329 [============================>.] - ETA: 41s - loss: 0.3823 - masked_accuracy: 0.9100"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7643/54329 [===>..........................] - ETA: 1:38:34 - loss: 0.3797 - masked_accuracy: 0.9103"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54329/54329 [==============================] - ETA: 0s - loss: 0.3785 - masked_accuracy: 0.9105"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as positional_embedding_layer_call_fn, positional_embedding_layer_call_and_return_conditional_losses, dropout_6_layer_call_fn, dropout_6_layer_call_and_return_conditional_losses, positional_embedding_1_layer_call_fn while saving (showing 5 of 468). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/20230330_8_6_64_128_15/model_epoch05ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/20230330_8_6_64_128_15/model_epoch05ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54329/54329 [==============================] - 6912s 127ms/step - loss: 0.3785 - masked_accuracy: 0.9105 - val_loss: 0.3527 - val_masked_accuracy: 0.9148\n",
      "Epoch 6/12\n",
      " 1367/54329 [..............................] - ETA: 1:52:16 - loss: 0.3764 - masked_accuracy: 0.9109"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9007/54329 [===>..........................] - ETA: 1:35:57 - loss: 0.3769 - masked_accuracy: 0.9108"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16809/54329 [========>.....................] - ETA: 1:19:26 - loss: 0.3767 - masked_accuracy: 0.9108"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24705/54329 [============>.................] - ETA: 1:02:43 - loss: 0.3765 - masked_accuracy: 0.9108"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32321/54329 [================>.............] - ETA: 46:36 - loss: 0.3764 - masked_accuracy: 0.9108"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35547/54329 [==================>...........] - ETA: 39:55 - loss: 0.3724 - masked_accuracy: 0.9115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43384/54329 [======================>.......] - ETA: 23:15 - loss: 0.3723 - masked_accuracy: 0.9115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50968/54329 [===========================>..] - ETA: 7:08 - loss: 0.3722 - masked_accuracy: 0.9115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3686/54329 [=>............................] - ETA: 1:47:05 - loss: 0.3709 - masked_accuracy: 0.9117"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49802/54329 [==========================>...] - ETA: 9:33 - loss: 0.3709 - masked_accuracy: 0.9117"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54329/54329 [==============================] - ETA: 0s - loss: 0.3708 - masked_accuracy: 0.9117"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as positional_embedding_layer_call_fn, positional_embedding_layer_call_and_return_conditional_losses, dropout_6_layer_call_fn, dropout_6_layer_call_and_return_conditional_losses, positional_embedding_1_layer_call_fn while saving (showing 5 of 468). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/20230330_8_6_64_128_15/model_epoch09ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/20230330_8_6_64_128_15/model_epoch09ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54329/54329 [==============================] - 6931s 128ms/step - loss: 0.3708 - masked_accuracy: 0.9117 - val_loss: 0.3472 - val_masked_accuracy: 0.9160\n",
      "Epoch 10/12\n",
      "22930/54329 [===========>..................] - ETA: 1:06:44 - loss: 0.3700 - masked_accuracy: 0.9118"
     ]
    }
   ],
   "source": [
    "history = transformer.fit(train_batches,\n",
    "                epochs=num_epochs,\n",
    "                validation_data=val_batches, \n",
    "                callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "843c37c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89382f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(str(history.history), open(f\"{filepath_1}_{num_epochs}EPOCHS_HISTORY.json\", 'w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c989aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79e93ed",
   "metadata": {},
   "source": [
    "### Transforming with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49810dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS=32\n",
    "MAX_LEN=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5cf9472d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n"
     ]
    }
   ],
   "source": [
    "transformer.set_weights(tf.keras.models.load_model(\"./models/20230330_8_6_64_128_15/model_epoch08ckpt/\", \n",
    "                                                    compile=False).get_weights()) \n",
    "transformer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b98ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = tf.keras.Model(inputs=(x,y), \n",
    "#                            outputs=transformer.decoder.dec_layers[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "824a2b96",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_TOKENS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTranslator\u001b[39;00m(tf\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokenizer, transformer):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n",
      "Cell \u001b[0;32mIn[29], line 6\u001b[0m, in \u001b[0;36mTranslator\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m transformer\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence, max_length\u001b[38;5;241m=\u001b[39m\u001b[43mMAX_TOKENS\u001b[49m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sentence, tf\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sentence\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_TOKENS' is not defined"
     ]
    }
   ],
   "source": [
    "class Translator(tf.Module):\n",
    "    def __init__(self, tokenizer, transformer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __call__(self, sentence, max_length=MAX_TOKENS):\n",
    "        # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\n",
    "        assert isinstance(sentence, tf.Tensor)\n",
    "        if len(sentence.shape) == 0:\n",
    "            sentence = sentence[tf.newaxis]\n",
    "\n",
    "        sentence = tf.keras.utils.pad_sequences(self.tokenizer([x.decode('utf-8') \n",
    "                                                    for x in sentence.numpy().tolist()])['input_ids'], \n",
    "                                                maxlen=MAX_TOKENS, \n",
    "                                                dtype='int32',\n",
    "                                                padding='post',\n",
    "                                                truncating='post',\n",
    "                                                value=0)\n",
    "\n",
    "        encoder_input = sentence\n",
    "\n",
    "        # As the output language is English, initialize the output with the\n",
    "        # English `[START]` token.\n",
    "        start_end = tf.constant(self.tokenizer(['[START][END]'])['input_ids'][0])\n",
    "        start = start_end[0][tf.newaxis]\n",
    "        end = start_end[1][tf.newaxis]\n",
    "\n",
    "        # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "        # dynamic-loop can be traced by `tf.function`.\n",
    "        output_array = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "        output_array = output_array.write(0, start)\n",
    "\n",
    "        for i in tf.range(max_length):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "            # Select the last token from the `seq_len` dimension.\n",
    "            predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "            predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "            # Concatenate the `predicted_id` to the output which is given to the\n",
    "            # decoder as its input.\n",
    "            output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "            if predicted_id == end:\n",
    "                break\n",
    "\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        # The output shape is `(1, tokens)`.\n",
    "        print(output)\n",
    "        text = self.tokenizer.decode(output.numpy().tolist()[0], skip_special_tokens=True, \n",
    "                 clean_up_tokenization_spaces=True).replace(\" ##\", \"\").replace(\" - \", \"-\")\n",
    "\n",
    "        tokens = output.numpy().tolist()\n",
    "\n",
    "        # `tf.function` prevents us from using the attention_weights that were\n",
    "        # calculated on the last iteration of the loop.\n",
    "        # So, recalculate them outside the loop.\n",
    "        self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "        attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "        return text, tokens, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "277196f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"name_transformer_wordpiece_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e518ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_tokenizer.decode(output_labels[4][:10], skip_special_tokens=True, \n",
    "#                  clean_up_tokenization_spaces=True).replace(\" ##\", \"\").replace(\" - \", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eef68848",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(name_tokenizer, trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c99aa06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translation(sentence, tokens, ground_truth):\n",
    "    print(f'{\"Input:\":15s}: {sentence}')\n",
    "    print(f'{\"Prediction\":15s}: {tokens}')\n",
    "    print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf571f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '[START]Oliver, John[END]'\n",
    "ground_truth = 'John Oliver'\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e049c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 32, 15264), dtype=float32, numpy=\n",
       "array([[[-10.503152  , -10.504201  ,   6.169404  , ...,  -8.845235  ,\n",
       "          -8.071333  ,  -5.4130135 ],\n",
       "        [ -5.9436817 ,  -5.9438343 ,  -1.4243493 , ...,   2.472556  ,\n",
       "           4.703054  ,   0.6202928 ],\n",
       "        [ -5.944227  ,  -5.9443827 ,  -1.4250772 , ...,   2.4673195 ,\n",
       "           4.7044845 ,   0.62325895],\n",
       "        ...,\n",
       "        [ -5.96473   ,  -5.9648876 ,  -1.377768  , ...,   2.4583683 ,\n",
       "           4.662452  ,   0.54505575],\n",
       "        [ -5.965795  ,  -5.9659495 ,  -1.3787947 , ...,   2.4593058 ,\n",
       "           4.6576986 ,   0.5326568 ],\n",
       "        [ -5.965687  ,  -5.9658384 ,  -1.3768545 , ...,   2.4614625 ,\n",
       "           4.658391  ,   0.53515875]]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63868233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(1, 32) dtype=int32 (created by layer 'input_2')>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8af71a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names = [\"J.R. Tolkien\",\n",
    "              \"Tolkien, J.R.\",\n",
    "              \"Justin Earl Tolkien\", \n",
    "              \"Jarvis Richard Tolkien\",\n",
    "              \"Tolkien, Justin\", \n",
    "              \"Tolkien JE\", \n",
    "              \"Max Trout\", \n",
    "              \"Maximus Trout\", \n",
    "              \"Maxe Trout\", \n",
    "              \"Mara Trout\", \n",
    "              \"M.R. Trout\",\n",
    "              \"Trout MRF\", \n",
    "              \"Gooding, Emily\", \n",
    "              \"Gooding, E.Y. MD\", \n",
    "              \"Jarvis Richard James\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca0614a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20bc02c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J.R. Tolkien\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Tolkien, J.R.\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Justin Earl Tolkien\n",
      "----Decoded name:  Justin Earl Tolkien\n",
      "Jarvis Richard Tolkien\n",
      "----Decoded name:  Jarvis Richard Tolkien\n",
      "Tolkien, Justin\n",
      "----Decoded name:  Justin Tolkien\n",
      "Tolkien JE\n",
      "----Decoded name:  J. E. Tolkien\n",
      "Max Trout\n",
      "----Decoded name:  Max Trout\n",
      "Maximus Trout\n",
      "----Decoded name:  Maximus Trout\n",
      "Maxe Trout\n",
      "----Decoded name:  Maxe Trout\n",
      "Mara Trout\n",
      "----Decoded name:  Mara Trout\n",
      "M.R. Trout\n",
      "----Decoded name:  M. R. Trout\n",
      "Trout MRF\n",
      "----Decoded name:  M. R. F. Trout\n",
      "Gooding, Emily\n",
      "----Decoded name:  Emily Gooding\n",
      "Gooding, E.Y. MD\n",
      "----Decoded name:  E. Y. Gooding\n",
      "Jarvis Richard James\n",
      "----Decoded name:  Richard James Jarvis\n"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "for test_name in test_names:\n",
    "    print(test_name)\n",
    "    start_end = tf.constant(name_tokenizer(['[START][END]'])['input_ids'][0])\n",
    "    start = start_end[0].numpy()\n",
    "    end = start_end[1].numpy()\n",
    "    encoder_input = name_tokenizer([f'[START]{test_name}[END]'])['input_ids'][0]\n",
    "    encoder_input = tf.convert_to_tensor([encoder_input + [0]*(MAX_LEN-len(encoder_input))])\n",
    "    final_output = [start]\n",
    "    for i in range(MAX_LEN):\n",
    "        output = tf.convert_to_tensor([final_output + [0]*(MAX_LEN-len(final_output))])\n",
    "        predictions = transformer((encoder_input, output), training=False)\n",
    "\n",
    "        # Select the last token\n",
    "        predicted_id = tf.argmax(predictions[0][i]).numpy()\n",
    "\n",
    "        # Add to output\n",
    "        final_output.append(predicted_id)\n",
    "\n",
    "        if predicted_id == end:\n",
    "            break\n",
    "    print(\"----Decoded name: \", name_tokenizer.decode(final_output, skip_special_tokens=True, \n",
    "                 clean_up_tokenization_spaces=True).replace(\" ##\", \"\").replace(\" - \", \"-\"))\n",
    "    _ = transformer((encoder_input, tf.convert_to_tensor([final_output[:1] + [0]*(MAX_LEN-len(final_output[:1]))])), \n",
    "            training=False)\n",
    "    \n",
    "    embs.append(transformer.decoder.dec_layers[-1].context_data[0][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01469443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f7303cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 J.R. Tolkien\n",
      "-------J.R. Tolkien: 1.0\n",
      "-------Tolkien, J.R.: 0.6725000143051147\n",
      "-------Justin Earl Tolkien: 0.5468999743461609\n",
      "-------Jarvis Richard Tolkien: 0.5748000144958496\n",
      "-------Tolkien, Justin: 0.43149998784065247\n",
      "-------Tolkien JE: 0.5461999773979187\n",
      "-------Max Trout: 0.24279999732971191\n",
      "-------Maximus Trout: 0.25119999051094055\n",
      "-------Maxe Trout: 0.2468000054359436\n",
      "-------Mara Trout: 0.1137000024318695\n",
      "-------M.R. Trout: 0.41499999165534973\n",
      "-------Trout MRF: 0.16910000145435333\n",
      "-------Gooding, Emily: 0.29100000858306885\n",
      "-------Gooding, E.Y. MD: 0.27379998564720154\n",
      "-------Jarvis Richard James: 0.5273000001907349\n",
      "\n",
      "\n",
      "1 Tolkien, J.R.\n",
      "-------J.R. Tolkien: 0.6725000143051147\n",
      "-------Tolkien, J.R.: 1.0\n",
      "-------Justin Earl Tolkien: 0.6068999767303467\n",
      "-------Jarvis Richard Tolkien: 0.47290000319480896\n",
      "-------Tolkien, Justin: 0.43050000071525574\n",
      "-------Tolkien JE: 0.5874999761581421\n",
      "-------Max Trout: 0.40700000524520874\n",
      "-------Maximus Trout: 0.44209998846054077\n",
      "-------Maxe Trout: 0.4047999978065491\n",
      "-------Mara Trout: 0.19189999997615814\n",
      "-------M.R. Trout: 0.15970000624656677\n",
      "-------Trout MRF: 0.373199999332428\n",
      "-------Gooding, Emily: 0.2827000021934509\n",
      "-------Gooding, E.Y. MD: 0.4147999882698059\n",
      "-------Jarvis Richard James: 0.3686999976634979\n",
      "\n",
      "\n",
      "2 Justin Earl Tolkien\n",
      "-------J.R. Tolkien: 0.5468999743461609\n",
      "-------Tolkien, J.R.: 0.6068999767303467\n",
      "-------Justin Earl Tolkien: 1.0\n",
      "-------Jarvis Richard Tolkien: 0.27790001034736633\n",
      "-------Tolkien, Justin: 0.8851000070571899\n",
      "-------Tolkien JE: 0.6025999784469604\n",
      "-------Max Trout: 0.34279999136924744\n",
      "-------Maximus Trout: 0.39590001106262207\n",
      "-------Maxe Trout: 0.35019999742507935\n",
      "-------Mara Trout: 0.3416000008583069\n",
      "-------M.R. Trout: 0.20149999856948853\n",
      "-------Trout MRF: 0.2994000017642975\n",
      "-------Gooding, Emily: 0.41999998688697815\n",
      "-------Gooding, E.Y. MD: 0.4426000118255615\n",
      "-------Jarvis Richard James: 0.13210000097751617\n",
      "\n",
      "\n",
      "3 Jarvis Richard Tolkien\n",
      "-------J.R. Tolkien: 0.5748000144958496\n",
      "-------Tolkien, J.R.: 0.47290000319480896\n",
      "-------Justin Earl Tolkien: 0.27790001034736633\n",
      "-------Jarvis Richard Tolkien: 1.0\n",
      "-------Tolkien, Justin: 0.2062000036239624\n",
      "-------Tolkien JE: 0.17299999296665192\n",
      "-------Max Trout: 0.49559998512268066\n",
      "-------Maximus Trout: 0.5005000233650208\n",
      "-------Maxe Trout: 0.5016999840736389\n",
      "-------Mara Trout: 0.2685999870300293\n",
      "-------M.R. Trout: 0.3147999942302704\n",
      "-------Trout MRF: 0.3944999873638153\n",
      "-------Gooding, Emily: 0.3285999894142151\n",
      "-------Gooding, E.Y. MD: 0.14219999313354492\n",
      "-------Jarvis Richard James: 0.9412000179290771\n",
      "\n",
      "\n",
      "4 Tolkien, Justin\n",
      "-------J.R. Tolkien: 0.43149998784065247\n",
      "-------Tolkien, J.R.: 0.43050000071525574\n",
      "-------Justin Earl Tolkien: 0.8851000070571899\n",
      "-------Jarvis Richard Tolkien: 0.2062000036239624\n",
      "-------Tolkien, Justin: 1.0\n",
      "-------Tolkien JE: 0.5234000086784363\n",
      "-------Max Trout: 0.2442999929189682\n",
      "-------Maximus Trout: 0.28049999475479126\n",
      "-------Maxe Trout: 0.24770000576972961\n",
      "-------Mara Trout: 0.22380000352859497\n",
      "-------M.R. Trout: 0.13199999928474426\n",
      "-------Trout MRF: 0.20730000734329224\n",
      "-------Gooding, Emily: 0.4636000096797943\n",
      "-------Gooding, E.Y. MD: 0.3345000147819519\n",
      "-------Jarvis Richard James: 0.12120000272989273\n",
      "\n",
      "\n",
      "5 Tolkien JE\n",
      "-------J.R. Tolkien: 0.5461999773979187\n",
      "-------Tolkien, J.R.: 0.5874999761581421\n",
      "-------Justin Earl Tolkien: 0.6025999784469604\n",
      "-------Jarvis Richard Tolkien: 0.17299999296665192\n",
      "-------Tolkien, Justin: 0.5234000086784363\n",
      "-------Tolkien JE: 1.0\n",
      "-------Max Trout: 0.23659999668598175\n",
      "-------Maximus Trout: 0.24619999527931213\n",
      "-------Maxe Trout: 0.23999999463558197\n",
      "-------Mara Trout: 0.08560000360012054\n",
      "-------M.R. Trout: 0.15940000116825104\n",
      "-------Trout MRF: 0.13199999928474426\n",
      "-------Gooding, Emily: 0.42149999737739563\n",
      "-------Gooding, E.Y. MD: 0.34450000524520874\n",
      "-------Jarvis Richard James: 0.09700000286102295\n",
      "\n",
      "\n",
      "6 Max Trout\n",
      "-------J.R. Tolkien: 0.24279999732971191\n",
      "-------Tolkien, J.R.: 0.40700000524520874\n",
      "-------Justin Earl Tolkien: 0.34279999136924744\n",
      "-------Jarvis Richard Tolkien: 0.49559998512268066\n",
      "-------Tolkien, Justin: 0.2442999929189682\n",
      "-------Tolkien JE: 0.23659999668598175\n",
      "-------Max Trout: 1.0\n",
      "-------Maximus Trout: 0.9915000200271606\n",
      "-------Maxe Trout: 0.9969000220298767\n",
      "-------Mara Trout: 0.5867999792098999\n",
      "-------M.R. Trout: 0.4205000102519989\n",
      "-------Trout MRF: 0.5200999975204468\n",
      "-------Gooding, Emily: 0.16439999639987946\n",
      "-------Gooding, E.Y. MD: 0.19859999418258667\n",
      "-------Jarvis Richard James: 0.37770000100135803\n",
      "\n",
      "\n",
      "7 Maximus Trout\n",
      "-------J.R. Tolkien: 0.25119999051094055\n",
      "-------Tolkien, J.R.: 0.44209998846054077\n",
      "-------Justin Earl Tolkien: 0.39590001106262207\n",
      "-------Jarvis Richard Tolkien: 0.5005000233650208\n",
      "-------Tolkien, Justin: 0.28049999475479126\n",
      "-------Tolkien JE: 0.24619999527931213\n",
      "-------Max Trout: 0.9915000200271606\n",
      "-------Maximus Trout: 1.0\n",
      "-------Maxe Trout: 0.9927999973297119\n",
      "-------Mara Trout: 0.5881999731063843\n",
      "-------M.R. Trout: 0.40139999985694885\n",
      "-------Trout MRF: 0.5159000158309937\n",
      "-------Gooding, Emily: 0.17730000615119934\n",
      "-------Gooding, E.Y. MD: 0.23360000550746918\n",
      "-------Jarvis Richard James: 0.3621000051498413\n",
      "\n",
      "\n",
      "8 Maxe Trout\n",
      "-------J.R. Tolkien: 0.2468000054359436\n",
      "-------Tolkien, J.R.: 0.4047999978065491\n",
      "-------Justin Earl Tolkien: 0.35019999742507935\n",
      "-------Jarvis Richard Tolkien: 0.5016999840736389\n",
      "-------Tolkien, Justin: 0.24770000576972961\n",
      "-------Tolkien JE: 0.23999999463558197\n",
      "-------Max Trout: 0.9969000220298767\n",
      "-------Maximus Trout: 0.9927999973297119\n",
      "-------Maxe Trout: 1.0\n",
      "-------Mara Trout: 0.6036999821662903\n",
      "-------M.R. Trout: 0.4293000102043152\n",
      "-------Trout MRF: 0.5156000256538391\n",
      "-------Gooding, Emily: 0.17759999632835388\n",
      "-------Gooding, E.Y. MD: 0.20080000162124634\n",
      "-------Jarvis Richard James: 0.385699987411499\n",
      "\n",
      "\n",
      "9 Mara Trout\n",
      "-------J.R. Tolkien: 0.1137000024318695\n",
      "-------Tolkien, J.R.: 0.19189999997615814\n",
      "-------Justin Earl Tolkien: 0.3416000008583069\n",
      "-------Jarvis Richard Tolkien: 0.2685999870300293\n",
      "-------Tolkien, Justin: 0.22380000352859497\n",
      "-------Tolkien JE: 0.08560000360012054\n",
      "-------Max Trout: 0.5867999792098999\n",
      "-------Maximus Trout: 0.5881999731063843\n",
      "-------Maxe Trout: 0.6036999821662903\n",
      "-------Mara Trout: 1.0\n",
      "-------M.R. Trout: 0.5728999972343445\n",
      "-------Trout MRF: 0.6377000212669373\n",
      "-------Gooding, Emily: 0.29789999127388\n",
      "-------Gooding, E.Y. MD: 0.2092999964952469\n",
      "-------Jarvis Richard James: 0.188400000333786\n",
      "\n",
      "\n",
      "10 M.R. Trout\n",
      "-------J.R. Tolkien: 0.41499999165534973\n",
      "-------Tolkien, J.R.: 0.15970000624656677\n",
      "-------Justin Earl Tolkien: 0.20149999856948853\n",
      "-------Jarvis Richard Tolkien: 0.3147999942302704\n",
      "-------Tolkien, Justin: 0.13199999928474426\n",
      "-------Tolkien JE: 0.15940000116825104\n",
      "-------Max Trout: 0.4205000102519989\n",
      "-------Maximus Trout: 0.40139999985694885\n",
      "-------Maxe Trout: 0.4293000102043152\n",
      "-------Mara Trout: 0.5728999972343445\n",
      "-------M.R. Trout: 1.0\n",
      "-------Trout MRF: 0.558899998664856\n",
      "-------Gooding, Emily: 0.32100000977516174\n",
      "-------Gooding, E.Y. MD: 0.1136000007390976\n",
      "-------Jarvis Richard James: 0.33160001039505005\n",
      "\n",
      "\n",
      "11 Trout MRF\n",
      "-------J.R. Tolkien: 0.16910000145435333\n",
      "-------Tolkien, J.R.: 0.373199999332428\n",
      "-------Justin Earl Tolkien: 0.2994000017642975\n",
      "-------Jarvis Richard Tolkien: 0.3944999873638153\n",
      "-------Tolkien, Justin: 0.20730000734329224\n",
      "-------Tolkien JE: 0.13199999928474426\n",
      "-------Max Trout: 0.5200999975204468\n",
      "-------Maximus Trout: 0.5159000158309937\n",
      "-------Maxe Trout: 0.5156000256538391\n",
      "-------Mara Trout: 0.6377000212669373\n",
      "-------M.R. Trout: 0.558899998664856\n",
      "-------Trout MRF: 1.0\n",
      "-------Gooding, Emily: 0.23250000178813934\n",
      "-------Gooding, E.Y. MD: 0.164000004529953\n",
      "-------Jarvis Richard James: 0.3889999985694885\n",
      "\n",
      "\n",
      "12 Gooding, Emily\n",
      "-------J.R. Tolkien: 0.29100000858306885\n",
      "-------Tolkien, J.R.: 0.2827000021934509\n",
      "-------Justin Earl Tolkien: 0.41999998688697815\n",
      "-------Jarvis Richard Tolkien: 0.3285999894142151\n",
      "-------Tolkien, Justin: 0.4636000096797943\n",
      "-------Tolkien JE: 0.42149999737739563\n",
      "-------Max Trout: 0.16439999639987946\n",
      "-------Maximus Trout: 0.17730000615119934\n",
      "-------Maxe Trout: 0.17759999632835388\n",
      "-------Mara Trout: 0.29789999127388\n",
      "-------M.R. Trout: 0.32100000977516174\n",
      "-------Trout MRF: 0.23250000178813934\n",
      "-------Gooding, Emily: 1.0\n",
      "-------Gooding, E.Y. MD: 0.3529999852180481\n",
      "-------Jarvis Richard James: 0.32179999351501465\n",
      "\n",
      "\n",
      "13 Gooding, E.Y. MD\n",
      "-------J.R. Tolkien: 0.27379998564720154\n",
      "-------Tolkien, J.R.: 0.4147999882698059\n",
      "-------Justin Earl Tolkien: 0.4426000118255615\n",
      "-------Jarvis Richard Tolkien: 0.14219999313354492\n",
      "-------Tolkien, Justin: 0.3345000147819519\n",
      "-------Tolkien JE: 0.34450000524520874\n",
      "-------Max Trout: 0.19859999418258667\n",
      "-------Maximus Trout: 0.23360000550746918\n",
      "-------Maxe Trout: 0.20080000162124634\n",
      "-------Mara Trout: 0.2092999964952469\n",
      "-------M.R. Trout: 0.1136000007390976\n",
      "-------Trout MRF: 0.164000004529953\n",
      "-------Gooding, Emily: 0.3529999852180481\n",
      "-------Gooding, E.Y. MD: 1.0\n",
      "-------Jarvis Richard James: 0.08030000329017639\n",
      "\n",
      "\n",
      "14 Jarvis Richard James\n",
      "-------J.R. Tolkien: 0.5273000001907349\n",
      "-------Tolkien, J.R.: 0.3686999976634979\n",
      "-------Justin Earl Tolkien: 0.13210000097751617\n",
      "-------Jarvis Richard Tolkien: 0.9412000179290771\n",
      "-------Tolkien, Justin: 0.12120000272989273\n",
      "-------Tolkien JE: 0.09700000286102295\n",
      "-------Max Trout: 0.37770000100135803\n",
      "-------Maximus Trout: 0.3621000051498413\n",
      "-------Maxe Trout: 0.385699987411499\n",
      "-------Mara Trout: 0.188400000333786\n",
      "-------M.R. Trout: 0.33160001039505005\n",
      "-------Trout MRF: 0.3889999985694885\n",
      "-------Gooding, Emily: 0.32179999351501465\n",
      "-------Gooding, E.Y. MD: 0.08030000329017639\n",
      "-------Jarvis Richard James: 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ith, (emb, test_name) in enumerate(zip(embs, test_names)):\n",
    "    print(ith, test_name)\n",
    "    for emb_1, test_name_1 in zip(embs, test_names):\n",
    "        print(f\"-------{test_name_1}: {round(cosine_similarity(emb.reshape(1, -1), emb_1.reshape(1, -1))[0][0], 4)}\")\n",
    "    print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d5a069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d5d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb429936",
   "metadata": {},
   "source": [
    "## Testing different embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47ee9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f126a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_embeddings(embs_to_test, names_to_test):\n",
    "    for ith, (emb, test_name) in enumerate(zip(embs_to_test, names_to_test)):\n",
    "        all_scores = []\n",
    "        print(ith, test_name)\n",
    "        for emb_1, test_name_1 in zip(embs, names_to_test):\n",
    "            all_scores.append(round(cosine_similarity(emb.reshape(1, -1), emb_1.reshape(1, -1))[0][0], 4))\n",
    "        \n",
    "        ind = np.argpartition(np.array(all_scores), -5)[-5:]\n",
    "        \n",
    "        top_5 = ind[np.argsort(np.array(all_scores)[ind])].tolist()[::-1]\n",
    "        \n",
    "        for top_ind in top_5[1:]:\n",
    "            print(f\"---------- {names_to_test[top_ind]} - {all_scores[top_ind]}\")\n",
    "        print(\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b9e14",
   "metadata": {},
   "source": [
    "### [START] token decoder output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7669683f",
   "metadata": {},
   "source": [
    "##### Last layer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36ac1382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J.R. Tolkien\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Tolkien, J.R.\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Justin Earl Tolkien\n",
      "----Decoded name:  Justin Earl Tolkien\n",
      "Jarvis Richard Tolkien\n",
      "----Decoded name:  Jarvis Richard Tolkien\n",
      "Tolkien, Justin\n",
      "----Decoded name:  Justin Tolkien\n",
      "Tolkien JE\n",
      "----Decoded name:  J. E. Tolkien\n",
      "Max Trout\n",
      "----Decoded name:  Max Trout\n",
      "Maximus Trout\n",
      "----Decoded name:  Maximus Trout\n",
      "Maxe Trout\n",
      "----Decoded name:  Maxe Trout\n",
      "Mara Trout\n",
      "----Decoded name:  Mara Trout\n",
      "M.R. Trout\n",
      "----Decoded name:  M. R. Trout\n",
      "Trout MRF\n",
      "----Decoded name:  M. R. F. Trout\n",
      "Gooding, Emily\n",
      "----Decoded name:  Emily Gooding\n",
      "Gooding, E.Y. MD\n",
      "----Decoded name:  E. Y. Gooding\n",
      "Jarvis Richard James\n",
      "----Decoded name:  Richard James Jarvis\n"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "for test_name in test_names:\n",
    "    print(test_name)\n",
    "    start_end = tf.constant(name_tokenizer(['[START][END]'])['input_ids'][0])\n",
    "    start = start_end[0].numpy()\n",
    "    end = start_end[1].numpy()\n",
    "    encoder_input = name_tokenizer([f'[START]{test_name}[END]'])['input_ids'][0]\n",
    "    encoder_input = tf.convert_to_tensor([encoder_input + [0]*(MAX_LEN-len(encoder_input))])\n",
    "    final_output = [start]\n",
    "    for i in range(MAX_LEN):\n",
    "        output = tf.convert_to_tensor([final_output + [0]*(MAX_LEN-len(final_output))])\n",
    "        predictions = transformer((encoder_input, output), training=False)\n",
    "\n",
    "        # Select the last token\n",
    "        predicted_id = tf.argmax(predictions[0][i]).numpy()\n",
    "\n",
    "        # Add to output\n",
    "        final_output.append(predicted_id)\n",
    "\n",
    "        if predicted_id == end:\n",
    "            break\n",
    "    print(\"----Decoded name: \", name_tokenizer.decode(final_output, skip_special_tokens=True, \n",
    "                 clean_up_tokenization_spaces=True).replace(\" ##\", \"\").replace(\" - \", \"-\"))\n",
    "    _ = transformer((encoder_input, tf.convert_to_tensor([final_output[:1] + [0]*(MAX_LEN-len(final_output[:1]))])), \n",
    "            training=False)\n",
    "    \n",
    "    embs.append(transformer.decoder.dec_layers[-1].context_data[0][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b0b70f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 J.R. Tolkien\n",
      "---------- Tolkien, J.R. - 0.6912999749183655\n",
      "---------- Justin Earl Tolkien - 0.6347000002861023\n",
      "---------- Tolkien JE - 0.611299991607666\n",
      "---------- M.R. Trout - 0.5792999863624573\n",
      "\n",
      "\n",
      "1 Tolkien, J.R.\n",
      "---------- J.R. Tolkien - 0.6912999749183655\n",
      "---------- Tolkien JE - 0.525600016117096\n",
      "---------- Gooding, E.Y. MD - 0.4986000061035156\n",
      "---------- Mara Trout - 0.47290000319480896\n",
      "\n",
      "\n",
      "2 Justin Earl Tolkien\n",
      "---------- Tolkien, Justin - 0.7226999998092651\n",
      "---------- J.R. Tolkien - 0.6347000002861023\n",
      "---------- Gooding, E.Y. MD - 0.5054000020027161\n",
      "---------- Maximus Trout - 0.5034000277519226\n",
      "\n",
      "\n",
      "3 Jarvis Richard Tolkien\n",
      "---------- Jarvis Richard James - 0.9329000115394592\n",
      "---------- J.R. Tolkien - 0.5389999747276306\n",
      "---------- Mara Trout - 0.5076000094413757\n",
      "---------- Tolkien, J.R. - 0.44119998812675476\n",
      "\n",
      "\n",
      "4 Tolkien, Justin\n",
      "---------- Justin Earl Tolkien - 0.7226999998092651\n",
      "---------- J.R. Tolkien - 0.37369999289512634\n",
      "---------- Maximus Trout - 0.30410000681877136\n",
      "---------- Jarvis Richard Tolkien - 0.2897000014781952\n",
      "\n",
      "\n",
      "5 Tolkien JE\n",
      "---------- J.R. Tolkien - 0.611299991607666\n",
      "---------- Tolkien, J.R. - 0.525600016117096\n",
      "---------- Justin Earl Tolkien - 0.4142000079154968\n",
      "---------- M.R. Trout - 0.4099000096321106\n",
      "\n",
      "\n",
      "6 Max Trout\n",
      "---------- Maxe Trout - 0.9801999926567078\n",
      "---------- Maximus Trout - 0.9758999943733215\n",
      "---------- Mara Trout - 0.7402999997138977\n",
      "---------- M.R. Trout - 0.7146999835968018\n",
      "\n",
      "\n",
      "7 Maximus Trout\n",
      "---------- Max Trout - 0.9758999943733215\n",
      "---------- Maxe Trout - 0.9751999974250793\n",
      "---------- Mara Trout - 0.7709000110626221\n",
      "---------- M.R. Trout - 0.6603999733924866\n",
      "\n",
      "\n",
      "8 Maxe Trout\n",
      "---------- Max Trout - 0.9801999926567078\n",
      "---------- Maximus Trout - 0.9751999974250793\n",
      "---------- Mara Trout - 0.7487000226974487\n",
      "---------- M.R. Trout - 0.6480000019073486\n",
      "\n",
      "\n",
      "9 Mara Trout\n",
      "---------- Maximus Trout - 0.7709000110626221\n",
      "---------- Maxe Trout - 0.7487000226974487\n",
      "---------- Max Trout - 0.7402999997138977\n",
      "---------- M.R. Trout - 0.6108999848365784\n",
      "\n",
      "\n",
      "10 M.R. Trout\n",
      "---------- Max Trout - 0.7146999835968018\n",
      "---------- Maximus Trout - 0.6603999733924866\n",
      "---------- Maxe Trout - 0.6480000019073486\n",
      "---------- Mara Trout - 0.6108999848365784\n",
      "\n",
      "\n",
      "11 Trout MRF\n",
      "---------- Maxe Trout - 0.5691999793052673\n",
      "---------- Mara Trout - 0.5584999918937683\n",
      "---------- Maximus Trout - 0.5164999961853027\n",
      "---------- Max Trout - 0.504800021648407\n",
      "\n",
      "\n",
      "12 Gooding, Emily\n",
      "---------- Tolkien JE - 0.3190000057220459\n",
      "---------- Tolkien, Justin - 0.2304999977350235\n",
      "---------- Gooding, E.Y. MD - 0.22390000522136688\n",
      "---------- Trout MRF - 0.21850000321865082\n",
      "\n",
      "\n",
      "13 Gooding, E.Y. MD\n",
      "---------- Justin Earl Tolkien - 0.5054000020027161\n",
      "---------- Tolkien, J.R. - 0.4986000061035156\n",
      "---------- J.R. Tolkien - 0.4018999934196472\n",
      "---------- Tolkien JE - 0.3822000026702881\n",
      "\n",
      "\n",
      "14 Jarvis Richard James\n",
      "---------- Jarvis Richard Tolkien - 0.9329000115394592\n",
      "---------- J.R. Tolkien - 0.44620001316070557\n",
      "---------- Trout MRF - 0.41290000081062317\n",
      "---------- Mara Trout - 0.41280001401901245\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_embeddings(embs, test_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac02452",
   "metadata": {},
   "source": [
    "#### Concat last 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ea8bcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J.R. Tolkien\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Tolkien, J.R.\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Justin Earl Tolkien\n",
      "----Decoded name:  Justin Earl Tolkien\n",
      "Jarvis Richard Tolkien\n",
      "----Decoded name:  Jarvis Richard Tolkien\n",
      "Tolkien, Justin\n",
      "----Decoded name:  Justin Tolkien\n",
      "Tolkien JE\n",
      "----Decoded name:  J. E. Tolkien\n",
      "Max Trout\n",
      "----Decoded name:  Max Trout\n",
      "Maximus Trout\n",
      "----Decoded name:  Maximus Trout\n",
      "Maxe Trout\n",
      "----Decoded name:  Maxe Trout\n",
      "Mara Trout\n",
      "----Decoded name:  Mara Trout\n",
      "M.R. Trout\n",
      "----Decoded name:  M. R. Trout\n",
      "Trout MRF\n",
      "----Decoded name:  M. R. F. Trout\n",
      "Gooding, Emily\n",
      "----Decoded name:  Emily Gooding\n",
      "Gooding, E.Y. MD\n",
      "----Decoded name:  E. Y. Gooding\n",
      "Jarvis Richard James\n",
      "----Decoded name:  Richard James Jarvis\n"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "for test_name in test_names:\n",
    "    print(test_name)\n",
    "    start_end = tf.constant(name_tokenizer(['[START][END]'])['input_ids'][0])\n",
    "    start = start_end[0].numpy()\n",
    "    end = start_end[1].numpy()\n",
    "    encoder_input = name_tokenizer([f'[START]{test_name}[END]'])['input_ids'][0]\n",
    "    encoder_input = tf.convert_to_tensor([encoder_input + [0]*(MAX_LEN-len(encoder_input))])\n",
    "    final_output = [start]\n",
    "    for i in range(MAX_LEN):\n",
    "        output = tf.convert_to_tensor([final_output + [0]*(MAX_LEN-len(final_output))])\n",
    "        predictions = transformer((encoder_input, output), training=False)\n",
    "\n",
    "        # Select the last token\n",
    "        predicted_id = tf.argmax(predictions[0][i]).numpy()\n",
    "\n",
    "        # Add to output\n",
    "        final_output.append(predicted_id)\n",
    "\n",
    "        if predicted_id == end:\n",
    "            break\n",
    "    print(\"----Decoded name: \", name_tokenizer.decode(final_output, skip_special_tokens=True, \n",
    "                 clean_up_tokenization_spaces=True).replace(\" ##\", \"\").replace(\" - \", \"-\"))\n",
    "    _ = transformer((encoder_input, tf.convert_to_tensor([final_output[:1] + [0]*(MAX_LEN-len(final_output[:1]))])), \n",
    "            training=False)\n",
    "    \n",
    "    embs.append(np.concatenate((transformer.decoder.dec_layers[-4].context_data[0][0].numpy(),\n",
    "                    transformer.decoder.dec_layers[-3].context_data[0][0].numpy(),\n",
    "                    transformer.decoder.dec_layers[-2].context_data[0][0].numpy(),\n",
    "                    transformer.decoder.dec_layers[-1].context_data[0][0].numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "110fae47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 J.R. Tolkien\n",
      "---------- Tolkien, J.R. - 0.6916999816894531\n",
      "---------- Justin Earl Tolkien - 0.6344000101089478\n",
      "---------- Tolkien JE - 0.6119999885559082\n",
      "---------- M.R. Trout - 0.5781000256538391\n",
      "\n",
      "\n",
      "1 Tolkien, J.R.\n",
      "---------- J.R. Tolkien - 0.6916999816894531\n",
      "---------- Tolkien JE - 0.5270000100135803\n",
      "---------- Gooding, E.Y. MD - 0.49799999594688416\n",
      "---------- Mara Trout - 0.47189998626708984\n",
      "\n",
      "\n",
      "2 Justin Earl Tolkien\n",
      "---------- Tolkien, Justin - 0.7232999801635742\n",
      "---------- J.R. Tolkien - 0.6344000101089478\n",
      "---------- Gooding, E.Y. MD - 0.5044999718666077\n",
      "---------- Maximus Trout - 0.5030999779701233\n",
      "\n",
      "\n",
      "3 Jarvis Richard Tolkien\n",
      "---------- Jarvis Richard James - 0.9330999851226807\n",
      "---------- J.R. Tolkien - 0.5388000011444092\n",
      "---------- Mara Trout - 0.5069000124931335\n",
      "---------- Tolkien, J.R. - 0.4415999948978424\n",
      "\n",
      "\n",
      "4 Tolkien, Justin\n",
      "---------- Justin Earl Tolkien - 0.7232999801635742\n",
      "---------- J.R. Tolkien - 0.3741999864578247\n",
      "---------- Maximus Trout - 0.3043000102043152\n",
      "---------- Jarvis Richard Tolkien - 0.28940001130104065\n",
      "\n",
      "\n",
      "5 Tolkien JE\n",
      "---------- J.R. Tolkien - 0.6119999885559082\n",
      "---------- Tolkien, J.R. - 0.5270000100135803\n",
      "---------- Justin Earl Tolkien - 0.41519999504089355\n",
      "---------- M.R. Trout - 0.40869998931884766\n",
      "\n",
      "\n",
      "6 Max Trout\n",
      "---------- Maxe Trout - 0.9803000092506409\n",
      "---------- Maximus Trout - 0.9758999943733215\n",
      "---------- Mara Trout - 0.7401999831199646\n",
      "---------- M.R. Trout - 0.7143999934196472\n",
      "\n",
      "\n",
      "7 Maximus Trout\n",
      "---------- Max Trout - 0.9758999943733215\n",
      "---------- Maxe Trout - 0.9753000140190125\n",
      "---------- Mara Trout - 0.7706000208854675\n",
      "---------- M.R. Trout - 0.6601999998092651\n",
      "\n",
      "\n",
      "8 Maxe Trout\n",
      "---------- Max Trout - 0.9803000092506409\n",
      "---------- Maximus Trout - 0.9753000140190125\n",
      "---------- Mara Trout - 0.7486000061035156\n",
      "---------- M.R. Trout - 0.6478999853134155\n",
      "\n",
      "\n",
      "9 Mara Trout\n",
      "---------- Maximus Trout - 0.7706000208854675\n",
      "---------- Maxe Trout - 0.7486000061035156\n",
      "---------- Max Trout - 0.7401999831199646\n",
      "---------- M.R. Trout - 0.6111000180244446\n",
      "\n",
      "\n",
      "10 M.R. Trout\n",
      "---------- Max Trout - 0.7143999934196472\n",
      "---------- Maximus Trout - 0.6601999998092651\n",
      "---------- Maxe Trout - 0.6478999853134155\n",
      "---------- Mara Trout - 0.6111000180244446\n",
      "\n",
      "\n",
      "11 Trout MRF\n",
      "---------- Maxe Trout - 0.5692999958992004\n",
      "---------- Mara Trout - 0.5590999722480774\n",
      "---------- Maximus Trout - 0.5167999863624573\n",
      "---------- Max Trout - 0.505299985408783\n",
      "\n",
      "\n",
      "12 Gooding, Emily\n",
      "---------- Tolkien JE - 0.3190000057220459\n",
      "---------- Tolkien, Justin - 0.23119999468326569\n",
      "---------- Gooding, E.Y. MD - 0.2249000072479248\n",
      "---------- Trout MRF - 0.21879999339580536\n",
      "\n",
      "\n",
      "13 Gooding, E.Y. MD\n",
      "---------- Justin Earl Tolkien - 0.5044999718666077\n",
      "---------- Tolkien, J.R. - 0.49799999594688416\n",
      "---------- J.R. Tolkien - 0.4011000096797943\n",
      "---------- Tolkien JE - 0.3822000026702881\n",
      "\n",
      "\n",
      "14 Jarvis Richard James\n",
      "---------- Jarvis Richard Tolkien - 0.9330999851226807\n",
      "---------- J.R. Tolkien - 0.44600000977516174\n",
      "---------- Mara Trout - 0.4122999906539917\n",
      "---------- Trout MRF - 0.4115999937057495\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_embeddings(embs, test_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee2b21b",
   "metadata": {},
   "source": [
    "### Pool full decoder output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9123696b",
   "metadata": {},
   "source": [
    "#### Last layer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e3b7c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J.R. Tolkien\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Tolkien, J.R.\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Justin Earl Tolkien\n",
      "----Decoded name:  Justin Earl Tolkien\n",
      "Jarvis Richard Tolkien\n",
      "----Decoded name:  Jarvis Richard Tolkien\n",
      "Tolkien, Justin\n",
      "----Decoded name:  Justin Tolkien\n",
      "Tolkien JE\n",
      "----Decoded name:  J. E. Tolkien\n",
      "Max Trout\n",
      "----Decoded name:  Max Trout\n",
      "Maximus Trout\n",
      "----Decoded name:  Maximus Trout\n",
      "Maxe Trout\n",
      "----Decoded name:  Maxe Trout\n",
      "Mara Trout\n",
      "----Decoded name:  Mara Trout\n",
      "M.R. Trout\n",
      "----Decoded name:  M. R. Trout\n",
      "Trout MRF\n",
      "----Decoded name:  M. R. F. Trout\n",
      "Gooding, Emily\n",
      "----Decoded name:  Emily Gooding\n",
      "Gooding, E.Y. MD\n",
      "----Decoded name:  E. Y. Gooding\n",
      "Jarvis Richard James\n",
      "----Decoded name:  Richard James Jarvis\n"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "for test_name in test_names:\n",
    "    print(test_name)\n",
    "    start_end = tf.constant(name_tokenizer(['[START][END]'])['input_ids'][0])\n",
    "    start = start_end[0].numpy()\n",
    "    end = start_end[1].numpy()\n",
    "    encoder_input = name_tokenizer([f'[START]{test_name}[END]'])['input_ids'][0]\n",
    "    encoder_input = tf.convert_to_tensor([encoder_input + [0]*(MAX_LEN-len(encoder_input))])\n",
    "    final_output = [start]\n",
    "    for i in range(MAX_LEN):\n",
    "        output = tf.convert_to_tensor([final_output + [0]*(MAX_LEN-len(final_output))])\n",
    "        predictions = transformer((encoder_input, output), training=False)\n",
    "\n",
    "        # Select the last token\n",
    "        predicted_id = tf.argmax(predictions[0][i]).numpy()\n",
    "\n",
    "        # Add to output\n",
    "        final_output.append(predicted_id)\n",
    "\n",
    "        if predicted_id == end:\n",
    "            break\n",
    "    final_output_len = len(final_output) - 1\n",
    "    print(\"----Decoded name: \", name_tokenizer.decode(final_output, skip_special_tokens=True, \n",
    "                 clean_up_tokenization_spaces=True).replace(\" ##\", \"\").replace(\" - \", \"-\"))\n",
    "    _ = transformer((encoder_input, tf.convert_to_tensor([final_output[:-1] + \n",
    "                                                          [0]*(MAX_LEN-len(final_output[:-1]))])), \n",
    "            training=False)\n",
    "    \n",
    "    embs.append(np.mean(transformer.decoder.dec_layers[-1].context_data[0][:final_output_len].numpy(), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cefd867d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 J.R. Tolkien\n",
      "---------- Justin Earl Tolkien - 0.8522999882698059\n",
      "---------- Tolkien, J.R. - 0.7605999708175659\n",
      "---------- Tolkien JE - 0.7462000250816345\n",
      "---------- Jarvis Richard Tolkien - 0.7218000292778015\n",
      "\n",
      "\n",
      "1 Tolkien, J.R.\n",
      "---------- J.R. Tolkien - 0.7605999708175659\n",
      "---------- Tolkien JE - 0.7236999869346619\n",
      "---------- Tolkien, Justin - 0.6431000232696533\n",
      "---------- Gooding, E.Y. MD - 0.5831000208854675\n",
      "\n",
      "\n",
      "2 Justin Earl Tolkien\n",
      "---------- J.R. Tolkien - 0.8522999882698059\n",
      "---------- Jarvis Richard Tolkien - 0.7939000129699707\n",
      "---------- Tolkien, Justin - 0.6883999705314636\n",
      "---------- Tolkien JE - 0.6370999813079834\n",
      "\n",
      "\n",
      "3 Jarvis Richard Tolkien\n",
      "---------- Justin Earl Tolkien - 0.7939000129699707\n",
      "---------- J.R. Tolkien - 0.7218000292778015\n",
      "---------- Tolkien, Justin - 0.6274999976158142\n",
      "---------- Jarvis Richard James - 0.5228000283241272\n",
      "\n",
      "\n",
      "4 Tolkien, Justin\n",
      "---------- Justin Earl Tolkien - 0.6883999705314636\n",
      "---------- J.R. Tolkien - 0.6549999713897705\n",
      "---------- Tolkien, J.R. - 0.6431000232696533\n",
      "---------- Tolkien JE - 0.6312000155448914\n",
      "\n",
      "\n",
      "5 Tolkien JE\n",
      "---------- J.R. Tolkien - 0.7462000250816345\n",
      "---------- Tolkien, J.R. - 0.7236999869346619\n",
      "---------- Justin Earl Tolkien - 0.6370999813079834\n",
      "---------- Tolkien, Justin - 0.6312000155448914\n",
      "\n",
      "\n",
      "6 Max Trout\n",
      "---------- Maxe Trout - 0.9821000099182129\n",
      "---------- Maximus Trout - 0.948199987411499\n",
      "---------- Mara Trout - 0.9125999808311462\n",
      "---------- M.R. Trout - 0.8343999981880188\n",
      "\n",
      "\n",
      "7 Maximus Trout\n",
      "---------- Max Trout - 0.948199987411499\n",
      "---------- Maxe Trout - 0.9200999736785889\n",
      "---------- Mara Trout - 0.89410001039505\n",
      "---------- M.R. Trout - 0.7961000204086304\n",
      "\n",
      "\n",
      "8 Maxe Trout\n",
      "---------- Max Trout - 0.9821000099182129\n",
      "---------- Mara Trout - 0.9265000224113464\n",
      "---------- Maximus Trout - 0.9200999736785889\n",
      "---------- M.R. Trout - 0.828499972820282\n",
      "\n",
      "\n",
      "9 Mara Trout\n",
      "---------- Maxe Trout - 0.9265000224113464\n",
      "---------- Max Trout - 0.9125999808311462\n",
      "---------- Maximus Trout - 0.89410001039505\n",
      "---------- M.R. Trout - 0.8537999987602234\n",
      "\n",
      "\n",
      "10 M.R. Trout\n",
      "---------- Mara Trout - 0.8537999987602234\n",
      "---------- Max Trout - 0.8343999981880188\n",
      "---------- Maxe Trout - 0.828499972820282\n",
      "---------- Maximus Trout - 0.7961000204086304\n",
      "\n",
      "\n",
      "11 Trout MRF\n",
      "---------- M.R. Trout - 0.7465000152587891\n",
      "---------- Max Trout - 0.5008000135421753\n",
      "---------- Mara Trout - 0.49639999866485596\n",
      "---------- Gooding, Emily - 0.47909998893737793\n",
      "\n",
      "\n",
      "12 Gooding, Emily\n",
      "---------- Gooding, E.Y. MD - 0.6674000024795532\n",
      "---------- Tolkien, J.R. - 0.5759999752044678\n",
      "---------- Tolkien JE - 0.5236999988555908\n",
      "---------- Trout MRF - 0.47909998893737793\n",
      "\n",
      "\n",
      "13 Gooding, E.Y. MD\n",
      "---------- Gooding, Emily - 0.6674000024795532\n",
      "---------- Tolkien, J.R. - 0.5831000208854675\n",
      "---------- Trout MRF - 0.46959999203681946\n",
      "---------- Tolkien JE - 0.4390000104904175\n",
      "\n",
      "\n",
      "14 Jarvis Richard James\n",
      "---------- Jarvis Richard Tolkien - 0.5228000283241272\n",
      "---------- Trout MRF - 0.4352000057697296\n",
      "---------- Gooding, Emily - 0.3490999937057495\n",
      "---------- Gooding, E.Y. MD - 0.2671999931335449\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_embeddings(embs, test_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1848b33",
   "metadata": {},
   "source": [
    "#### Concat last 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4f530bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J.R. Tolkien\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Tolkien, J.R.\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Justin Earl Tolkien\n",
      "----Decoded name:  Justin Earl Tolkien\n",
      "Jarvis Richard Tolkien\n",
      "----Decoded name:  Jarvis Richard Tolkien\n",
      "Tolkien, Justin\n",
      "----Decoded name:  Justin Tolkien\n",
      "Tolkien JE\n",
      "----Decoded name:  J. E. Tolkien\n",
      "Max Trout\n",
      "----Decoded name:  Max Trout\n",
      "Maximus Trout\n",
      "----Decoded name:  Maximus Trout\n",
      "Maxe Trout\n",
      "----Decoded name:  Maxe Trout\n",
      "Mara Trout\n",
      "----Decoded name:  Mara Trout\n",
      "M.R. Trout\n",
      "----Decoded name:  M. R. Trout\n",
      "Trout MRF\n",
      "----Decoded name:  M. R. F. Trout\n",
      "Gooding, Emily\n",
      "----Decoded name:  Emily Gooding\n",
      "Gooding, E.Y. MD\n",
      "----Decoded name:  E. Y. Gooding\n",
      "Jarvis Richard James\n",
      "----Decoded name:  Richard James Jarvis\n"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "for test_name in test_names:\n",
    "    print(test_name)\n",
    "    start_end = tf.constant(name_tokenizer(['[START][END]'])['input_ids'][0])\n",
    "    start = start_end[0].numpy()\n",
    "    end = start_end[1].numpy()\n",
    "    encoder_input = name_tokenizer([f'[START]{test_name}[END]'])['input_ids'][0]\n",
    "    encoder_input = tf.convert_to_tensor([encoder_input + [0]*(MAX_LEN-len(encoder_input))])\n",
    "    final_output = [start]\n",
    "    for i in range(MAX_LEN):\n",
    "        output = tf.convert_to_tensor([final_output + [0]*(MAX_LEN-len(final_output))])\n",
    "        predictions = transformer((encoder_input, output), training=False)\n",
    "\n",
    "        # Select the last token\n",
    "        predicted_id = tf.argmax(predictions[0][i]).numpy()\n",
    "\n",
    "        # Add to output\n",
    "        final_output.append(predicted_id)\n",
    "\n",
    "        if predicted_id == end:\n",
    "            break\n",
    "    final_output_len = len(final_output) - 1\n",
    "    print(\"----Decoded name: \", name_tokenizer.decode(final_output, skip_special_tokens=True, \n",
    "                 clean_up_tokenization_spaces=True).replace(\" ##\", \"\").replace(\" - \", \"-\"))\n",
    "    _ = transformer((encoder_input, tf.convert_to_tensor([final_output[:-1] + \n",
    "                                                          [0]*(MAX_LEN-len(final_output[:-1]))])), \n",
    "            training=False)\n",
    "    \n",
    "    embs.append(np.mean(transformer.decoder.dec_layers[-4].context_data[0][:final_output_len].numpy(), axis=0))\n",
    "    embs.append(np.mean(transformer.decoder.dec_layers[-3].context_data[0][:final_output_len].numpy(), axis=0))\n",
    "    embs.append(np.mean(transformer.decoder.dec_layers[-2].context_data[0][:final_output_len].numpy(), axis=0))\n",
    "    embs.append(np.mean(transformer.decoder.dec_layers[-1].context_data[0][:final_output_len].numpy(), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7fb66bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 J.R. Tolkien\n",
      "---------- Maxe Trout - 0.819599986076355\n",
      "---------- Tolkien, Justin - 0.8172000050544739\n",
      "---------- Gooding, Emily - 0.8073999881744385\n",
      "---------- Tolkien, J.R. - 0.7820000052452087\n",
      "\n",
      "\n",
      "1 Tolkien, J.R.\n",
      "---------- Mara Trout - 0.8208000063896179\n",
      "---------- Gooding, E.Y. MD - 0.8140000104904175\n",
      "---------- Tolkien JE - 0.78329998254776\n",
      "---------- J.R. Tolkien - 0.7820000052452087\n",
      "\n",
      "\n",
      "2 Justin Earl Tolkien\n",
      "---------- M.R. Trout - 0.8011000156402588\n",
      "---------- Jarvis Richard James - 0.7623999714851379\n",
      "---------- Max Trout - 0.7620999813079834\n",
      "---------- Tolkien, J.R. - 0.6766999959945679\n",
      "\n",
      "\n",
      "3 Jarvis Richard Tolkien\n",
      "---------- Trout MRF - 0.8522999882698059\n",
      "---------- Maximus Trout - 0.7605999708175659\n",
      "---------- Justin Earl Tolkien - 0.446399986743927\n",
      "---------- M.R. Trout - 0.27639999985694885\n",
      "\n",
      "\n",
      "4 Tolkien, Justin\n",
      "---------- J.R. Tolkien - 0.8172000050544739\n",
      "---------- Maxe Trout - 0.8046000003814697\n",
      "---------- Gooding, Emily - 0.7900999784469604\n",
      "---------- Tolkien JE - 0.762499988079071\n",
      "\n",
      "\n",
      "5 Tolkien JE\n",
      "---------- Tolkien, J.R. - 0.78329998254776\n",
      "---------- Tolkien, Justin - 0.762499988079071\n",
      "---------- Mara Trout - 0.7149999737739563\n",
      "---------- Gooding, E.Y. MD - 0.7013000249862671\n",
      "\n",
      "\n",
      "6 Max Trout\n",
      "---------- Justin Earl Tolkien - 0.7620999813079834\n",
      "---------- Tolkien JE - 0.6776000261306763\n",
      "---------- M.R. Trout - 0.5978000164031982\n",
      "---------- Jarvis Richard James - 0.5795000195503235\n",
      "\n",
      "\n",
      "7 Maximus Trout\n",
      "---------- Jarvis Richard Tolkien - 0.7605999708175659\n",
      "---------- Justin Earl Tolkien - 0.5649999976158142\n",
      "---------- Trout MRF - 0.5020999908447266\n",
      "---------- Max Trout - 0.47099998593330383\n",
      "\n",
      "\n",
      "8 Maxe Trout\n",
      "---------- Gooding, Emily - 0.9463000297546387\n",
      "---------- J.R. Tolkien - 0.819599986076355\n",
      "---------- Mara Trout - 0.805899977684021\n",
      "---------- Tolkien, Justin - 0.8046000003814697\n",
      "\n",
      "\n",
      "9 Mara Trout\n",
      "---------- Gooding, E.Y. MD - 0.9656999707221985\n",
      "---------- Tolkien, J.R. - 0.8208000063896179\n",
      "---------- Maxe Trout - 0.805899977684021\n",
      "---------- Gooding, Emily - 0.7753999829292297\n",
      "\n",
      "\n",
      "10 M.R. Trout\n",
      "---------- Jarvis Richard James - 0.8575000166893005\n",
      "---------- Justin Earl Tolkien - 0.8011000156402588\n",
      "---------- Mara Trout - 0.6603000164031982\n",
      "---------- Gooding, E.Y. MD - 0.635699987411499\n",
      "\n",
      "\n",
      "11 Trout MRF\n",
      "---------- Jarvis Richard Tolkien - 0.8522999882698059\n",
      "---------- Maximus Trout - 0.5020999908447266\n",
      "---------- M.R. Trout - 0.3483000099658966\n",
      "---------- Justin Earl Tolkien - 0.2630999982357025\n",
      "\n",
      "\n",
      "12 Gooding, Emily\n",
      "---------- Maxe Trout - 0.9463000297546387\n",
      "---------- J.R. Tolkien - 0.8073999881744385\n",
      "---------- Gooding, E.Y. MD - 0.8069000244140625\n",
      "---------- Tolkien, Justin - 0.7900999784469604\n",
      "\n",
      "\n",
      "13 Gooding, E.Y. MD\n",
      "---------- Mara Trout - 0.9656999707221985\n",
      "---------- Tolkien, J.R. - 0.8140000104904175\n",
      "---------- Gooding, Emily - 0.8069000244140625\n",
      "---------- Maxe Trout - 0.7763000130653381\n",
      "\n",
      "\n",
      "14 Jarvis Richard James\n",
      "---------- M.R. Trout - 0.8575000166893005\n",
      "---------- Justin Earl Tolkien - 0.7623999714851379\n",
      "---------- Gooding, E.Y. MD - 0.6565999984741211\n",
      "---------- Mara Trout - 0.6324999928474426\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_embeddings(embs, test_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c773b815",
   "metadata": {},
   "source": [
    "### Pool full encoder output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d5413",
   "metadata": {},
   "source": [
    "#### Last layer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2cb29ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J.R. Tolkien\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Tolkien, J.R.\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Justin Earl Tolkien\n",
      "----Decoded name:  Justin Earl Tolkien\n",
      "Jarvis Richard Tolkien\n",
      "----Decoded name:  Jarvis Richard Tolkien\n",
      "Tolkien, Justin\n",
      "----Decoded name:  Justin Tolkien\n",
      "Tolkien JE\n",
      "----Decoded name:  J. E. Tolkien\n",
      "Max Trout\n",
      "----Decoded name:  Max Trout\n",
      "Maximus Trout\n",
      "----Decoded name:  Maximus Trout\n",
      "Maxe Trout\n",
      "----Decoded name:  Maxe Trout\n",
      "Mara Trout\n",
      "----Decoded name:  Mara Trout\n",
      "M.R. Trout\n",
      "----Decoded name:  M. R. Trout\n",
      "Trout MRF\n",
      "----Decoded name:  M. R. F. Trout\n",
      "Gooding, Emily\n",
      "----Decoded name:  Emily Gooding\n",
      "Gooding, E.Y. MD\n",
      "----Decoded name:  E. Y. Gooding\n",
      "Jarvis Richard James\n",
      "----Decoded name:  Richard James Jarvis\n"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "for test_name in test_names:\n",
    "    print(test_name)\n",
    "    start_end = tf.constant(name_tokenizer(['[START][END]'])['input_ids'][0])\n",
    "    start = start_end[0].numpy()\n",
    "    end = start_end[1].numpy()\n",
    "    encoder_input = name_tokenizer([f'[START]{test_name}[END]'])['input_ids'][0]\n",
    "    final_input_len = len(encoder_input)\n",
    "    encoder_input = tf.convert_to_tensor([encoder_input + [0]*(MAX_LEN-len(encoder_input))])\n",
    "    final_output = [start]\n",
    "    for i in range(MAX_LEN):\n",
    "        output = tf.convert_to_tensor([final_output + [0]*(MAX_LEN-len(final_output))])\n",
    "        predictions = transformer((encoder_input, output), training=False)\n",
    "\n",
    "        # Select the last token\n",
    "        predicted_id = tf.argmax(predictions[0][i]).numpy()\n",
    "\n",
    "        # Add to output\n",
    "        final_output.append(predicted_id)\n",
    "\n",
    "        if predicted_id == end:\n",
    "            break\n",
    "    print(\"----Decoded name: \", name_tokenizer.decode(final_output, skip_special_tokens=True, \n",
    "                 clean_up_tokenization_spaces=True).replace(\" ##\", \"\").replace(\" - \", \"-\"))\n",
    "    _ = transformer((encoder_input, tf.convert_to_tensor([final_output[:-1] + \n",
    "                                                          [0]*(MAX_LEN-len(final_output[:-1]))])), \n",
    "            training=False)\n",
    "    \n",
    "    embs.append(np.mean(transformer.encoder.enc_layers[-1].context_data[0][:final_input_len].numpy(), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3646ffca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 J.R. Tolkien\n",
      "---------- Tolkien, J.R. - 0.7846999764442444\n",
      "---------- M.R. Trout - 0.7739999890327454\n",
      "---------- Justin Earl Tolkien - 0.7416999936103821\n",
      "---------- Jarvis Richard Tolkien - 0.728600025177002\n",
      "\n",
      "\n",
      "1 Tolkien, J.R.\n",
      "---------- Gooding, E.Y. MD - 0.8529000282287598\n",
      "---------- Tolkien, Justin - 0.8291000127792358\n",
      "---------- Gooding, Emily - 0.817300021648407\n",
      "---------- Tolkien JE - 0.7897999882698059\n",
      "\n",
      "\n",
      "2 Justin Earl Tolkien\n",
      "---------- Jarvis Richard Tolkien - 0.9140999913215637\n",
      "---------- Tolkien, Justin - 0.8712999820709229\n",
      "---------- Tolkien JE - 0.795199990272522\n",
      "---------- Maximus Trout - 0.7799000144004822\n",
      "\n",
      "\n",
      "3 Jarvis Richard Tolkien\n",
      "---------- Justin Earl Tolkien - 0.9140999913215637\n",
      "---------- Jarvis Richard James - 0.8639000058174133\n",
      "---------- Tolkien, Justin - 0.8409000039100647\n",
      "---------- Gooding, Emily - 0.7947999835014343\n",
      "\n",
      "\n",
      "4 Tolkien, Justin\n",
      "---------- Gooding, Emily - 0.8968999981880188\n",
      "---------- Justin Earl Tolkien - 0.8712999820709229\n",
      "---------- Jarvis Richard Tolkien - 0.8409000039100647\n",
      "---------- Tolkien JE - 0.8327000141143799\n",
      "\n",
      "\n",
      "5 Tolkien JE\n",
      "---------- Tolkien, Justin - 0.8327000141143799\n",
      "---------- Justin Earl Tolkien - 0.795199990272522\n",
      "---------- Tolkien, J.R. - 0.7897999882698059\n",
      "---------- Gooding, Emily - 0.7893000245094299\n",
      "\n",
      "\n",
      "6 Max Trout\n",
      "---------- Maxe Trout - 0.9660999774932861\n",
      "---------- Maximus Trout - 0.8942999839782715\n",
      "---------- Mara Trout - 0.868399977684021\n",
      "---------- Trout MRF - 0.7038000226020813\n",
      "\n",
      "\n",
      "7 Maximus Trout\n",
      "---------- Maxe Trout - 0.9193000197410583\n",
      "---------- Max Trout - 0.8942999839782715\n",
      "---------- Mara Trout - 0.84170001745224\n",
      "---------- Tolkien, Justin - 0.8109999895095825\n",
      "\n",
      "\n",
      "8 Maxe Trout\n",
      "---------- Max Trout - 0.9660999774932861\n",
      "---------- Maximus Trout - 0.9193000197410583\n",
      "---------- Mara Trout - 0.8795999884605408\n",
      "---------- Tolkien, Justin - 0.7451000213623047\n",
      "\n",
      "\n",
      "9 Mara Trout\n",
      "---------- Maxe Trout - 0.8795999884605408\n",
      "---------- Max Trout - 0.868399977684021\n",
      "---------- Maximus Trout - 0.84170001745224\n",
      "---------- Trout MRF - 0.7171000242233276\n",
      "\n",
      "\n",
      "10 M.R. Trout\n",
      "---------- J.R. Tolkien - 0.7739999890327454\n",
      "---------- Trout MRF - 0.7261999845504761\n",
      "---------- Mara Trout - 0.6858999729156494\n",
      "---------- Tolkien, J.R. - 0.670199990272522\n",
      "\n",
      "\n",
      "11 Trout MRF\n",
      "---------- M.R. Trout - 0.7261999845504761\n",
      "---------- Tolkien, J.R. - 0.725600004196167\n",
      "---------- Tolkien JE - 0.7184000015258789\n",
      "---------- Mara Trout - 0.7171000242233276\n",
      "\n",
      "\n",
      "12 Gooding, Emily\n",
      "---------- Tolkien, Justin - 0.8968999981880188\n",
      "---------- Tolkien, J.R. - 0.817300021648407\n",
      "---------- Jarvis Richard Tolkien - 0.7947999835014343\n",
      "---------- Tolkien JE - 0.7893000245094299\n",
      "\n",
      "\n",
      "13 Gooding, E.Y. MD\n",
      "---------- Tolkien, J.R. - 0.8529000282287598\n",
      "---------- Gooding, Emily - 0.6960999965667725\n",
      "---------- Tolkien JE - 0.6650000214576721\n",
      "---------- Trout MRF - 0.6463000178337097\n",
      "\n",
      "\n",
      "14 Jarvis Richard James\n",
      "---------- Jarvis Richard Tolkien - 0.8639000058174133\n",
      "---------- Gooding, Emily - 0.7013000249862671\n",
      "---------- Justin Earl Tolkien - 0.6969000101089478\n",
      "---------- Tolkien, Justin - 0.6585000157356262\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_embeddings(embs, test_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaab349",
   "metadata": {},
   "source": [
    "#### Concat last 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2b08367d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J.R. Tolkien\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Tolkien, J.R.\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Justin Earl Tolkien\n",
      "----Decoded name:  Justin Earl Tolkien\n",
      "Jarvis Richard Tolkien\n",
      "----Decoded name:  Jarvis Richard Tolkien\n",
      "Tolkien, Justin\n",
      "----Decoded name:  Justin Tolkien\n",
      "Tolkien JE\n",
      "----Decoded name:  J. E. Tolkien\n",
      "Max Trout\n",
      "----Decoded name:  Max Trout\n",
      "Maximus Trout\n",
      "----Decoded name:  Maximus Trout\n",
      "Maxe Trout\n",
      "----Decoded name:  Maxe Trout\n",
      "Mara Trout\n",
      "----Decoded name:  Mara Trout\n",
      "M.R. Trout\n",
      "----Decoded name:  M. R. Trout\n",
      "Trout MRF\n",
      "----Decoded name:  M. R. F. Trout\n",
      "Gooding, Emily\n",
      "----Decoded name:  Emily Gooding\n",
      "Gooding, E.Y. MD\n",
      "----Decoded name:  E. Y. Gooding\n",
      "Jarvis Richard James\n",
      "----Decoded name:  Richard James Jarvis\n"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "for test_name in test_names:\n",
    "    print(test_name)\n",
    "    start_end = tf.constant(name_tokenizer(['[START][END]'])['input_ids'][0])\n",
    "    start = start_end[0].numpy()\n",
    "    end = start_end[1].numpy()\n",
    "    encoder_input = name_tokenizer([f'[START]{test_name}[END]'])['input_ids'][0]\n",
    "    final_input_len = len(encoder_input)\n",
    "    encoder_input = tf.convert_to_tensor([encoder_input + [0]*(MAX_LEN-len(encoder_input))])\n",
    "    final_output = [start]\n",
    "    for i in range(MAX_LEN):\n",
    "        output = tf.convert_to_tensor([final_output + [0]*(MAX_LEN-len(final_output))])\n",
    "        predictions = transformer((encoder_input, output), training=False)\n",
    "\n",
    "        # Select the last token\n",
    "        predicted_id = tf.argmax(predictions[0][i]).numpy()\n",
    "\n",
    "        # Add to output\n",
    "        final_output.append(predicted_id)\n",
    "\n",
    "        if predicted_id == end:\n",
    "            break\n",
    "    \n",
    "    print(\"----Decoded name: \", name_tokenizer.decode(final_output, skip_special_tokens=True, \n",
    "                 clean_up_tokenization_spaces=True).replace(\" ##\", \"\").replace(\" - \", \"-\"))\n",
    "    _ = transformer((encoder_input, tf.convert_to_tensor([final_output[:-1] + \n",
    "                                                          [0]*(MAX_LEN-len(final_output[:-1]))])), \n",
    "            training=False)\n",
    "    \n",
    "    embs.append(np.mean(transformer.encoder.enc_layers[-4].context_data[0][1:final_input_len+1].numpy(), axis=0))\n",
    "    embs.append(np.mean(transformer.encoder.enc_layers[-3].context_data[0][1:final_input_len+1].numpy(), axis=0))\n",
    "    embs.append(np.mean(transformer.encoder.enc_layers[-2].context_data[0][1:final_input_len+1].numpy(), axis=0))\n",
    "    embs.append(np.mean(transformer.encoder.enc_layers[-1].context_data[0][1:final_input_len+1].numpy(), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3bd8c7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 J.R. Tolkien\n",
      "---------- Tolkien, Justin - 0.9211000204086304\n",
      "---------- Tolkien, J.R. - 0.8880000114440918\n",
      "---------- Tolkien JE - 0.8255000114440918\n",
      "---------- Maxe Trout - 0.8184999823570251\n",
      "\n",
      "\n",
      "1 Tolkien, J.R.\n",
      "---------- J.R. Tolkien - 0.8880000114440918\n",
      "---------- Justin Earl Tolkien - 0.8725000023841858\n",
      "---------- Tolkien JE - 0.8712000250816345\n",
      "---------- Tolkien, Justin - 0.8111000061035156\n",
      "\n",
      "\n",
      "2 Justin Earl Tolkien\n",
      "---------- Tolkien, J.R. - 0.8725000023841858\n",
      "---------- Max Trout - 0.8222000002861023\n",
      "---------- Jarvis Richard Tolkien - 0.7678999900817871\n",
      "---------- J.R. Tolkien - 0.7429999709129333\n",
      "\n",
      "\n",
      "3 Jarvis Richard Tolkien\n",
      "---------- Maximus Trout - 0.7853999733924866\n",
      "---------- Justin Earl Tolkien - 0.7678999900817871\n",
      "---------- Trout MRF - 0.7423999905586243\n",
      "---------- Tolkien, J.R. - 0.66839998960495\n",
      "\n",
      "\n",
      "4 Tolkien, Justin\n",
      "---------- J.R. Tolkien - 0.9211000204086304\n",
      "---------- Tolkien JE - 0.896399974822998\n",
      "---------- Tolkien, J.R. - 0.8111000061035156\n",
      "---------- Gooding, Emily - 0.8101999759674072\n",
      "\n",
      "\n",
      "5 Tolkien JE\n",
      "---------- Tolkien, Justin - 0.896399974822998\n",
      "---------- Tolkien, J.R. - 0.8712000250816345\n",
      "---------- Max Trout - 0.8454999923706055\n",
      "---------- J.R. Tolkien - 0.8255000114440918\n",
      "\n",
      "\n",
      "6 Max Trout\n",
      "---------- Tolkien JE - 0.8454999923706055\n",
      "---------- Justin Earl Tolkien - 0.8222000002861023\n",
      "---------- Tolkien, J.R. - 0.8004999756813049\n",
      "---------- Tolkien, Justin - 0.7633000016212463\n",
      "\n",
      "\n",
      "7 Maximus Trout\n",
      "---------- Jarvis Richard Tolkien - 0.7853999733924866\n",
      "---------- Trout MRF - 0.734499990940094\n",
      "---------- Max Trout - 0.7195000052452087\n",
      "---------- Tolkien JE - 0.6355999708175659\n",
      "\n",
      "\n",
      "8 Maxe Trout\n",
      "---------- Gooding, Emily - 0.9187999963760376\n",
      "---------- Mara Trout - 0.9031999707221985\n",
      "---------- J.R. Tolkien - 0.8184999823570251\n",
      "---------- Gooding, E.Y. MD - 0.8109999895095825\n",
      "\n",
      "\n",
      "9 Mara Trout\n",
      "---------- Maxe Trout - 0.9031999707221985\n",
      "---------- Gooding, E.Y. MD - 0.9000999927520752\n",
      "---------- M.R. Trout - 0.8932999968528748\n",
      "---------- Gooding, Emily - 0.8450000286102295\n",
      "\n",
      "\n",
      "10 M.R. Trout\n",
      "---------- Jarvis Richard James - 0.8988000154495239\n",
      "---------- Mara Trout - 0.8932999968528748\n",
      "---------- Gooding, E.Y. MD - 0.7919999957084656\n",
      "---------- Maxe Trout - 0.7850000262260437\n",
      "\n",
      "\n",
      "11 Trout MRF\n",
      "---------- M.R. Trout - 0.7598999738693237\n",
      "---------- Jarvis Richard Tolkien - 0.7423999905586243\n",
      "---------- Maximus Trout - 0.734499990940094\n",
      "---------- Mara Trout - 0.7167999744415283\n",
      "\n",
      "\n",
      "12 Gooding, Emily\n",
      "---------- Maxe Trout - 0.9187999963760376\n",
      "---------- Gooding, E.Y. MD - 0.9121000170707703\n",
      "---------- Mara Trout - 0.8450000286102295\n",
      "---------- Tolkien, Justin - 0.8101999759674072\n",
      "\n",
      "\n",
      "13 Gooding, E.Y. MD\n",
      "---------- Gooding, Emily - 0.9121000170707703\n",
      "---------- Mara Trout - 0.9000999927520752\n",
      "---------- Jarvis Richard James - 0.8776000142097473\n",
      "---------- Maxe Trout - 0.8109999895095825\n",
      "\n",
      "\n",
      "14 Jarvis Richard James\n",
      "---------- M.R. Trout - 0.8988000154495239\n",
      "---------- Gooding, E.Y. MD - 0.8776000142097473\n",
      "---------- Mara Trout - 0.7925999760627747\n",
      "---------- Gooding, Emily - 0.7713000178337097\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_embeddings(embs, test_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443ce2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0238b34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
