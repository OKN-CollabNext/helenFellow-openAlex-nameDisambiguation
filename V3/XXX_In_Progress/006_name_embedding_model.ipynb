{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f071eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09cc4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ed5363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.models import WordPiece\n",
    "# from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a93c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "so = open(\"data.log\", 'w', 10)\n",
    "sys.stdout.echo = so\n",
    "sys.stderr.echo = so\n",
    "\n",
    "get_ipython().log.handlers[0].stream = so\n",
    "get_ipython().log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae7251cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tensorflow_datasets\n",
    "# !pip install -q -U tensorflow-text tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edcc5c4",
   "metadata": {},
   "source": [
    "### Load and save data as TF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4566f77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘./training_data_text_files/train/*’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "for i in ['train','val','test']:\n",
    "    try:\n",
    "        os.system(f\"rm -r ./embedding_model_data/processed_pair_data/{i}/*\")\n",
    "        print(\"Done\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c6c6cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_file_to_directory(pos_file_name, neg_file_name, type_of_data, file_i, hard_data_filename=\"\"):\n",
    "    temp_pos_df = pd.read_parquet(pos_file_name, columns=['name_1','name_2'])\n",
    "    temp_pos_df['label'] = 1\n",
    "    temp_neg_df = pd.read_parquet(neg_file_name, columns=['name_1','name_2'])\n",
    "    temp_neg_df['label'] = 0\n",
    "    if hard_data_filename:\n",
    "        temp_hard_df = pd.read_parquet(hard_data_filename)\n",
    "        temp_hard_df['name_1'] = temp_hard_df['hard_pairs'].apply(lambda x: x[0])\n",
    "        temp_hard_df['name_2'] = temp_hard_df['hard_pairs'].apply(lambda x: x[1])\n",
    "        temp_hard_df['label'] = 0\n",
    "        \n",
    "        pos_count = temp_pos_df.shape[0]\n",
    "        hard_count = temp_hard_df.shape[0]\n",
    "        temp_neg_df = pd.concat([temp_neg_df.sample(pos_count - hard_count), \n",
    "                                 temp_hard_df[['name_1','name_2','label']].copy()], \n",
    "                                axis=0)\n",
    "    \n",
    "    temp_df = pd.concat([temp_pos_df, temp_neg_df], axis=0)\n",
    "    \n",
    "    temp_df = temp_df[(~temp_df['name_1'].isnull()) & \n",
    "                      (~temp_df['name_2'].isnull())].copy()\n",
    "    \n",
    "    \n",
    "    temp_df['name_1'] = temp_df['name_1'].apply(lambda x: \" \".join(x.split()))\n",
    "    temp_df['name_2'] = temp_df['name_2'].apply(lambda x: \" \".join(x.split()))\n",
    "    temp_df.sample(temp_df.shape[0]) \\\n",
    "        .to_parquet(f\"./embedding_model_data/processed_pair_data/{type_of_data}/{file_i}.parquet\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eeca61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data_files():\n",
    "    final_df = pd.DataFrame()\n",
    "    for type_of_data in ['train','val','test']:\n",
    "        list_of_pos_files = glob.glob(f\"./embedding_model_data/raw_paired_data/{type_of_data}/pos/*\")\n",
    "        list_of_pos_files.sort()\n",
    "        list_of_neg_files = glob.glob(f\"./embedding_model_data/raw_paired_data/{type_of_data}/neg/*\")\n",
    "        list_of_neg_files.sort()\n",
    "        for ith, (pos_file_name, neg_file_name) in enumerate(zip(list_of_pos_files, \n",
    "                                                                 list_of_neg_files[:len(list_of_pos_files)])):\n",
    "            if ith == 0:\n",
    "                hard_data_filename=glob.glob(f\"./embedding_model_data/raw_paired_data/{type_of_data}/neg/hard*\")[0]\n",
    "            else:\n",
    "                hard_data_filename = \"\"\n",
    "            print(f\"{type_of_data} - {ith} - {pos_file_name} - {neg_file_name}\")\n",
    "            _ = write_data_file_to_directory(pos_file_name, neg_file_name, type_of_data, ith, hard_data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0863728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train - 0 - ./embedding_model_data/raw_paired_data/train/pos/0.parquet - ./embedding_model_data/raw_paired_data/train/neg/0.parquet\n",
      "train - 1 - ./embedding_model_data/raw_paired_data/train/pos/1.parquet - ./embedding_model_data/raw_paired_data/train/neg/1.parquet\n",
      "train - 2 - ./embedding_model_data/raw_paired_data/train/pos/10.parquet - ./embedding_model_data/raw_paired_data/train/neg/10.parquet\n",
      "train - 3 - ./embedding_model_data/raw_paired_data/train/pos/11.parquet - ./embedding_model_data/raw_paired_data/train/neg/11.parquet\n",
      "train - 4 - ./embedding_model_data/raw_paired_data/train/pos/12.parquet - ./embedding_model_data/raw_paired_data/train/neg/12.parquet\n",
      "train - 5 - ./embedding_model_data/raw_paired_data/train/pos/13.parquet - ./embedding_model_data/raw_paired_data/train/neg/13.parquet\n",
      "train - 6 - ./embedding_model_data/raw_paired_data/train/pos/14.parquet - ./embedding_model_data/raw_paired_data/train/neg/14.parquet\n",
      "train - 7 - ./embedding_model_data/raw_paired_data/train/pos/15.parquet - ./embedding_model_data/raw_paired_data/train/neg/15.parquet\n",
      "train - 8 - ./embedding_model_data/raw_paired_data/train/pos/16.parquet - ./embedding_model_data/raw_paired_data/train/neg/16.parquet\n",
      "train - 9 - ./embedding_model_data/raw_paired_data/train/pos/17.parquet - ./embedding_model_data/raw_paired_data/train/neg/17.parquet\n",
      "train - 10 - ./embedding_model_data/raw_paired_data/train/pos/18.parquet - ./embedding_model_data/raw_paired_data/train/neg/18.parquet\n",
      "train - 11 - ./embedding_model_data/raw_paired_data/train/pos/19.parquet - ./embedding_model_data/raw_paired_data/train/neg/19.parquet\n",
      "train - 12 - ./embedding_model_data/raw_paired_data/train/pos/2.parquet - ./embedding_model_data/raw_paired_data/train/neg/2.parquet\n",
      "train - 13 - ./embedding_model_data/raw_paired_data/train/pos/20.parquet - ./embedding_model_data/raw_paired_data/train/neg/20.parquet\n",
      "train - 14 - ./embedding_model_data/raw_paired_data/train/pos/21.parquet - ./embedding_model_data/raw_paired_data/train/neg/21.parquet\n",
      "train - 15 - ./embedding_model_data/raw_paired_data/train/pos/22.parquet - ./embedding_model_data/raw_paired_data/train/neg/22.parquet\n",
      "train - 16 - ./embedding_model_data/raw_paired_data/train/pos/23.parquet - ./embedding_model_data/raw_paired_data/train/neg/23.parquet\n",
      "train - 17 - ./embedding_model_data/raw_paired_data/train/pos/24.parquet - ./embedding_model_data/raw_paired_data/train/neg/24.parquet\n",
      "train - 18 - ./embedding_model_data/raw_paired_data/train/pos/25.parquet - ./embedding_model_data/raw_paired_data/train/neg/25.parquet\n",
      "train - 19 - ./embedding_model_data/raw_paired_data/train/pos/26.parquet - ./embedding_model_data/raw_paired_data/train/neg/26.parquet\n",
      "train - 20 - ./embedding_model_data/raw_paired_data/train/pos/27.parquet - ./embedding_model_data/raw_paired_data/train/neg/27.parquet\n",
      "train - 21 - ./embedding_model_data/raw_paired_data/train/pos/28.parquet - ./embedding_model_data/raw_paired_data/train/neg/28.parquet\n",
      "train - 22 - ./embedding_model_data/raw_paired_data/train/pos/29.parquet - ./embedding_model_data/raw_paired_data/train/neg/29.parquet\n",
      "train - 23 - ./embedding_model_data/raw_paired_data/train/pos/3.parquet - ./embedding_model_data/raw_paired_data/train/neg/3.parquet\n",
      "train - 24 - ./embedding_model_data/raw_paired_data/train/pos/30.parquet - ./embedding_model_data/raw_paired_data/train/neg/30.parquet\n",
      "train - 25 - ./embedding_model_data/raw_paired_data/train/pos/31.parquet - ./embedding_model_data/raw_paired_data/train/neg/31.parquet\n",
      "train - 26 - ./embedding_model_data/raw_paired_data/train/pos/32.parquet - ./embedding_model_data/raw_paired_data/train/neg/32.parquet\n",
      "train - 27 - ./embedding_model_data/raw_paired_data/train/pos/33.parquet - ./embedding_model_data/raw_paired_data/train/neg/33.parquet\n",
      "train - 28 - ./embedding_model_data/raw_paired_data/train/pos/34.parquet - ./embedding_model_data/raw_paired_data/train/neg/34.parquet\n",
      "train - 29 - ./embedding_model_data/raw_paired_data/train/pos/35.parquet - ./embedding_model_data/raw_paired_data/train/neg/35.parquet\n",
      "train - 30 - ./embedding_model_data/raw_paired_data/train/pos/36.parquet - ./embedding_model_data/raw_paired_data/train/neg/36.parquet\n",
      "train - 31 - ./embedding_model_data/raw_paired_data/train/pos/37.parquet - ./embedding_model_data/raw_paired_data/train/neg/37.parquet\n",
      "train - 32 - ./embedding_model_data/raw_paired_data/train/pos/38.parquet - ./embedding_model_data/raw_paired_data/train/neg/38.parquet\n",
      "train - 33 - ./embedding_model_data/raw_paired_data/train/pos/39.parquet - ./embedding_model_data/raw_paired_data/train/neg/39.parquet\n",
      "train - 34 - ./embedding_model_data/raw_paired_data/train/pos/4.parquet - ./embedding_model_data/raw_paired_data/train/neg/4.parquet\n",
      "train - 35 - ./embedding_model_data/raw_paired_data/train/pos/40.parquet - ./embedding_model_data/raw_paired_data/train/neg/40.parquet\n",
      "train - 36 - ./embedding_model_data/raw_paired_data/train/pos/41.parquet - ./embedding_model_data/raw_paired_data/train/neg/41.parquet\n",
      "train - 37 - ./embedding_model_data/raw_paired_data/train/pos/42.parquet - ./embedding_model_data/raw_paired_data/train/neg/42.parquet\n",
      "train - 38 - ./embedding_model_data/raw_paired_data/train/pos/43.parquet - ./embedding_model_data/raw_paired_data/train/neg/43.parquet\n",
      "train - 39 - ./embedding_model_data/raw_paired_data/train/pos/44.parquet - ./embedding_model_data/raw_paired_data/train/neg/44.parquet\n",
      "train - 40 - ./embedding_model_data/raw_paired_data/train/pos/45.parquet - ./embedding_model_data/raw_paired_data/train/neg/45.parquet\n",
      "train - 41 - ./embedding_model_data/raw_paired_data/train/pos/46.parquet - ./embedding_model_data/raw_paired_data/train/neg/46.parquet\n",
      "train - 42 - ./embedding_model_data/raw_paired_data/train/pos/47.parquet - ./embedding_model_data/raw_paired_data/train/neg/47.parquet\n",
      "train - 43 - ./embedding_model_data/raw_paired_data/train/pos/48.parquet - ./embedding_model_data/raw_paired_data/train/neg/48.parquet\n",
      "train - 44 - ./embedding_model_data/raw_paired_data/train/pos/49.parquet - ./embedding_model_data/raw_paired_data/train/neg/49.parquet\n",
      "train - 45 - ./embedding_model_data/raw_paired_data/train/pos/5.parquet - ./embedding_model_data/raw_paired_data/train/neg/5.parquet\n",
      "train - 46 - ./embedding_model_data/raw_paired_data/train/pos/6.parquet - ./embedding_model_data/raw_paired_data/train/neg/6.parquet\n",
      "train - 47 - ./embedding_model_data/raw_paired_data/train/pos/7.parquet - ./embedding_model_data/raw_paired_data/train/neg/7.parquet\n",
      "train - 48 - ./embedding_model_data/raw_paired_data/train/pos/8.parquet - ./embedding_model_data/raw_paired_data/train/neg/8.parquet\n",
      "train - 49 - ./embedding_model_data/raw_paired_data/train/pos/9.parquet - ./embedding_model_data/raw_paired_data/train/neg/9.parquet\n",
      "val - 0 - ./embedding_model_data/raw_paired_data/val/pos/0.parquet - ./embedding_model_data/raw_paired_data/val/neg/0.parquet\n",
      "val - 1 - ./embedding_model_data/raw_paired_data/val/pos/1.parquet - ./embedding_model_data/raw_paired_data/val/neg/1.parquet\n",
      "test - 0 - ./embedding_model_data/raw_paired_data/test/pos/0.parquet - ./embedding_model_data/raw_paired_data/test/neg/0.parquet\n"
     ]
    }
   ],
   "source": [
    "_ = get_all_data_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cba2aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file(file_name, new_num):\n",
    "    temp_df = pd.read_parquet(file_name)\n",
    "    temp_df.iloc[:int(temp_df.shape[0]/2), :] \\\n",
    "        .to_parquet(f\"./embedding_model_data/new_proc_pair_data/train/{new_num}a.parquet\")\n",
    "    \n",
    "    temp_df.iloc[int(temp_df.shape[0]/2):, :] \\\n",
    "        .to_parquet(f\"./embedding_model_data/new_proc_pair_data/train/{new_num}b.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fa6014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ith, file_name in enumerate(glob.glob(\"./embedding_model_data/processed_pair_data/train/*\")):\n",
    "    _ = split_file(file_name, ith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc36181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08d73b73",
   "metadata": {},
   "source": [
    "### Make TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd1a45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 32\n",
    "name_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"name_transformer_wordpiece_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a7772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ['train','val','test']:\n",
    "#     try:\n",
    "#         os.system(f\"rm -r ./embedding_model_data/tfrecords/{i}/*\")\n",
    "#         print(\"Done\")\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef9ec0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_serialize_example(f0, f1, f2, f3, f4):\n",
    "    \"\"\"\n",
    "    Serialization function.\n",
    "    \"\"\"\n",
    "    tf_string = tf.py_function(serialize_example, (f0, f1, f2, f3, f4), tf.string)\n",
    "    return tf.reshape(tf_string, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fb0bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(name_1_x, name_1_y, name_2_x, name_2_y, labels):\n",
    "    \"\"\"\n",
    "    Takes in features and outputs them to a serialized string that can be written to\n",
    "    a file using the TFRecord Writer.\n",
    "    \"\"\"\n",
    "    name_1_x_list = tf.train.Int64List(value=name_1_x.numpy().tolist())\n",
    "    name_1_y_list = tf.train.Int64List(value=name_1_y.numpy().tolist())\n",
    "    name_2_x_list = tf.train.Int64List(value=name_2_x.numpy().tolist())\n",
    "    name_2_y_list = tf.train.Int64List(value=name_2_y.numpy().tolist())\n",
    "    labels_list = tf.train.Int64List(value=labels.numpy().tolist())\n",
    "    \n",
    "    name_1_x_feature = tf.train.Feature(int64_list = name_1_x_list)\n",
    "    name_1_y_feature = tf.train.Feature(int64_list = name_1_y_list)\n",
    "    name_2_x_feature = tf.train.Feature(int64_list = name_2_x_list)\n",
    "    name_2_y_feature = tf.train.Feature(int64_list = name_2_y_list)\n",
    "    labels_feature = tf.train.Feature(int64_list = labels_list)\n",
    "    \n",
    "    features_for_example = {\n",
    "        'name_1_x': name_1_x_feature,\n",
    "        'name_1_y': name_1_y_feature,\n",
    "        'name_2_x': name_2_x_feature,\n",
    "        'name_2_y': name_2_y_feature,\n",
    "        'labels': labels_feature\n",
    "    }\n",
    "    \n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=features_for_example))\n",
    "    \n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "591291a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_raw_input(tok_input, start_tok, end_tok):\n",
    "    temp_tok_input = (start_tok + tok_input)[:(MAX_LEN-1)]+end_tok\n",
    "    final_tok_input = temp_tok_input + [0]*(MAX_LEN-len(temp_tok_input))\n",
    "    return np.asarray(final_tok_input, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7215676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecords_dataset(filename, iter_num, dataset_type='train'):\n",
    "    \"\"\"\n",
    "    Creates a TF Dataset that can then be saved to a file to make it faster to read\n",
    "    data during training and allow for transferring of data between compute instances.\n",
    "    \"\"\"\n",
    "    data = pd.read_parquet(filename)\n",
    "    \n",
    "    # Getting tokenized data\n",
    "    start_token = name_tokenizer(\"[START]\")['input_ids']\n",
    "    end_token = name_tokenizer(\"[END]\")['input_ids']\n",
    "    \n",
    "    print(\"------------tokenizing name_1\")\n",
    "    data['tokenized_name_1_x'] = name_tokenizer(data['name_1'].tolist())['input_ids']\n",
    "    data['tokenized_name_1_x'] = data['tokenized_name_1_x'].apply(lambda x: \n",
    "                                                                  create_labeled_raw_input(x, \n",
    "                                                                                           start_token, \n",
    "                                                                                           end_token))\n",
    "    data['tokenized_name_1_y'] = [np.asarray(start_token + [0]*(MAX_LEN-1), dtype=np.int64)]*data.shape[0]\n",
    "    print(\"------------tokenizing name_2\")\n",
    "    data['tokenized_name_2_x'] = name_tokenizer(data['name_2'].tolist())['input_ids']\n",
    "    data['tokenized_name_2_x'] = data['tokenized_name_2_x'].apply(lambda x: \n",
    "                                                                  create_labeled_raw_input(x, \n",
    "                                                                                           start_token, \n",
    "                                                                                           end_token))\n",
    "    data['tokenized_name_2_y'] = [np.asarray(start_token + [0]*(MAX_LEN-1), dtype=np.int64)]*data.shape[0]\n",
    "    \n",
    "    print(\"------------getting labeled data\")\n",
    "    data['labels'] = data['label'].apply(lambda x: np.asarray([x], dtype=np.int64))\n",
    "    \n",
    "    # Creating TF Dataset\n",
    "    ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(data['tokenized_name_1_x'].to_list()),\n",
    "                              tf.data.Dataset.from_tensor_slices(data['tokenized_name_1_y'].to_list()),\n",
    "                              tf.data.Dataset.from_tensor_slices(data['tokenized_name_2_x'].to_list()),\n",
    "                              tf.data.Dataset.from_tensor_slices(data['tokenized_name_2_y'].to_list()),\n",
    "                              tf.data.Dataset.from_tensor_slices(data['labels'].to_list())))\n",
    "    \n",
    "    serialized_features_dataset = ds.map(tf_serialize_example)\n",
    "    \n",
    "    print(\"------------writing to tfrecord\")\n",
    "    \n",
    "    filename = f\"./embedding_model_data/tfrecords/{dataset_type}/{str(iter_num).zfill(4)}.tfrecord\"\n",
    "    writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "    writer.write(serialized_features_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77eecd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./embedding_model_data/new_proc_pair_data/train/2a.parquet 2a\n",
      "------------tokenizing name_1\n",
      "------------tokenizing name_2\n",
      "------------getting labeled data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 14:55:11.134995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-21 14:55:11.136072: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-21 14:55:11.136284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-21 14:55:11.136771: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-21 14:55:11.137247: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-21 14:55:11.137468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-21 14:55:11.137668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-21 14:55:13.035641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-21 14:55:13.035900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-21 14:55:13.036107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-21 14:55:13.036276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14200 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------writing to tfrecord\n",
      "WARNING:tensorflow:From /tmp/ipykernel_29327/448722704.py:42: TFRecordWriter.__init__ (from tensorflow.python.data.experimental.ops.writers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To write TFRecords to disk, use `tf.io.TFRecordWriter`. To save and load the contents of a dataset, use `tf.data.experimental.save` and `tf.data.experimental.load`\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in glob.glob(\"./embedding_model_data/new_proc_pair_data/train/*\")[4:16]:\n",
    "    print(f\"{i} {i.split('/')[-1].split('.parquet')[0]}\")\n",
    "    create_tfrecords_dataset(i, i.split('/')[-1].split('.parquet')[0], 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d3acb47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - ./embedding_model_data/processed_pair_data/val/0.parquet\n",
      "------------tokenizing name_1\n",
      "------------tokenizing name_2\n",
      "------------getting labeled data\n",
      "------------writing to tfrecord\n",
      "CPU times: user 41.6 s, sys: 3.74 s, total: 45.3 s\n",
      "Wall time: 36.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for ith, i in enumerate(glob.glob(\"./embedding_model_data/processed_pair_data/val/*\")):\n",
    "    print(f\"{ith} - {i}\")\n",
    "    create_tfrecords_dataset(i, ith, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d9465",
   "metadata": {},
   "source": [
    "### Read training data from tfrecord files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e541808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "    \"\"\"\n",
    "    Parses the TFRecord file.\n",
    "    \"\"\"\n",
    "    feature_description = {\n",
    "        'name_1_x': tf.io.FixedLenFeature((32,), tf.int64),\n",
    "        'name_1_y': tf.io.FixedLenFeature((32,), tf.int64),\n",
    "        'name_2_x': tf.io.FixedLenFeature((32,), tf.int64),\n",
    "        'name_2_y': tf.io.FixedLenFeature((32,), tf.int64),\n",
    "        'labels': tf.io.FixedLenFeature((1,), tf.int64)\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    name_1_x = tf.cast(example['name_1_x'], dtype=tf.int32)\n",
    "    name_1_y = tf.cast(example['name_1_y'], dtype=tf.int32)\n",
    "    name_2_x = tf.cast(example['name_2_x'], dtype=tf.int32)\n",
    "    name_2_y = tf.cast(example['name_2_y'], dtype=tf.int32)\n",
    "    labels = tf.cast(example['labels'][0], dtype=tf.int32)\n",
    "\n",
    "    return ((name_1_x,name_1_y),(name_2_x,name_2_y)),labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c9fdae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path, data_type='train', batch_size=512):\n",
    "    \"\"\"\n",
    "    Takes in a path to the TFRecords and returns a TF Dataset to be used for training.\n",
    "    \"\"\"\n",
    "    if data_type=='train':\n",
    "        tfrecords = [f\"{path}{data_type}/{x}\" for x in os.listdir(f\"{path}{data_type}/\") \n",
    "                     if x.endswith('tfrecord')][:5]\n",
    "    else:\n",
    "        tfrecords = [f\"{path}{data_type}/{x}\" for x in os.listdir(f\"{path}{data_type}/\") \n",
    "                     if x.endswith('tfrecord')]\n",
    "    tfrecords.sort()\n",
    "    \n",
    "    32\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecords, num_parallel_reads=tf.data.AUTOTUNE)\n",
    "    parsed_dataset = raw_dataset.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    parsed_dataset = parsed_dataset\\\n",
    "        .shuffle(batch_size)\\\n",
    "        .batch(batch_size,drop_remainder=True) \\\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return parsed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f8cc6",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b518395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "    angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1) \n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b728187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'vocab_size': int(self.vocab_size),\n",
    "            'd_model': int(self.d_model)\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9edf72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        \n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "    \n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask = True)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f043b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff, activation='relu'),\n",
    "          tf.keras.layers.Dense(d_model),\n",
    "          tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x) \n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'd_model': int(self.d_model),\n",
    "            'dff': int(self.dff),\n",
    "            'dropout_rate': float(self.dropout_rate)\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b685b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        self.context_data = x\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'd_model': int(self.d_model),\n",
    "            'num_heads': int(self.num_heads),\n",
    "            'dff': int(self.dff),\n",
    "            'dropout_rate': float(self.dropout_rate)\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cba5290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads,\n",
    "                 dff, vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(\n",
    "            vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # `x` is token-IDs shape: (batch, seq_len)\n",
    "        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "            \n",
    "        self.last_1of4_encoder_output = self.enc_layers[-4].context_data\n",
    "        self.last_2of4_encoder_output = self.enc_layers[-3].context_data\n",
    "        self.last_3of4_encoder_output = self.enc_layers[-2].context_data\n",
    "        self.last_4of4_encoder_output = self.enc_layers[-1].context_data\n",
    "        \n",
    "        self.encoder_context_data = x\n",
    "\n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_layers': int(self.num_layers),\n",
    "            'd_model': int(self.d_model),\n",
    "            'num_heads': int(self.num_heads),\n",
    "            'dff': int(self.dff),\n",
    "            'vocab_size': int(self.vocab_size),\n",
    "            'dropout_rate': float(self.dropout_rate)\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe2edf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context):\n",
    "        x = self.causal_self_attention(x=x)\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "        # Cache the last attention scores for plotting later\n",
    "        self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        self.context_data = x\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'd_model': int(self.d_model),\n",
    "            'num_heads': int(self.num_heads),\n",
    "            'dff': int(self.dff),\n",
    "            'dropout_rate': float(self.dropout_rate)\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "980bcd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                                 d_model=d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=self.d_model, num_heads=num_heads,\n",
    "                         dff=self.dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "        # `x` is token-IDs shape (batch, target_seq_len)\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x  = self.dec_layers[i](x, context)\n",
    "\n",
    "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "        self.last_1of4_decoder_output = self.dec_layers[-4].context_data\n",
    "        self.last_2of4_decoder_output = self.dec_layers[-3].context_data\n",
    "        self.last_3of4_decoder_output = self.dec_layers[-2].context_data\n",
    "        self.last_4of4_decoder_output = self.dec_layers[-1].context_data\n",
    "\n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        \n",
    "        self.decoder_context_data = x\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_layers': int(self.num_layers),\n",
    "            'd_model': int(self.d_model),\n",
    "            'num_heads': int(self.num_heads),\n",
    "            'dff': int(self.dff),\n",
    "            'vocab_size': int(self.vocab_size),\n",
    "            'dropout_rate': float(self.dropout_rate)\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "564a8295",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForSiam(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=input_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=target_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        self.dense_layer_1 = tf.keras.layers.Dense(1024)\n",
    "        self.dense_layer_2 = tf.keras.layers.Dense(1024)\n",
    "        self.pooling_layer = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.name_embedding_layer = tf.keras.layers.Dense(64)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "        # first argument.\n",
    "        context, x  = inputs\n",
    "\n",
    "        context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "\n",
    "        x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "        \n",
    "        # Adding head to get name embedding\n",
    "        x = self.dense_layer_1(x)\n",
    "        x = self.dense_layer_2(x)\n",
    "        x = self.pooling_layer(x)\n",
    "        logits = self.name_embedding_layer(x)\n",
    "\n",
    "        try:\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_layers': int(self.num_layers),\n",
    "            'd_model': int(self.d_model),\n",
    "            'num_heads': int(self.num_heads),\n",
    "            'dff': int(self.dff),\n",
    "            'input_vocab_size': int(self.input_vocab_size),\n",
    "            'target_vocab_size': int(self.target_vocab_size),\n",
    "            'dropout_rate': float(self.dropout_rate)\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cec0b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiamNet(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.transformer = TransformerForSiam(num_layers=num_layers, d_model=d_model,\n",
    "                                                num_heads=num_heads, dff=dff,\n",
    "                                                input_vocab_size=input_vocab_size,\n",
    "                                                target_vocab_size=target_vocab_size,\n",
    "                                                dropout_rate=dropout_rate)\n",
    "        \n",
    "        self.final_output = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        first_name, second_name  = inputs\n",
    "        \n",
    "        first_name_output = self.transformer(first_name)\n",
    "        second_name_output = self.transformer(second_name)\n",
    "        \n",
    "        emb_diff = tf.keras.layers.subtract([first_name_output, second_name_output])\n",
    "        concat_vector = tf.keras.layers.concatenate([first_name_output, second_name_output, emb_diff])\n",
    "        \n",
    "        x = self.final_output(concat_vector)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_layers': int(self.num_layers),\n",
    "            'd_model': int(self.d_model),\n",
    "            'num_heads': int(self.num_heads),\n",
    "            'dff': int(self.dff),\n",
    "            'input_vocab_size': int(self.input_vocab_size),\n",
    "            'target_vocab_size': int(self.target_vocab_size),\n",
    "            'dropout_rate': float(self.dropout_rate)\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2174d1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7945f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'd_model': int(self.d_model),\n",
    "            'warmup_steps': int(self.warmup_steps),\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4aacc3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def masked_loss(label, pred):\n",
    "#     mask = label != 0\n",
    "#     loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "#     loss = loss_object(label, pred)\n",
    "\n",
    "#     mask = tf.cast(mask, dtype=loss.dtype)\n",
    "#     loss *= mask\n",
    "\n",
    "#     loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# def masked_accuracy(label, pred):\n",
    "#     pred = tf.argmax(pred, axis=2)\n",
    "#     label = tf.cast(label, pred.dtype)\n",
    "#     match = label == pred\n",
    "\n",
    "#     mask = label != 0\n",
    "\n",
    "#     match = match & mask\n",
    "\n",
    "#     match = tf.cast(match, dtype=tf.float32)\n",
    "#     mask = tf.cast(mask, dtype=tf.float32)\n",
    "#     return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b6241a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, curr_lr):\n",
    "    \"\"\"\n",
    "    Setting up a exponentially decaying learning rate.\n",
    "    \"\"\"\n",
    "    rampup_epochs = 2\n",
    "    exp_decay = 0.17\n",
    "    def lr(epoch, beg_lr, rampup_epochs, exp_decay):\n",
    "        if epoch < rampup_epochs:\n",
    "            return beg_lr*4*epoch\n",
    "        else:\n",
    "            return beg_lr * math.exp(-exp_decay * epoch)\n",
    "    return lr(epoch, start_lr, rampup_epochs, exp_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64021114",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 6\n",
    "d_model = 64\n",
    "dff = 128\n",
    "num_heads = 8\n",
    "dropout_rate = 0.15\n",
    "num_epochs = 12\n",
    "MAX_LEN=32\n",
    "BATCH_SIZE=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13eafc8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 14:38:26.227095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-21 14:38:26.228167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-21 14:38:26.228382: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-21 14:38:26.229074: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-21 14:38:26.229606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-21 14:38:26.229833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-21 14:38:26.230027: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-21 14:38:28.223060: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-21 14:38:28.223321: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-21 14:38:28.223527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-21 14:38:28.223696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14200 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 14:38:56.922179: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8302\n"
     ]
    }
   ],
   "source": [
    "# Allow for use of multiple GPUs\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    \n",
    "    enc_weights = tf.keras.models.load_model(\"./transformer_data/models/20230418_8_6_64_128_15/model_epoch12ckpt/\", \n",
    "                                                    compile=False).encoder.get_weights()\n",
    "    \n",
    "    dec_weights = tf.keras.models.load_model(\"./transformer_data/models/20230418_8_6_64_128_15/model_epoch12ckpt/\", \n",
    "                                                    compile=False).decoder.get_weights()\n",
    "    \n",
    "    siamese_network = SiamNet(\n",
    "        num_layers=num_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dff=dff,\n",
    "        input_vocab_size=name_tokenizer.vocab_size,\n",
    "        target_vocab_size=name_tokenizer.vocab_size,\n",
    "        dropout_rate=dropout_rate)\n",
    "    \n",
    "    learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                         epsilon=1e-9)\n",
    "\n",
    "    siamese_network.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(name='binary_crossentropy'),\n",
    "        optimizer=optimizer,\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "    curr_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "    filepath_1 = f\"./embedding_model_data/models/{curr_date}_{num_heads}_{num_layers}_{d_model}_{dff}_{int(dropout_rate*100)}/\" \\\n",
    "\n",
    "\n",
    "    filepath = filepath_1 + \"model_epoch{epoch:02d}ckpt\"\n",
    "\n",
    "    # Adding in checkpointing\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', \n",
    "                                                          verbose=0, save_best_only=False,\n",
    "                                                          save_weights_only=False, mode='auto',\n",
    "                                                          save_freq='epoch')\n",
    "\n",
    "    # Adding in early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=4)\n",
    "\n",
    "#     lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "    callbacks = [model_checkpoint, early_stopping]\n",
    "    \n",
    "    train_batches = get_dataset(\"./embedding_model_data/tfrecords/\", data_type='train', batch_size=BATCH_SIZE)\n",
    "    val_batches = get_dataset(\"./embedding_model_data/tfrecords/\", data_type='val', batch_size=BATCH_SIZE)\n",
    "    \n",
    "    for (name_1, name_2), labels in val_batches.take(1):\n",
    "        pass\n",
    "        \n",
    "    output = siamese_network((name_1, name_2))\n",
    "    \n",
    "    siamese_network.transformer.encoder.set_weights(enc_weights)\n",
    "    siamese_network.transformer.decoder.set_weights(dec_weights)\n",
    "    siamese_network.transformer.encoder.trainable = False\n",
    "    siamese_network.transformer.decoder.trainable = False\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166a2ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7ccc8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"siam_net\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer_for_siam (Trans  multiple                 6207040   \n",
      " formerForSiam)                                                  \n",
      "                                                                 \n",
      " dense_27 (Dense)            multiple                  193       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,207,233\n",
      "Trainable params: 1,181,953\n",
      "Non-trainable params: 5,025,280\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6455292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 14:40:04.902027: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f8a2000e7c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-21 14:40:04.902099: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2023-04-21 14:40:04.976147: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-04-21 14:40:05.651116: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    120/Unknown - 100s 325ms/step - loss: 0.9275 - binary_accuracy: 0.5675"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43msiamese_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/keras/engine/training.py:1712\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1709\u001b[0m     }\n\u001b[1;32m   1710\u001b[0m     epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n\u001b[0;32m-> 1712\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_logs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1713\u001b[0m training_logs \u001b[38;5;241m=\u001b[39m epoch_logs\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/keras/callbacks.py:454\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_logs(logs)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 454\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/keras/callbacks.py:1476\u001b[0m, in \u001b[0;36mModelCheckpoint.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs_since_last_save \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1476\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/keras/callbacks.py:1563\u001b[0m, in \u001b[0;36mModelCheckpoint._save_model\u001b[0;34m(self, epoch, batch, logs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_weights(\n\u001b[1;32m   1560\u001b[0m                 filepath, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options\n\u001b[1;32m   1561\u001b[0m             )\n\u001b[1;32m   1562\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1563\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1567\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_remove_file()\n\u001b[1;32m   1568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIsADirectoryError\u001b[39;00m:  \u001b[38;5;66;03m# h5py 3.x\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/keras/engine/training.py:2795\u001b[0m, in \u001b[0;36mModel.save\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m   2740\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[1;32m   2741\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\n\u001b[1;32m   2742\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2749\u001b[0m     save_traces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2750\u001b[0m ):\n\u001b[1;32m   2752\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Saves the model to Tensorflow SavedModel or a single HDF5 file.\u001b[39;00m\n\u001b[1;32m   2753\u001b[0m \n\u001b[1;32m   2754\u001b[0m \u001b[38;5;124;03m    Please see `tf.keras.models.save_model` or the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2792\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   2793\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2795\u001b[0m     \u001b[43msave\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2796\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2798\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2799\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2800\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2801\u001b[0m \u001b[43m        \u001b[49m\u001b[43msignatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2802\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2803\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_traces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2804\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/keras/saving/legacy/save.py:167\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m serialization\u001b[38;5;241m.\u001b[39mSharedObjectSavingScope():\n\u001b[0;32m--> 167\u001b[0m         \u001b[43msaved_model_save\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43minclude_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43msignatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_traces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/keras/saving/legacy/saved_model/save.py:97\u001b[0m, in \u001b[0;36msave\u001b[0;34m(model, filepath, overwrite, include_optimizer, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mdeprecated_internal_learning_phase_scope(\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mkeras_option_scope(save_traces):\n\u001b[0;32m---> 97\u001b[0m         saved_nodes, node_paths \u001b[38;5;241m=\u001b[39m \u001b[43msave_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_and_return_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# Save all metadata to a separate file in the SavedModel directory.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m generate_keras_metadata(saved_nodes, node_paths)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py:1267\u001b[0m, in \u001b[0;36msave_and_return_nodes\u001b[0;34m(obj, export_dir, signatures, options, experimental_skip_checkpoint)\u001b[0m\n\u001b[1;32m   1263\u001b[0m saved_model \u001b[38;5;241m=\u001b[39m saved_model_pb2\u001b[38;5;241m.\u001b[39mSavedModel()\n\u001b[1;32m   1264\u001b[0m meta_graph_def \u001b[38;5;241m=\u001b[39m saved_model\u001b[38;5;241m.\u001b[39mmeta_graphs\u001b[38;5;241m.\u001b[39madd()\n\u001b[1;32m   1266\u001b[0m _, exported_graph, object_saver, asset_info, saved_nodes, node_paths \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1267\u001b[0m     \u001b[43m_build_meta_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_graph_def\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1268\u001b[0m saved_model\u001b[38;5;241m.\u001b[39msaved_model_schema_version \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1269\u001b[0m     constants\u001b[38;5;241m.\u001b[39mSAVED_MODEL_SCHEMA_VERSION)\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;66;03m# Write the checkpoint, copy assets into the assets directory, and write out\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;66;03m# the SavedModel proto itself.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py:1440\u001b[0m, in \u001b[0;36m_build_meta_graph\u001b[0;34m(obj, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a MetaGraph under a save context.\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m \n\u001b[1;32m   1415\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;124;03m  saveable_view.node_paths: _SaveableView paths.\u001b[39;00m\n\u001b[1;32m   1437\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m save_context\u001b[38;5;241m.\u001b[39msave_context(options):\n\u001b[0;32m-> 1440\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_build_meta_graph_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_graph_def\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py:1393\u001b[0m, in \u001b[0;36m_build_meta_graph_impl\u001b[0;34m(obj, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1390\u001b[0m augmented_graph_view\u001b[38;5;241m.\u001b[39mset_signature(signature_map, wrapped_functions)\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;66;03m# Use _SaveableView to provide a frozen listing of properties and functions.\u001b[39;00m\n\u001b[0;32m-> 1393\u001b[0m saveable_view \u001b[38;5;241m=\u001b[39m \u001b[43m_SaveableView\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugmented_graph_view\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1394\u001b[0m object_saver \u001b[38;5;241m=\u001b[39m checkpoint\u001b[38;5;241m.\u001b[39mTrackableSaver(augmented_graph_view)\n\u001b[1;32m   1395\u001b[0m asset_info, exported_graph \u001b[38;5;241m=\u001b[39m _fill_meta_graph_def(\n\u001b[1;32m   1396\u001b[0m     meta_graph_def, saveable_view, signatures, options\u001b[38;5;241m.\u001b[39mnamespace_whitelist,\n\u001b[1;32m   1397\u001b[0m     options\u001b[38;5;241m.\u001b[39mexperimental_custom_gradients)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py:266\u001b[0m, in \u001b[0;36m_SaveableView.__init__\u001b[0;34m(self, augmented_graph_view, options)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugmented_graph_view \u001b[38;5;241m=\u001b[39m augmented_graph_view\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options \u001b[38;5;241m=\u001b[39m options\n\u001b[1;32m    264\u001b[0m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trackable_objects, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_paths, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_ids,\n\u001b[1;32m    265\u001b[0m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slot_variables, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobject_names) \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 266\u001b[0m      \u001b[43mcheckpoint_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjects_ids_and_slot_variables_and_paths\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugmented_graph_view\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    269\u001b[0m untraced_functions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugmented_graph_view\u001b[38;5;241m.\u001b[39muntraced_functions\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m untraced_functions:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/checkpoint/util.py:159\u001b[0m, in \u001b[0;36mobjects_ids_and_slot_variables_and_paths\u001b[0;34m(graph_view)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjects_ids_and_slot_variables_and_paths\u001b[39m(graph_view):\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Traverse the object graph and list all accessible objects.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m  Looks for `Trackable` objects which are dependencies of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m                object -> node id, slot variables, object_names)\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m   trackable_objects, node_paths \u001b[38;5;241m=\u001b[39m \u001b[43mgraph_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbreadth_first_traversal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m   object_names \u001b[38;5;241m=\u001b[39m object_identity\u001b[38;5;241m.\u001b[39mObjectIdentityDictionary()\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m obj, path \u001b[38;5;129;01min\u001b[39;00m node_paths\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/checkpoint/graph_view.py:124\u001b[0m, in \u001b[0;36mObjectGraphView.breadth_first_traversal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbreadth_first_traversal\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 124\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_breadth_first_traversal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py:143\u001b[0m, in \u001b[0;36m_AugmentedGraphView._breadth_first_traversal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all trackable objects in the SavedObjectGraph.\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# This method is overriden to merge all equivalent constant tensors and\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Assets in the object graph.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m trackable_objects, _ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_AugmentedGraphView\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_breadth_first_traversal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    145\u001b[0m asset_paths \u001b[38;5;241m=\u001b[39m object_identity\u001b[38;5;241m.\u001b[39mObjectIdentityDictionary()\n\u001b[1;32m    146\u001b[0m constant_captures \u001b[38;5;241m=\u001b[39m object_identity\u001b[38;5;241m.\u001b[39mObjectIdentityDictionary()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/checkpoint/graph_view.py:128\u001b[0m, in \u001b[0;36mObjectGraphView._breadth_first_traversal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_breadth_first_traversal\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Find shortest paths to all dependencies of self.root.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mObjectGraphView\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_descendants_with_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/checkpoint/trackable_view.py:111\u001b[0m, in \u001b[0;36mTrackableView._descendants_with_paths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m current_trackable \u001b[38;5;241m=\u001b[39m to_visit\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m    110\u001b[0m bfs_sorted\u001b[38;5;241m.\u001b[39mappend(current_trackable)\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, dependency \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_trackable\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    112\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dependency \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m node_paths:\n\u001b[1;32m    113\u001b[0m     node_paths[dependency] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    114\u001b[0m         node_paths[current_trackable] \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    115\u001b[0m         (base\u001b[38;5;241m.\u001b[39mTrackableReference(name, dependency),))\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/checkpoint/graph_view.py:97\u001b[0m, in \u001b[0;36mObjectGraphView.children\u001b[0;34m(self, obj, save_type, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all child trackables attached to obj.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m  Dictionary of all children attached to the object with name to trackable.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m children \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, ref \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlist_children(obj, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     98\u001b[0m   children[name] \u001b[38;5;241m=\u001b[39m ref\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m children\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py:177\u001b[0m, in \u001b[0;36m_AugmentedGraphView.list_children\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_children_cache:\n\u001b[1;32m    175\u001b[0m   children \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_children_cache[obj] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 177\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_AugmentedGraphView\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_children\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m      \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43msave_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSaveType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSAVEDMODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialization_cache\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child, defun\u001b[38;5;241m.\u001b[39mConcreteFunction):\n\u001b[1;32m    182\u001b[0m       child \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_uncache_variable_captures(child)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/checkpoint/graph_view.py:75\u001b[0m, in \u001b[0;36mObjectGraphView.list_children\u001b[0;34m(self, obj, save_type, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns list of all child trackables attached to obj.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m  List of all children attached to the object.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m children \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, ref \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mObjectGraphView\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     77\u001b[0m   children\u001b[38;5;241m.\u001b[39mappend(base\u001b[38;5;241m.\u001b[39mTrackableReference(name, ref))\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# GraphView objects may define children of the root object that are not\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# actually attached, e.g. a Checkpoint object's save_counter.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/checkpoint/trackable_view.py:84\u001b[0m, in \u001b[0;36mTrackableView.children\u001b[0;34m(cls, obj, save_type, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m obj\u001b[38;5;241m.\u001b[39m_maybe_initialize_trackable()\n\u001b[1;32m     83\u001b[0m children \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, ref \u001b[38;5;129;01min\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trackable_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     85\u001b[0m   ref \u001b[38;5;241m=\u001b[39m converter\u001b[38;5;241m.\u001b[39mconvert_to_trackable(ref, parent\u001b[38;5;241m=\u001b[39mobj)\n\u001b[1;32m     86\u001b[0m   children[name] \u001b[38;5;241m=\u001b[39m ref\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/trackable/autotrackable.py:115\u001b[0m, in \u001b[0;36mAutoTrackable._trackable_children\u001b[0;34m(self, save_type, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m functions\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    114\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fn, core_types\u001b[38;5;241m.\u001b[39mGenericFunction):\n\u001b[0;32m--> 115\u001b[0m     \u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_list_all_concrete_functions_for_serialization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Additional dependencies may have been generated during function tracing\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# (e.g. captured variables). Make sure we return those too.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m children \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1160\u001b[0m, in \u001b[0;36mFunction._list_all_concrete_functions_for_serialization\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1158\u001b[0m concrete_functions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m args, kwargs \u001b[38;5;129;01min\u001b[39;00m seen_signatures:\n\u001b[0;32m-> 1160\u001b[0m   concrete_functions\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_concrete_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_functions\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1215\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1214\u001b[0m   \u001b[38;5;66;03m# Implements GenericFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1215\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1216\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1206\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1201\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn\u001b[38;5;241m.\u001b[39m_get_concrete_function_garbage_collected(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m       \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1204\u001b[0m   \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m   \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m-> 1206\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m   1207\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1208\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1210\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:192\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mvalidate_inputs_with_signature(args, kwargs)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 192\u001b[0m   concrete_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_concrete_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m   seen_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    194\u001b[0m   captured \u001b[38;5;241m=\u001b[39m object_identity\u001b[38;5;241m.\u001b[39mObjectIdentitySet(\n\u001b[1;32m    195\u001b[0m       concrete_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minternal_captures)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:157\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m   args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    155\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:372\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# Create a cache_key with args and captures\u001b[39;00m\n\u001b[1;32m    369\u001b[0m traced_func_key, traced_func_deletion_observer \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    370\u001b[0m     function_context\u001b[38;5;241m.\u001b[39mmake_cache_key((args, kwargs), captures))\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_cache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraced_func_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtraced_func_deletion_observer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mconcrete_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function, filtered_flat_args\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/core/function/polymorphism/function_cache.py:143\u001b[0m, in \u001b[0;36mFunctionCache.add\u001b[0;34m(self, key, deletion_observer, concrete)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Adds a new concrete function alongside its key.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m  concrete: The concrete function to be added to the cache.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_primary[key] \u001b[38;5;241m=\u001b[39m concrete\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m deletion_observer\u001b[38;5;241m.\u001b[39madd_listener(\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(key) \u001b[38;5;28;01mif\u001b[39;00m DELETE_WITH_WEAKREF \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/core/function/polymorphism/type_dispatch.py:51\u001b[0m, in \u001b[0;36mTypeDispatchTable.add_target\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_target\u001b[39m(\u001b[38;5;28mself\u001b[39m, target: trace\u001b[38;5;241m.\u001b[39mTraceType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Adds a new target type.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     52\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m request \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_cache:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39mis_subtype_of(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_cache[request]):\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/core/function/polymorphism/function_cache.py:77\u001b[0m, in \u001b[0;36mFunctionCacheKey.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m---> 77\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/tensorflow/core/function/polymorphism/function_type.py:247\u001b[0m, in \u001b[0;36mFunctionType.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    246\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m(\n\u001b[0;32m--> 247\u001b[0m       (\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaptures\u001b[38;5;241m.\u001b[39mitems())))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = siamese_network.fit(val_batches,\n",
    "                epochs=num_epochs,\n",
    "                validation_data=val_batches,\n",
    "                callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "843c37c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89382f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(str(history.history), open(f\"{filepath_1}_{num_epochs}EPOCHS_HISTORY.json\", 'w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c989aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79e93ed",
   "metadata": {},
   "source": [
    "### Transforming with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49810dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS=32\n",
    "MAX_LEN=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a279664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n"
     ]
    }
   ],
   "source": [
    "temp_model = tf.keras.models.load_model(\"./models/20230418_8_6_64_128_15/model_epoch12ckpt/\", \n",
    "                                                    compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5589eacd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enc_weights = temp_model.encoder.get_weights()\n",
    "dec_weights = temp_model.decoder.get_weights()\n",
    "final_layer_weights = temp_model.final_layer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b763daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.encoder.set_weights(enc_weights)\n",
    "transformer.decoder.set_weights(dec_weights)\n",
    "transformer.final_layer.set_weights(final_layer_weights)\n",
    "transformer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b98ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = tf.keras.Model(inputs=(x,y), \n",
    "#                            outputs=transformer.decoder.dec_layers[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "824a2b96",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_TOKENS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTranslator\u001b[39;00m(tf\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokenizer, transformer):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n",
      "Cell \u001b[0;32mIn[29], line 6\u001b[0m, in \u001b[0;36mTranslator\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m transformer\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence, max_length\u001b[38;5;241m=\u001b[39m\u001b[43mMAX_TOKENS\u001b[49m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sentence, tf\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sentence\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_TOKENS' is not defined"
     ]
    }
   ],
   "source": [
    "class Translator(tf.Module):\n",
    "    def __init__(self, tokenizer, transformer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __call__(self, sentence, max_length=MAX_TOKENS):\n",
    "        # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\n",
    "        assert isinstance(sentence, tf.Tensor)\n",
    "        if len(sentence.shape) == 0:\n",
    "            sentence = sentence[tf.newaxis]\n",
    "\n",
    "        sentence = tf.keras.utils.pad_sequences(self.tokenizer([x.decode('utf-8') \n",
    "                                                    for x in sentence.numpy().tolist()])['input_ids'], \n",
    "                                                maxlen=MAX_TOKENS, \n",
    "                                                dtype='int32',\n",
    "                                                padding='post',\n",
    "                                                truncating='post',\n",
    "                                                value=0)\n",
    "\n",
    "        encoder_input = sentence\n",
    "\n",
    "        # As the output language is English, initialize the output with the\n",
    "        # English `[START]` token.\n",
    "        start_end = tf.constant(self.tokenizer(['[START][END]'])['input_ids'][0])\n",
    "        start = start_end[0][tf.newaxis]\n",
    "        end = start_end[1][tf.newaxis]\n",
    "\n",
    "        # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "        # dynamic-loop can be traced by `tf.function`.\n",
    "        output_array = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "        output_array = output_array.write(0, start)\n",
    "\n",
    "        for i in tf.range(max_length):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "            # Select the last token from the `seq_len` dimension.\n",
    "            predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "            predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "            # Concatenate the `predicted_id` to the output which is given to the\n",
    "            # decoder as its input.\n",
    "            output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "            if predicted_id == end:\n",
    "                break\n",
    "\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        # The output shape is `(1, tokens)`.\n",
    "        print(output)\n",
    "        text = self.tokenizer.decode(output.numpy().tolist()[0], skip_special_tokens=True, \n",
    "                 clean_up_tokenization_spaces=True).replace(\" ##\", \"\").replace(\" - \", \"-\")\n",
    "\n",
    "        tokens = output.numpy().tolist()\n",
    "\n",
    "        # `tf.function` prevents us from using the attention_weights that were\n",
    "        # calculated on the last iteration of the loop.\n",
    "        # So, recalculate them outside the loop.\n",
    "        self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "        attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "        return text, tokens, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "277196f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"name_transformer_wordpiece_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e518ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_tokenizer.decode(output_labels[4][:10], skip_special_tokens=True, \n",
    "#                  clean_up_tokenization_spaces=True).replace(\" ##\", \"\").replace(\" - \", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eef68848",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(name_tokenizer, trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c99aa06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translation(sentence, tokens, ground_truth):\n",
    "    print(f'{\"Input:\":15s}: {sentence}')\n",
    "    print(f'{\"Prediction\":15s}: {tokens}')\n",
    "    print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf571f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '[START]Oliver, John[END]'\n",
    "ground_truth = 'John Oliver'\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e049c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 32, 15264), dtype=float32, numpy=\n",
       "array([[[-10.503152  , -10.504201  ,   6.169404  , ...,  -8.845235  ,\n",
       "          -8.071333  ,  -5.4130135 ],\n",
       "        [ -5.9436817 ,  -5.9438343 ,  -1.4243493 , ...,   2.472556  ,\n",
       "           4.703054  ,   0.6202928 ],\n",
       "        [ -5.944227  ,  -5.9443827 ,  -1.4250772 , ...,   2.4673195 ,\n",
       "           4.7044845 ,   0.62325895],\n",
       "        ...,\n",
       "        [ -5.96473   ,  -5.9648876 ,  -1.377768  , ...,   2.4583683 ,\n",
       "           4.662452  ,   0.54505575],\n",
       "        [ -5.965795  ,  -5.9659495 ,  -1.3787947 , ...,   2.4593058 ,\n",
       "           4.6576986 ,   0.5326568 ],\n",
       "        [ -5.965687  ,  -5.9658384 ,  -1.3768545 , ...,   2.4614625 ,\n",
       "           4.658391  ,   0.53515875]]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8af71a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names = [\"J.R. Tolkien\",\n",
    "              \"Tolkien, J.R.\",\n",
    "              \"Justin Earl Tolkien\", \n",
    "              \"Jarvis Richard Tolkien\",\n",
    "              \"Tolkien, Justin\", \n",
    "              \"Tolkien JE\", \n",
    "              \"Max Trout\", \n",
    "              \"Maximus Trout\", \n",
    "              \"Maxe Trout\", \n",
    "              \"Mara Trout\", \n",
    "              \"M.R. Trout\",\n",
    "              \"Trout MRF\", \n",
    "              \"Gooding, Emily\", \n",
    "              \"Gooding, E.Y. MD\", \n",
    "              \"Jarvis Richard James\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca0614a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20bc02c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J.R. Tolkien\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Tolkien, J.R.\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Justin Earl Tolkien\n",
      "----Decoded name:  Justin Earl Tolkien\n",
      "Jarvis Richard Tolkien\n",
      "----Decoded name:  Richard Tolkien Jarvis\n",
      "Tolkien, Justin\n",
      "----Decoded name:  Justin Tolkien\n",
      "Tolkien JE\n",
      "----Decoded name:  J. E. Tolkien\n",
      "Max Trout\n",
      "----Decoded name:  Max Trout\n",
      "Maximus Trout\n",
      "----Decoded name:  Maximus Trout\n",
      "Maxe Trout\n",
      "----Decoded name:  Maxe Trout\n",
      "Mara Trout\n",
      "----Decoded name:  Mara Trout\n",
      "M.R. Trout\n",
      "----Decoded name:  M. R. Trout\n",
      "Trout MRF\n",
      "----Decoded name:  M. R. F. Trout\n",
      "Gooding, Emily\n",
      "----Decoded name:  Emily Gooding\n",
      "Gooding, E.Y. MD\n",
      "----Decoded name:  E. Y. Gooding\n",
      "Jarvis Richard James\n",
      "----Decoded name:  Richard James Jarvis\n"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "for test_name in test_names:\n",
    "    print(test_name)\n",
    "    start_end = tf.constant(name_tokenizer(['[START][END]'])['input_ids'][0])\n",
    "    start = start_end[0].numpy()\n",
    "    end = start_end[1].numpy()\n",
    "    encoder_input = name_tokenizer([f'[START]{test_name}[END]'])['input_ids'][0]\n",
    "    encoder_input = tf.convert_to_tensor([encoder_input + [0]*(MAX_LEN-len(encoder_input))])\n",
    "    final_output = [start]\n",
    "    for i in range(MAX_LEN):\n",
    "        output = tf.convert_to_tensor([final_output + [0]*(MAX_LEN-len(final_output))])\n",
    "        predictions = transformer((encoder_input, output), training=False)\n",
    "\n",
    "        # Select the last token\n",
    "        predicted_id = tf.argmax(predictions[0][i]).numpy()\n",
    "\n",
    "        # Add to output\n",
    "        final_output.append(predicted_id)\n",
    "\n",
    "        if predicted_id == end:\n",
    "            break\n",
    "    print(\"----Decoded name: \", name_tokenizer.decode(final_output, skip_special_tokens=True, \n",
    "                 clean_up_tokenization_spaces=True).replace(\" ##\", \"\").replace(\" - \", \"-\"))\n",
    "    _ = transformer((encoder_input, tf.convert_to_tensor([final_output[:1] + [0]*(MAX_LEN-len(final_output[:1]))])), \n",
    "            training=False)\n",
    "    \n",
    "    embs.append(transformer.decoder.dec_layers[-1].context_data[0][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01469443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f7303cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 J.R. Tolkien\n",
      "-------J.R. Tolkien: 1.0\n",
      "-------Tolkien, J.R.: 0.6725000143051147\n",
      "-------Justin Earl Tolkien: 0.5468999743461609\n",
      "-------Jarvis Richard Tolkien: 0.5748000144958496\n",
      "-------Tolkien, Justin: 0.43149998784065247\n",
      "-------Tolkien JE: 0.5461999773979187\n",
      "-------Max Trout: 0.24279999732971191\n",
      "-------Maximus Trout: 0.25119999051094055\n",
      "-------Maxe Trout: 0.2468000054359436\n",
      "-------Mara Trout: 0.1137000024318695\n",
      "-------M.R. Trout: 0.41499999165534973\n",
      "-------Trout MRF: 0.16910000145435333\n",
      "-------Gooding, Emily: 0.29100000858306885\n",
      "-------Gooding, E.Y. MD: 0.27379998564720154\n",
      "-------Jarvis Richard James: 0.5273000001907349\n",
      "\n",
      "\n",
      "1 Tolkien, J.R.\n",
      "-------J.R. Tolkien: 0.6725000143051147\n",
      "-------Tolkien, J.R.: 1.0\n",
      "-------Justin Earl Tolkien: 0.6068999767303467\n",
      "-------Jarvis Richard Tolkien: 0.47290000319480896\n",
      "-------Tolkien, Justin: 0.43050000071525574\n",
      "-------Tolkien JE: 0.5874999761581421\n",
      "-------Max Trout: 0.40700000524520874\n",
      "-------Maximus Trout: 0.44209998846054077\n",
      "-------Maxe Trout: 0.4047999978065491\n",
      "-------Mara Trout: 0.19189999997615814\n",
      "-------M.R. Trout: 0.15970000624656677\n",
      "-------Trout MRF: 0.373199999332428\n",
      "-------Gooding, Emily: 0.2827000021934509\n",
      "-------Gooding, E.Y. MD: 0.4147999882698059\n",
      "-------Jarvis Richard James: 0.3686999976634979\n",
      "\n",
      "\n",
      "2 Justin Earl Tolkien\n",
      "-------J.R. Tolkien: 0.5468999743461609\n",
      "-------Tolkien, J.R.: 0.6068999767303467\n",
      "-------Justin Earl Tolkien: 1.0\n",
      "-------Jarvis Richard Tolkien: 0.27790001034736633\n",
      "-------Tolkien, Justin: 0.8851000070571899\n",
      "-------Tolkien JE: 0.6025999784469604\n",
      "-------Max Trout: 0.34279999136924744\n",
      "-------Maximus Trout: 0.39590001106262207\n",
      "-------Maxe Trout: 0.35019999742507935\n",
      "-------Mara Trout: 0.3416000008583069\n",
      "-------M.R. Trout: 0.20149999856948853\n",
      "-------Trout MRF: 0.2994000017642975\n",
      "-------Gooding, Emily: 0.41999998688697815\n",
      "-------Gooding, E.Y. MD: 0.4426000118255615\n",
      "-------Jarvis Richard James: 0.13210000097751617\n",
      "\n",
      "\n",
      "3 Jarvis Richard Tolkien\n",
      "-------J.R. Tolkien: 0.5748000144958496\n",
      "-------Tolkien, J.R.: 0.47290000319480896\n",
      "-------Justin Earl Tolkien: 0.27790001034736633\n",
      "-------Jarvis Richard Tolkien: 1.0\n",
      "-------Tolkien, Justin: 0.2062000036239624\n",
      "-------Tolkien JE: 0.17299999296665192\n",
      "-------Max Trout: 0.49559998512268066\n",
      "-------Maximus Trout: 0.5005000233650208\n",
      "-------Maxe Trout: 0.5016999840736389\n",
      "-------Mara Trout: 0.2685999870300293\n",
      "-------M.R. Trout: 0.3147999942302704\n",
      "-------Trout MRF: 0.3944999873638153\n",
      "-------Gooding, Emily: 0.3285999894142151\n",
      "-------Gooding, E.Y. MD: 0.14219999313354492\n",
      "-------Jarvis Richard James: 0.9412000179290771\n",
      "\n",
      "\n",
      "4 Tolkien, Justin\n",
      "-------J.R. Tolkien: 0.43149998784065247\n",
      "-------Tolkien, J.R.: 0.43050000071525574\n",
      "-------Justin Earl Tolkien: 0.8851000070571899\n",
      "-------Jarvis Richard Tolkien: 0.2062000036239624\n",
      "-------Tolkien, Justin: 1.0\n",
      "-------Tolkien JE: 0.5234000086784363\n",
      "-------Max Trout: 0.2442999929189682\n",
      "-------Maximus Trout: 0.28049999475479126\n",
      "-------Maxe Trout: 0.24770000576972961\n",
      "-------Mara Trout: 0.22380000352859497\n",
      "-------M.R. Trout: 0.13199999928474426\n",
      "-------Trout MRF: 0.20730000734329224\n",
      "-------Gooding, Emily: 0.4636000096797943\n",
      "-------Gooding, E.Y. MD: 0.3345000147819519\n",
      "-------Jarvis Richard James: 0.12120000272989273\n",
      "\n",
      "\n",
      "5 Tolkien JE\n",
      "-------J.R. Tolkien: 0.5461999773979187\n",
      "-------Tolkien, J.R.: 0.5874999761581421\n",
      "-------Justin Earl Tolkien: 0.6025999784469604\n",
      "-------Jarvis Richard Tolkien: 0.17299999296665192\n",
      "-------Tolkien, Justin: 0.5234000086784363\n",
      "-------Tolkien JE: 1.0\n",
      "-------Max Trout: 0.23659999668598175\n",
      "-------Maximus Trout: 0.24619999527931213\n",
      "-------Maxe Trout: 0.23999999463558197\n",
      "-------Mara Trout: 0.08560000360012054\n",
      "-------M.R. Trout: 0.15940000116825104\n",
      "-------Trout MRF: 0.13199999928474426\n",
      "-------Gooding, Emily: 0.42149999737739563\n",
      "-------Gooding, E.Y. MD: 0.34450000524520874\n",
      "-------Jarvis Richard James: 0.09700000286102295\n",
      "\n",
      "\n",
      "6 Max Trout\n",
      "-------J.R. Tolkien: 0.24279999732971191\n",
      "-------Tolkien, J.R.: 0.40700000524520874\n",
      "-------Justin Earl Tolkien: 0.34279999136924744\n",
      "-------Jarvis Richard Tolkien: 0.49559998512268066\n",
      "-------Tolkien, Justin: 0.2442999929189682\n",
      "-------Tolkien JE: 0.23659999668598175\n",
      "-------Max Trout: 1.0\n",
      "-------Maximus Trout: 0.9915000200271606\n",
      "-------Maxe Trout: 0.9969000220298767\n",
      "-------Mara Trout: 0.5867999792098999\n",
      "-------M.R. Trout: 0.4205000102519989\n",
      "-------Trout MRF: 0.5200999975204468\n",
      "-------Gooding, Emily: 0.16439999639987946\n",
      "-------Gooding, E.Y. MD: 0.19859999418258667\n",
      "-------Jarvis Richard James: 0.37770000100135803\n",
      "\n",
      "\n",
      "7 Maximus Trout\n",
      "-------J.R. Tolkien: 0.25119999051094055\n",
      "-------Tolkien, J.R.: 0.44209998846054077\n",
      "-------Justin Earl Tolkien: 0.39590001106262207\n",
      "-------Jarvis Richard Tolkien: 0.5005000233650208\n",
      "-------Tolkien, Justin: 0.28049999475479126\n",
      "-------Tolkien JE: 0.24619999527931213\n",
      "-------Max Trout: 0.9915000200271606\n",
      "-------Maximus Trout: 1.0\n",
      "-------Maxe Trout: 0.9927999973297119\n",
      "-------Mara Trout: 0.5881999731063843\n",
      "-------M.R. Trout: 0.40139999985694885\n",
      "-------Trout MRF: 0.5159000158309937\n",
      "-------Gooding, Emily: 0.17730000615119934\n",
      "-------Gooding, E.Y. MD: 0.23360000550746918\n",
      "-------Jarvis Richard James: 0.3621000051498413\n",
      "\n",
      "\n",
      "8 Maxe Trout\n",
      "-------J.R. Tolkien: 0.2468000054359436\n",
      "-------Tolkien, J.R.: 0.4047999978065491\n",
      "-------Justin Earl Tolkien: 0.35019999742507935\n",
      "-------Jarvis Richard Tolkien: 0.5016999840736389\n",
      "-------Tolkien, Justin: 0.24770000576972961\n",
      "-------Tolkien JE: 0.23999999463558197\n",
      "-------Max Trout: 0.9969000220298767\n",
      "-------Maximus Trout: 0.9927999973297119\n",
      "-------Maxe Trout: 1.0\n",
      "-------Mara Trout: 0.6036999821662903\n",
      "-------M.R. Trout: 0.4293000102043152\n",
      "-------Trout MRF: 0.5156000256538391\n",
      "-------Gooding, Emily: 0.17759999632835388\n",
      "-------Gooding, E.Y. MD: 0.20080000162124634\n",
      "-------Jarvis Richard James: 0.385699987411499\n",
      "\n",
      "\n",
      "9 Mara Trout\n",
      "-------J.R. Tolkien: 0.1137000024318695\n",
      "-------Tolkien, J.R.: 0.19189999997615814\n",
      "-------Justin Earl Tolkien: 0.3416000008583069\n",
      "-------Jarvis Richard Tolkien: 0.2685999870300293\n",
      "-------Tolkien, Justin: 0.22380000352859497\n",
      "-------Tolkien JE: 0.08560000360012054\n",
      "-------Max Trout: 0.5867999792098999\n",
      "-------Maximus Trout: 0.5881999731063843\n",
      "-------Maxe Trout: 0.6036999821662903\n",
      "-------Mara Trout: 1.0\n",
      "-------M.R. Trout: 0.5728999972343445\n",
      "-------Trout MRF: 0.6377000212669373\n",
      "-------Gooding, Emily: 0.29789999127388\n",
      "-------Gooding, E.Y. MD: 0.2092999964952469\n",
      "-------Jarvis Richard James: 0.188400000333786\n",
      "\n",
      "\n",
      "10 M.R. Trout\n",
      "-------J.R. Tolkien: 0.41499999165534973\n",
      "-------Tolkien, J.R.: 0.15970000624656677\n",
      "-------Justin Earl Tolkien: 0.20149999856948853\n",
      "-------Jarvis Richard Tolkien: 0.3147999942302704\n",
      "-------Tolkien, Justin: 0.13199999928474426\n",
      "-------Tolkien JE: 0.15940000116825104\n",
      "-------Max Trout: 0.4205000102519989\n",
      "-------Maximus Trout: 0.40139999985694885\n",
      "-------Maxe Trout: 0.4293000102043152\n",
      "-------Mara Trout: 0.5728999972343445\n",
      "-------M.R. Trout: 1.0\n",
      "-------Trout MRF: 0.558899998664856\n",
      "-------Gooding, Emily: 0.32100000977516174\n",
      "-------Gooding, E.Y. MD: 0.1136000007390976\n",
      "-------Jarvis Richard James: 0.33160001039505005\n",
      "\n",
      "\n",
      "11 Trout MRF\n",
      "-------J.R. Tolkien: 0.16910000145435333\n",
      "-------Tolkien, J.R.: 0.373199999332428\n",
      "-------Justin Earl Tolkien: 0.2994000017642975\n",
      "-------Jarvis Richard Tolkien: 0.3944999873638153\n",
      "-------Tolkien, Justin: 0.20730000734329224\n",
      "-------Tolkien JE: 0.13199999928474426\n",
      "-------Max Trout: 0.5200999975204468\n",
      "-------Maximus Trout: 0.5159000158309937\n",
      "-------Maxe Trout: 0.5156000256538391\n",
      "-------Mara Trout: 0.6377000212669373\n",
      "-------M.R. Trout: 0.558899998664856\n",
      "-------Trout MRF: 1.0\n",
      "-------Gooding, Emily: 0.23250000178813934\n",
      "-------Gooding, E.Y. MD: 0.164000004529953\n",
      "-------Jarvis Richard James: 0.3889999985694885\n",
      "\n",
      "\n",
      "12 Gooding, Emily\n",
      "-------J.R. Tolkien: 0.29100000858306885\n",
      "-------Tolkien, J.R.: 0.2827000021934509\n",
      "-------Justin Earl Tolkien: 0.41999998688697815\n",
      "-------Jarvis Richard Tolkien: 0.3285999894142151\n",
      "-------Tolkien, Justin: 0.4636000096797943\n",
      "-------Tolkien JE: 0.42149999737739563\n",
      "-------Max Trout: 0.16439999639987946\n",
      "-------Maximus Trout: 0.17730000615119934\n",
      "-------Maxe Trout: 0.17759999632835388\n",
      "-------Mara Trout: 0.29789999127388\n",
      "-------M.R. Trout: 0.32100000977516174\n",
      "-------Trout MRF: 0.23250000178813934\n",
      "-------Gooding, Emily: 1.0\n",
      "-------Gooding, E.Y. MD: 0.3529999852180481\n",
      "-------Jarvis Richard James: 0.32179999351501465\n",
      "\n",
      "\n",
      "13 Gooding, E.Y. MD\n",
      "-------J.R. Tolkien: 0.27379998564720154\n",
      "-------Tolkien, J.R.: 0.4147999882698059\n",
      "-------Justin Earl Tolkien: 0.4426000118255615\n",
      "-------Jarvis Richard Tolkien: 0.14219999313354492\n",
      "-------Tolkien, Justin: 0.3345000147819519\n",
      "-------Tolkien JE: 0.34450000524520874\n",
      "-------Max Trout: 0.19859999418258667\n",
      "-------Maximus Trout: 0.23360000550746918\n",
      "-------Maxe Trout: 0.20080000162124634\n",
      "-------Mara Trout: 0.2092999964952469\n",
      "-------M.R. Trout: 0.1136000007390976\n",
      "-------Trout MRF: 0.164000004529953\n",
      "-------Gooding, Emily: 0.3529999852180481\n",
      "-------Gooding, E.Y. MD: 1.0\n",
      "-------Jarvis Richard James: 0.08030000329017639\n",
      "\n",
      "\n",
      "14 Jarvis Richard James\n",
      "-------J.R. Tolkien: 0.5273000001907349\n",
      "-------Tolkien, J.R.: 0.3686999976634979\n",
      "-------Justin Earl Tolkien: 0.13210000097751617\n",
      "-------Jarvis Richard Tolkien: 0.9412000179290771\n",
      "-------Tolkien, Justin: 0.12120000272989273\n",
      "-------Tolkien JE: 0.09700000286102295\n",
      "-------Max Trout: 0.37770000100135803\n",
      "-------Maximus Trout: 0.3621000051498413\n",
      "-------Maxe Trout: 0.385699987411499\n",
      "-------Mara Trout: 0.188400000333786\n",
      "-------M.R. Trout: 0.33160001039505005\n",
      "-------Trout MRF: 0.3889999985694885\n",
      "-------Gooding, Emily: 0.32179999351501465\n",
      "-------Gooding, E.Y. MD: 0.08030000329017639\n",
      "-------Jarvis Richard James: 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ith, (emb, test_name) in enumerate(zip(embs, test_names)):\n",
    "    print(ith, test_name)\n",
    "    for emb_1, test_name_1 in zip(embs, test_names):\n",
    "        print(f\"-------{test_name_1}: {round(cosine_similarity(emb.reshape(1, -1), emb_1.reshape(1, -1))[0][0], 4)}\")\n",
    "    print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d5a069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d5d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb429936",
   "metadata": {},
   "source": [
    "## Testing different embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47ee9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f126a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_embeddings(embs_to_test, names_to_test):\n",
    "    for ith, (emb, test_name) in enumerate(zip(embs_to_test, names_to_test)):\n",
    "        all_scores = []\n",
    "        print(ith, test_name)\n",
    "        for emb_1, test_name_1 in zip(embs, names_to_test):\n",
    "            all_scores.append(round(cosine_similarity(emb.reshape(1, -1), emb_1.reshape(1, -1))[0][0], 4))\n",
    "        \n",
    "        ind = np.argpartition(np.array(all_scores), -5)[-5:]\n",
    "        \n",
    "        top_5 = ind[np.argsort(np.array(all_scores)[ind])].tolist()[::-1]\n",
    "        \n",
    "        for top_ind in top_5[1:]:\n",
    "            print(f\"---------- {names_to_test[top_ind]} - {all_scores[top_ind]}\")\n",
    "        print(\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b9e14",
   "metadata": {},
   "source": [
    "### [START] token decoder output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7669683f",
   "metadata": {},
   "source": [
    "##### Last layer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36ac1382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J.R. Tolkien\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Tolkien, J.R.\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Justin Earl Tolkien\n",
      "----Decoded name:  Justin Earl Tolkien\n",
      "Jarvis Richard Tolkien\n",
      "----Decoded name:  Jarvis Richard Tolkien\n",
      "Tolkien, Justin\n",
      "----Decoded name:  Justin Tolkien\n",
      "Tolkien JE\n",
      "----Decoded name:  J. E. Tolkien\n",
      "Max Trout\n",
      "----Decoded name:  Max Trout\n",
      "Maximus Trout\n",
      "----Decoded name:  Maximus Trout\n",
      "Maxe Trout\n",
      "----Decoded name:  Maxe Trout\n",
      "Mara Trout\n",
      "----Decoded name:  Mara Trout\n",
      "M.R. Trout\n",
      "----Decoded name:  M. R. Trout\n",
      "Trout MRF\n",
      "----Decoded name:  M. R. F. Trout\n",
      "Gooding, Emily\n",
      "----Decoded name:  Emily Gooding\n",
      "Gooding, E.Y. MD\n",
      "----Decoded name:  E. Y. Gooding\n",
      "Jarvis Richard James\n",
      "----Decoded name:  Richard James Jarvis\n"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "for test_name in test_names:\n",
    "    print(test_name)\n",
    "    start_end = tf.constant(name_tokenizer(['[START][END]'])['input_ids'][0])\n",
    "    start = start_end[0].numpy()\n",
    "    end = start_end[1].numpy()\n",
    "    encoder_input = name_tokenizer([f'[START]{test_name}[END]'])['input_ids'][0]\n",
    "    encoder_input = tf.convert_to_tensor([encoder_input + [0]*(MAX_LEN-len(encoder_input))])\n",
    "    final_output = [start]\n",
    "    for i in range(MAX_LEN):\n",
    "        output = tf.convert_to_tensor([final_output + [0]*(MAX_LEN-len(final_output))])\n",
    "        predictions = transformer((encoder_input, output), training=False)\n",
    "\n",
    "        # Select the last token\n",
    "        predicted_id = tf.argmax(predictions[0][i]).numpy()\n",
    "\n",
    "        # Add to output\n",
    "        final_output.append(predicted_id)\n",
    "\n",
    "        if predicted_id == end:\n",
    "            break\n",
    "    print(\"----Decoded name: \", name_tokenizer.decode(final_output, skip_special_tokens=True, \n",
    "                 clean_up_tokenization_spaces=True).replace(\" ##\", \"\").replace(\" - \", \"-\"))\n",
    "    _ = transformer((encoder_input, tf.convert_to_tensor([final_output[:1] + [0]*(MAX_LEN-len(final_output[:1]))])), \n",
    "            training=False)\n",
    "    \n",
    "    embs.append(transformer.decoder.dec_layers[-1].context_data[0][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b0b70f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 J.R. Tolkien\n",
      "---------- Tolkien, J.R. - 0.6912999749183655\n",
      "---------- Justin Earl Tolkien - 0.6347000002861023\n",
      "---------- Tolkien JE - 0.611299991607666\n",
      "---------- M.R. Trout - 0.5792999863624573\n",
      "\n",
      "\n",
      "1 Tolkien, J.R.\n",
      "---------- J.R. Tolkien - 0.6912999749183655\n",
      "---------- Tolkien JE - 0.525600016117096\n",
      "---------- Gooding, E.Y. MD - 0.4986000061035156\n",
      "---------- Mara Trout - 0.47290000319480896\n",
      "\n",
      "\n",
      "2 Justin Earl Tolkien\n",
      "---------- Tolkien, Justin - 0.7226999998092651\n",
      "---------- J.R. Tolkien - 0.6347000002861023\n",
      "---------- Gooding, E.Y. MD - 0.5054000020027161\n",
      "---------- Maximus Trout - 0.5034000277519226\n",
      "\n",
      "\n",
      "3 Jarvis Richard Tolkien\n",
      "---------- Jarvis Richard James - 0.9329000115394592\n",
      "---------- J.R. Tolkien - 0.5389999747276306\n",
      "---------- Mara Trout - 0.5076000094413757\n",
      "---------- Tolkien, J.R. - 0.44119998812675476\n",
      "\n",
      "\n",
      "4 Tolkien, Justin\n",
      "---------- Justin Earl Tolkien - 0.7226999998092651\n",
      "---------- J.R. Tolkien - 0.37369999289512634\n",
      "---------- Maximus Trout - 0.30410000681877136\n",
      "---------- Jarvis Richard Tolkien - 0.2897000014781952\n",
      "\n",
      "\n",
      "5 Tolkien JE\n",
      "---------- J.R. Tolkien - 0.611299991607666\n",
      "---------- Tolkien, J.R. - 0.525600016117096\n",
      "---------- Justin Earl Tolkien - 0.4142000079154968\n",
      "---------- M.R. Trout - 0.4099000096321106\n",
      "\n",
      "\n",
      "6 Max Trout\n",
      "---------- Maxe Trout - 0.9801999926567078\n",
      "---------- Maximus Trout - 0.9758999943733215\n",
      "---------- Mara Trout - 0.7402999997138977\n",
      "---------- M.R. Trout - 0.7146999835968018\n",
      "\n",
      "\n",
      "7 Maximus Trout\n",
      "---------- Max Trout - 0.9758999943733215\n",
      "---------- Maxe Trout - 0.9751999974250793\n",
      "---------- Mara Trout - 0.7709000110626221\n",
      "---------- M.R. Trout - 0.6603999733924866\n",
      "\n",
      "\n",
      "8 Maxe Trout\n",
      "---------- Max Trout - 0.9801999926567078\n",
      "---------- Maximus Trout - 0.9751999974250793\n",
      "---------- Mara Trout - 0.7487000226974487\n",
      "---------- M.R. Trout - 0.6480000019073486\n",
      "\n",
      "\n",
      "9 Mara Trout\n",
      "---------- Maximus Trout - 0.7709000110626221\n",
      "---------- Maxe Trout - 0.7487000226974487\n",
      "---------- Max Trout - 0.7402999997138977\n",
      "---------- M.R. Trout - 0.6108999848365784\n",
      "\n",
      "\n",
      "10 M.R. Trout\n",
      "---------- Max Trout - 0.7146999835968018\n",
      "---------- Maximus Trout - 0.6603999733924866\n",
      "---------- Maxe Trout - 0.6480000019073486\n",
      "---------- Mara Trout - 0.6108999848365784\n",
      "\n",
      "\n",
      "11 Trout MRF\n",
      "---------- Maxe Trout - 0.5691999793052673\n",
      "---------- Mara Trout - 0.5584999918937683\n",
      "---------- Maximus Trout - 0.5164999961853027\n",
      "---------- Max Trout - 0.504800021648407\n",
      "\n",
      "\n",
      "12 Gooding, Emily\n",
      "---------- Tolkien JE - 0.3190000057220459\n",
      "---------- Tolkien, Justin - 0.2304999977350235\n",
      "---------- Gooding, E.Y. MD - 0.22390000522136688\n",
      "---------- Trout MRF - 0.21850000321865082\n",
      "\n",
      "\n",
      "13 Gooding, E.Y. MD\n",
      "---------- Justin Earl Tolkien - 0.5054000020027161\n",
      "---------- Tolkien, J.R. - 0.4986000061035156\n",
      "---------- J.R. Tolkien - 0.4018999934196472\n",
      "---------- Tolkien JE - 0.3822000026702881\n",
      "\n",
      "\n",
      "14 Jarvis Richard James\n",
      "---------- Jarvis Richard Tolkien - 0.9329000115394592\n",
      "---------- J.R. Tolkien - 0.44620001316070557\n",
      "---------- Trout MRF - 0.41290000081062317\n",
      "---------- Mara Trout - 0.41280001401901245\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_embeddings(embs, test_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac02452",
   "metadata": {},
   "source": [
    "#### Concat last 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ea8bcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J.R. Tolkien\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Tolkien, J.R.\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Justin Earl Tolkien\n",
      "----Decoded name:  Justin Earl Tolkien\n",
      "Jarvis Richard Tolkien\n",
      "----Decoded name:  Jarvis Richard Tolkien\n",
      "Tolkien, Justin\n",
      "----Decoded name:  Justin Tolkien\n",
      "Tolkien JE\n",
      "----Decoded name:  J. E. Tolkien\n",
      "Max Trout\n",
      "----Decoded name:  Max Trout\n",
      "Maximus Trout\n",
      "----Decoded name:  Maximus Trout\n",
      "Maxe Trout\n",
      "----Decoded name:  Maxe Trout\n",
      "Mara Trout\n",
      "----Decoded name:  Mara Trout\n",
      "M.R. Trout\n",
      "----Decoded name:  M. R. Trout\n",
      "Trout MRF\n",
      "----Decoded name:  M. R. F. Trout\n",
      "Gooding, Emily\n",
      "----Decoded name:  Emily Gooding\n",
      "Gooding, E.Y. MD\n",
      "----Decoded name:  E. Y. Gooding\n",
      "Jarvis Richard James\n",
      "----Decoded name:  Richard James Jarvis\n"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "for test_name in test_names:\n",
    "    print(test_name)\n",
    "    start_end = tf.constant(name_tokenizer(['[START][END]'])['input_ids'][0])\n",
    "    start = start_end[0].numpy()\n",
    "    end = start_end[1].numpy()\n",
    "    encoder_input = name_tokenizer([f'[START]{test_name}[END]'])['input_ids'][0]\n",
    "    encoder_input = tf.convert_to_tensor([encoder_input + [0]*(MAX_LEN-len(encoder_input))])\n",
    "    final_output = [start]\n",
    "    for i in range(MAX_LEN):\n",
    "        output = tf.convert_to_tensor([final_output + [0]*(MAX_LEN-len(final_output))])\n",
    "        predictions = transformer((encoder_input, output), training=False)\n",
    "\n",
    "        # Select the last token\n",
    "        predicted_id = tf.argmax(predictions[0][i]).numpy()\n",
    "\n",
    "        # Add to output\n",
    "        final_output.append(predicted_id)\n",
    "\n",
    "        if predicted_id == end:\n",
    "            break\n",
    "    print(\"----Decoded name: \", name_tokenizer.decode(final_output, skip_special_tokens=True, \n",
    "                 clean_up_tokenization_spaces=True).replace(\" ##\", \"\").replace(\" - \", \"-\"))\n",
    "    _ = transformer((encoder_input, tf.convert_to_tensor([final_output[:1] + [0]*(MAX_LEN-len(final_output[:1]))])), \n",
    "            training=False)\n",
    "    \n",
    "    embs.append(np.concatenate((transformer.decoder.dec_layers[-4].context_data[0][0].numpy(),\n",
    "                    transformer.decoder.dec_layers[-3].context_data[0][0].numpy(),\n",
    "                    transformer.decoder.dec_layers[-2].context_data[0][0].numpy(),\n",
    "                    transformer.decoder.dec_layers[-1].context_data[0][0].numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "110fae47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 J.R. Tolkien\n",
      "---------- Tolkien, J.R. - 0.6916999816894531\n",
      "---------- Justin Earl Tolkien - 0.6344000101089478\n",
      "---------- Tolkien JE - 0.6119999885559082\n",
      "---------- M.R. Trout - 0.5781000256538391\n",
      "\n",
      "\n",
      "1 Tolkien, J.R.\n",
      "---------- J.R. Tolkien - 0.6916999816894531\n",
      "---------- Tolkien JE - 0.5270000100135803\n",
      "---------- Gooding, E.Y. MD - 0.49799999594688416\n",
      "---------- Mara Trout - 0.47189998626708984\n",
      "\n",
      "\n",
      "2 Justin Earl Tolkien\n",
      "---------- Tolkien, Justin - 0.7232999801635742\n",
      "---------- J.R. Tolkien - 0.6344000101089478\n",
      "---------- Gooding, E.Y. MD - 0.5044999718666077\n",
      "---------- Maximus Trout - 0.5030999779701233\n",
      "\n",
      "\n",
      "3 Jarvis Richard Tolkien\n",
      "---------- Jarvis Richard James - 0.9330999851226807\n",
      "---------- J.R. Tolkien - 0.5388000011444092\n",
      "---------- Mara Trout - 0.5069000124931335\n",
      "---------- Tolkien, J.R. - 0.4415999948978424\n",
      "\n",
      "\n",
      "4 Tolkien, Justin\n",
      "---------- Justin Earl Tolkien - 0.7232999801635742\n",
      "---------- J.R. Tolkien - 0.3741999864578247\n",
      "---------- Maximus Trout - 0.3043000102043152\n",
      "---------- Jarvis Richard Tolkien - 0.28940001130104065\n",
      "\n",
      "\n",
      "5 Tolkien JE\n",
      "---------- J.R. Tolkien - 0.6119999885559082\n",
      "---------- Tolkien, J.R. - 0.5270000100135803\n",
      "---------- Justin Earl Tolkien - 0.41519999504089355\n",
      "---------- M.R. Trout - 0.40869998931884766\n",
      "\n",
      "\n",
      "6 Max Trout\n",
      "---------- Maxe Trout - 0.9803000092506409\n",
      "---------- Maximus Trout - 0.9758999943733215\n",
      "---------- Mara Trout - 0.7401999831199646\n",
      "---------- M.R. Trout - 0.7143999934196472\n",
      "\n",
      "\n",
      "7 Maximus Trout\n",
      "---------- Max Trout - 0.9758999943733215\n",
      "---------- Maxe Trout - 0.9753000140190125\n",
      "---------- Mara Trout - 0.7706000208854675\n",
      "---------- M.R. Trout - 0.6601999998092651\n",
      "\n",
      "\n",
      "8 Maxe Trout\n",
      "---------- Max Trout - 0.9803000092506409\n",
      "---------- Maximus Trout - 0.9753000140190125\n",
      "---------- Mara Trout - 0.7486000061035156\n",
      "---------- M.R. Trout - 0.6478999853134155\n",
      "\n",
      "\n",
      "9 Mara Trout\n",
      "---------- Maximus Trout - 0.7706000208854675\n",
      "---------- Maxe Trout - 0.7486000061035156\n",
      "---------- Max Trout - 0.7401999831199646\n",
      "---------- M.R. Trout - 0.6111000180244446\n",
      "\n",
      "\n",
      "10 M.R. Trout\n",
      "---------- Max Trout - 0.7143999934196472\n",
      "---------- Maximus Trout - 0.6601999998092651\n",
      "---------- Maxe Trout - 0.6478999853134155\n",
      "---------- Mara Trout - 0.6111000180244446\n",
      "\n",
      "\n",
      "11 Trout MRF\n",
      "---------- Maxe Trout - 0.5692999958992004\n",
      "---------- Mara Trout - 0.5590999722480774\n",
      "---------- Maximus Trout - 0.5167999863624573\n",
      "---------- Max Trout - 0.505299985408783\n",
      "\n",
      "\n",
      "12 Gooding, Emily\n",
      "---------- Tolkien JE - 0.3190000057220459\n",
      "---------- Tolkien, Justin - 0.23119999468326569\n",
      "---------- Gooding, E.Y. MD - 0.2249000072479248\n",
      "---------- Trout MRF - 0.21879999339580536\n",
      "\n",
      "\n",
      "13 Gooding, E.Y. MD\n",
      "---------- Justin Earl Tolkien - 0.5044999718666077\n",
      "---------- Tolkien, J.R. - 0.49799999594688416\n",
      "---------- J.R. Tolkien - 0.4011000096797943\n",
      "---------- Tolkien JE - 0.3822000026702881\n",
      "\n",
      "\n",
      "14 Jarvis Richard James\n",
      "---------- Jarvis Richard Tolkien - 0.9330999851226807\n",
      "---------- J.R. Tolkien - 0.44600000977516174\n",
      "---------- Mara Trout - 0.4122999906539917\n",
      "---------- Trout MRF - 0.4115999937057495\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_embeddings(embs, test_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee2b21b",
   "metadata": {},
   "source": [
    "### Pool full decoder output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9123696b",
   "metadata": {},
   "source": [
    "#### Last layer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e3b7c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J.R. Tolkien\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Tolkien, J.R.\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Justin Earl Tolkien\n",
      "----Decoded name:  Justin Earl Tolkien\n",
      "Jarvis Richard Tolkien\n",
      "----Decoded name:  Jarvis Richard Tolkien\n",
      "Tolkien, Justin\n",
      "----Decoded name:  Justin Tolkien\n",
      "Tolkien JE\n",
      "----Decoded name:  J. E. Tolkien\n",
      "Max Trout\n",
      "----Decoded name:  Max Trout\n",
      "Maximus Trout\n",
      "----Decoded name:  Maximus Trout\n",
      "Maxe Trout\n",
      "----Decoded name:  Maxe Trout\n",
      "Mara Trout\n",
      "----Decoded name:  Mara Trout\n",
      "M.R. Trout\n",
      "----Decoded name:  M. R. Trout\n",
      "Trout MRF\n",
      "----Decoded name:  M. R. F. Trout\n",
      "Gooding, Emily\n",
      "----Decoded name:  Emily Gooding\n",
      "Gooding, E.Y. MD\n",
      "----Decoded name:  E. Y. Gooding\n",
      "Jarvis Richard James\n",
      "----Decoded name:  Richard James Jarvis\n"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "for test_name in test_names:\n",
    "    print(test_name)\n",
    "    start_end = tf.constant(name_tokenizer(['[START][END]'])['input_ids'][0])\n",
    "    start = start_end[0].numpy()\n",
    "    end = start_end[1].numpy()\n",
    "    encoder_input = name_tokenizer([f'[START]{test_name}[END]'])['input_ids'][0]\n",
    "    encoder_input = tf.convert_to_tensor([encoder_input + [0]*(MAX_LEN-len(encoder_input))])\n",
    "    final_output = [start]\n",
    "    for i in range(MAX_LEN):\n",
    "        output = tf.convert_to_tensor([final_output + [0]*(MAX_LEN-len(final_output))])\n",
    "        predictions = transformer((encoder_input, output), training=False)\n",
    "\n",
    "        # Select the last token\n",
    "        predicted_id = tf.argmax(predictions[0][i]).numpy()\n",
    "\n",
    "        # Add to output\n",
    "        final_output.append(predicted_id)\n",
    "\n",
    "        if predicted_id == end:\n",
    "            break\n",
    "    final_output_len = len(final_output) - 1\n",
    "    print(\"----Decoded name: \", name_tokenizer.decode(final_output, skip_special_tokens=True, \n",
    "                 clean_up_tokenization_spaces=True).replace(\" ##\", \"\").replace(\" - \", \"-\"))\n",
    "    _ = transformer((encoder_input, tf.convert_to_tensor([final_output[:-1] + \n",
    "                                                          [0]*(MAX_LEN-len(final_output[:-1]))])), \n",
    "            training=False)\n",
    "    \n",
    "    embs.append(np.mean(transformer.decoder.dec_layers[-1].context_data[0][:final_output_len].numpy(), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cefd867d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 J.R. Tolkien\n",
      "---------- Justin Earl Tolkien - 0.8522999882698059\n",
      "---------- Tolkien, J.R. - 0.7605999708175659\n",
      "---------- Tolkien JE - 0.7462000250816345\n",
      "---------- Jarvis Richard Tolkien - 0.7218000292778015\n",
      "\n",
      "\n",
      "1 Tolkien, J.R.\n",
      "---------- J.R. Tolkien - 0.7605999708175659\n",
      "---------- Tolkien JE - 0.7236999869346619\n",
      "---------- Tolkien, Justin - 0.6431000232696533\n",
      "---------- Gooding, E.Y. MD - 0.5831000208854675\n",
      "\n",
      "\n",
      "2 Justin Earl Tolkien\n",
      "---------- J.R. Tolkien - 0.8522999882698059\n",
      "---------- Jarvis Richard Tolkien - 0.7939000129699707\n",
      "---------- Tolkien, Justin - 0.6883999705314636\n",
      "---------- Tolkien JE - 0.6370999813079834\n",
      "\n",
      "\n",
      "3 Jarvis Richard Tolkien\n",
      "---------- Justin Earl Tolkien - 0.7939000129699707\n",
      "---------- J.R. Tolkien - 0.7218000292778015\n",
      "---------- Tolkien, Justin - 0.6274999976158142\n",
      "---------- Jarvis Richard James - 0.5228000283241272\n",
      "\n",
      "\n",
      "4 Tolkien, Justin\n",
      "---------- Justin Earl Tolkien - 0.6883999705314636\n",
      "---------- J.R. Tolkien - 0.6549999713897705\n",
      "---------- Tolkien, J.R. - 0.6431000232696533\n",
      "---------- Tolkien JE - 0.6312000155448914\n",
      "\n",
      "\n",
      "5 Tolkien JE\n",
      "---------- J.R. Tolkien - 0.7462000250816345\n",
      "---------- Tolkien, J.R. - 0.7236999869346619\n",
      "---------- Justin Earl Tolkien - 0.6370999813079834\n",
      "---------- Tolkien, Justin - 0.6312000155448914\n",
      "\n",
      "\n",
      "6 Max Trout\n",
      "---------- Maxe Trout - 0.9821000099182129\n",
      "---------- Maximus Trout - 0.948199987411499\n",
      "---------- Mara Trout - 0.9125999808311462\n",
      "---------- M.R. Trout - 0.8343999981880188\n",
      "\n",
      "\n",
      "7 Maximus Trout\n",
      "---------- Max Trout - 0.948199987411499\n",
      "---------- Maxe Trout - 0.9200999736785889\n",
      "---------- Mara Trout - 0.89410001039505\n",
      "---------- M.R. Trout - 0.7961000204086304\n",
      "\n",
      "\n",
      "8 Maxe Trout\n",
      "---------- Max Trout - 0.9821000099182129\n",
      "---------- Mara Trout - 0.9265000224113464\n",
      "---------- Maximus Trout - 0.9200999736785889\n",
      "---------- M.R. Trout - 0.828499972820282\n",
      "\n",
      "\n",
      "9 Mara Trout\n",
      "---------- Maxe Trout - 0.9265000224113464\n",
      "---------- Max Trout - 0.9125999808311462\n",
      "---------- Maximus Trout - 0.89410001039505\n",
      "---------- M.R. Trout - 0.8537999987602234\n",
      "\n",
      "\n",
      "10 M.R. Trout\n",
      "---------- Mara Trout - 0.8537999987602234\n",
      "---------- Max Trout - 0.8343999981880188\n",
      "---------- Maxe Trout - 0.828499972820282\n",
      "---------- Maximus Trout - 0.7961000204086304\n",
      "\n",
      "\n",
      "11 Trout MRF\n",
      "---------- M.R. Trout - 0.7465000152587891\n",
      "---------- Max Trout - 0.5008000135421753\n",
      "---------- Mara Trout - 0.49639999866485596\n",
      "---------- Gooding, Emily - 0.47909998893737793\n",
      "\n",
      "\n",
      "12 Gooding, Emily\n",
      "---------- Gooding, E.Y. MD - 0.6674000024795532\n",
      "---------- Tolkien, J.R. - 0.5759999752044678\n",
      "---------- Tolkien JE - 0.5236999988555908\n",
      "---------- Trout MRF - 0.47909998893737793\n",
      "\n",
      "\n",
      "13 Gooding, E.Y. MD\n",
      "---------- Gooding, Emily - 0.6674000024795532\n",
      "---------- Tolkien, J.R. - 0.5831000208854675\n",
      "---------- Trout MRF - 0.46959999203681946\n",
      "---------- Tolkien JE - 0.4390000104904175\n",
      "\n",
      "\n",
      "14 Jarvis Richard James\n",
      "---------- Jarvis Richard Tolkien - 0.5228000283241272\n",
      "---------- Trout MRF - 0.4352000057697296\n",
      "---------- Gooding, Emily - 0.3490999937057495\n",
      "---------- Gooding, E.Y. MD - 0.2671999931335449\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_embeddings(embs, test_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1848b33",
   "metadata": {},
   "source": [
    "#### Concat last 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4f530bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J.R. Tolkien\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Tolkien, J.R.\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Justin Earl Tolkien\n",
      "----Decoded name:  Justin Earl Tolkien\n",
      "Jarvis Richard Tolkien\n",
      "----Decoded name:  Jarvis Richard Tolkien\n",
      "Tolkien, Justin\n",
      "----Decoded name:  Justin Tolkien\n",
      "Tolkien JE\n",
      "----Decoded name:  J. E. Tolkien\n",
      "Max Trout\n",
      "----Decoded name:  Max Trout\n",
      "Maximus Trout\n",
      "----Decoded name:  Maximus Trout\n",
      "Maxe Trout\n",
      "----Decoded name:  Maxe Trout\n",
      "Mara Trout\n",
      "----Decoded name:  Mara Trout\n",
      "M.R. Trout\n",
      "----Decoded name:  M. R. Trout\n",
      "Trout MRF\n",
      "----Decoded name:  M. R. F. Trout\n",
      "Gooding, Emily\n",
      "----Decoded name:  Emily Gooding\n",
      "Gooding, E.Y. MD\n",
      "----Decoded name:  E. Y. Gooding\n",
      "Jarvis Richard James\n",
      "----Decoded name:  Richard James Jarvis\n"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "for test_name in test_names:\n",
    "    print(test_name)\n",
    "    start_end = tf.constant(name_tokenizer(['[START][END]'])['input_ids'][0])\n",
    "    start = start_end[0].numpy()\n",
    "    end = start_end[1].numpy()\n",
    "    encoder_input = name_tokenizer([f'[START]{test_name}[END]'])['input_ids'][0]\n",
    "    encoder_input = tf.convert_to_tensor([encoder_input + [0]*(MAX_LEN-len(encoder_input))])\n",
    "    final_output = [start]\n",
    "    for i in range(MAX_LEN):\n",
    "        output = tf.convert_to_tensor([final_output + [0]*(MAX_LEN-len(final_output))])\n",
    "        predictions = transformer((encoder_input, output), training=False)\n",
    "\n",
    "        # Select the last token\n",
    "        predicted_id = tf.argmax(predictions[0][i]).numpy()\n",
    "\n",
    "        # Add to output\n",
    "        final_output.append(predicted_id)\n",
    "\n",
    "        if predicted_id == end:\n",
    "            break\n",
    "    final_output_len = len(final_output) - 1\n",
    "    print(\"----Decoded name: \", name_tokenizer.decode(final_output, skip_special_tokens=True, \n",
    "                 clean_up_tokenization_spaces=True).replace(\" ##\", \"\").replace(\" - \", \"-\"))\n",
    "    _ = transformer((encoder_input, tf.convert_to_tensor([final_output[:-1] + \n",
    "                                                          [0]*(MAX_LEN-len(final_output[:-1]))])), \n",
    "            training=False)\n",
    "    \n",
    "    embs.append(np.mean(transformer.decoder.dec_layers[-4].context_data[0][:final_output_len].numpy(), axis=0))\n",
    "    embs.append(np.mean(transformer.decoder.dec_layers[-3].context_data[0][:final_output_len].numpy(), axis=0))\n",
    "    embs.append(np.mean(transformer.decoder.dec_layers[-2].context_data[0][:final_output_len].numpy(), axis=0))\n",
    "    embs.append(np.mean(transformer.decoder.dec_layers[-1].context_data[0][:final_output_len].numpy(), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7fb66bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 J.R. Tolkien\n",
      "---------- Maxe Trout - 0.819599986076355\n",
      "---------- Tolkien, Justin - 0.8172000050544739\n",
      "---------- Gooding, Emily - 0.8073999881744385\n",
      "---------- Tolkien, J.R. - 0.7820000052452087\n",
      "\n",
      "\n",
      "1 Tolkien, J.R.\n",
      "---------- Mara Trout - 0.8208000063896179\n",
      "---------- Gooding, E.Y. MD - 0.8140000104904175\n",
      "---------- Tolkien JE - 0.78329998254776\n",
      "---------- J.R. Tolkien - 0.7820000052452087\n",
      "\n",
      "\n",
      "2 Justin Earl Tolkien\n",
      "---------- M.R. Trout - 0.8011000156402588\n",
      "---------- Jarvis Richard James - 0.7623999714851379\n",
      "---------- Max Trout - 0.7620999813079834\n",
      "---------- Tolkien, J.R. - 0.6766999959945679\n",
      "\n",
      "\n",
      "3 Jarvis Richard Tolkien\n",
      "---------- Trout MRF - 0.8522999882698059\n",
      "---------- Maximus Trout - 0.7605999708175659\n",
      "---------- Justin Earl Tolkien - 0.446399986743927\n",
      "---------- M.R. Trout - 0.27639999985694885\n",
      "\n",
      "\n",
      "4 Tolkien, Justin\n",
      "---------- J.R. Tolkien - 0.8172000050544739\n",
      "---------- Maxe Trout - 0.8046000003814697\n",
      "---------- Gooding, Emily - 0.7900999784469604\n",
      "---------- Tolkien JE - 0.762499988079071\n",
      "\n",
      "\n",
      "5 Tolkien JE\n",
      "---------- Tolkien, J.R. - 0.78329998254776\n",
      "---------- Tolkien, Justin - 0.762499988079071\n",
      "---------- Mara Trout - 0.7149999737739563\n",
      "---------- Gooding, E.Y. MD - 0.7013000249862671\n",
      "\n",
      "\n",
      "6 Max Trout\n",
      "---------- Justin Earl Tolkien - 0.7620999813079834\n",
      "---------- Tolkien JE - 0.6776000261306763\n",
      "---------- M.R. Trout - 0.5978000164031982\n",
      "---------- Jarvis Richard James - 0.5795000195503235\n",
      "\n",
      "\n",
      "7 Maximus Trout\n",
      "---------- Jarvis Richard Tolkien - 0.7605999708175659\n",
      "---------- Justin Earl Tolkien - 0.5649999976158142\n",
      "---------- Trout MRF - 0.5020999908447266\n",
      "---------- Max Trout - 0.47099998593330383\n",
      "\n",
      "\n",
      "8 Maxe Trout\n",
      "---------- Gooding, Emily - 0.9463000297546387\n",
      "---------- J.R. Tolkien - 0.819599986076355\n",
      "---------- Mara Trout - 0.805899977684021\n",
      "---------- Tolkien, Justin - 0.8046000003814697\n",
      "\n",
      "\n",
      "9 Mara Trout\n",
      "---------- Gooding, E.Y. MD - 0.9656999707221985\n",
      "---------- Tolkien, J.R. - 0.8208000063896179\n",
      "---------- Maxe Trout - 0.805899977684021\n",
      "---------- Gooding, Emily - 0.7753999829292297\n",
      "\n",
      "\n",
      "10 M.R. Trout\n",
      "---------- Jarvis Richard James - 0.8575000166893005\n",
      "---------- Justin Earl Tolkien - 0.8011000156402588\n",
      "---------- Mara Trout - 0.6603000164031982\n",
      "---------- Gooding, E.Y. MD - 0.635699987411499\n",
      "\n",
      "\n",
      "11 Trout MRF\n",
      "---------- Jarvis Richard Tolkien - 0.8522999882698059\n",
      "---------- Maximus Trout - 0.5020999908447266\n",
      "---------- M.R. Trout - 0.3483000099658966\n",
      "---------- Justin Earl Tolkien - 0.2630999982357025\n",
      "\n",
      "\n",
      "12 Gooding, Emily\n",
      "---------- Maxe Trout - 0.9463000297546387\n",
      "---------- J.R. Tolkien - 0.8073999881744385\n",
      "---------- Gooding, E.Y. MD - 0.8069000244140625\n",
      "---------- Tolkien, Justin - 0.7900999784469604\n",
      "\n",
      "\n",
      "13 Gooding, E.Y. MD\n",
      "---------- Mara Trout - 0.9656999707221985\n",
      "---------- Tolkien, J.R. - 0.8140000104904175\n",
      "---------- Gooding, Emily - 0.8069000244140625\n",
      "---------- Maxe Trout - 0.7763000130653381\n",
      "\n",
      "\n",
      "14 Jarvis Richard James\n",
      "---------- M.R. Trout - 0.8575000166893005\n",
      "---------- Justin Earl Tolkien - 0.7623999714851379\n",
      "---------- Gooding, E.Y. MD - 0.6565999984741211\n",
      "---------- Mara Trout - 0.6324999928474426\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_embeddings(embs, test_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c773b815",
   "metadata": {},
   "source": [
    "### Pool full encoder output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d5413",
   "metadata": {},
   "source": [
    "#### Last layer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2cb29ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J.R. Tolkien\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Tolkien, J.R.\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Justin Earl Tolkien\n",
      "----Decoded name:  Justin Earl Tolkien\n",
      "Jarvis Richard Tolkien\n",
      "----Decoded name:  Jarvis Richard Tolkien\n",
      "Tolkien, Justin\n",
      "----Decoded name:  Justin Tolkien\n",
      "Tolkien JE\n",
      "----Decoded name:  J. E. Tolkien\n",
      "Max Trout\n",
      "----Decoded name:  Max Trout\n",
      "Maximus Trout\n",
      "----Decoded name:  Maximus Trout\n",
      "Maxe Trout\n",
      "----Decoded name:  Maxe Trout\n",
      "Mara Trout\n",
      "----Decoded name:  Mara Trout\n",
      "M.R. Trout\n",
      "----Decoded name:  M. R. Trout\n",
      "Trout MRF\n",
      "----Decoded name:  M. R. F. Trout\n",
      "Gooding, Emily\n",
      "----Decoded name:  Emily Gooding\n",
      "Gooding, E.Y. MD\n",
      "----Decoded name:  E. Y. Gooding\n",
      "Jarvis Richard James\n",
      "----Decoded name:  Richard James Jarvis\n"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "for test_name in test_names:\n",
    "    print(test_name)\n",
    "    start_end = tf.constant(name_tokenizer(['[START][END]'])['input_ids'][0])\n",
    "    start = start_end[0].numpy()\n",
    "    end = start_end[1].numpy()\n",
    "    encoder_input = name_tokenizer([f'[START]{test_name}[END]'])['input_ids'][0]\n",
    "    final_input_len = len(encoder_input)\n",
    "    encoder_input = tf.convert_to_tensor([encoder_input + [0]*(MAX_LEN-len(encoder_input))])\n",
    "    final_output = [start]\n",
    "    for i in range(MAX_LEN):\n",
    "        output = tf.convert_to_tensor([final_output + [0]*(MAX_LEN-len(final_output))])\n",
    "        predictions = transformer((encoder_input, output), training=False)\n",
    "\n",
    "        # Select the last token\n",
    "        predicted_id = tf.argmax(predictions[0][i]).numpy()\n",
    "\n",
    "        # Add to output\n",
    "        final_output.append(predicted_id)\n",
    "\n",
    "        if predicted_id == end:\n",
    "            break\n",
    "    print(\"----Decoded name: \", name_tokenizer.decode(final_output, skip_special_tokens=True, \n",
    "                 clean_up_tokenization_spaces=True).replace(\" ##\", \"\").replace(\" - \", \"-\"))\n",
    "    _ = transformer((encoder_input, tf.convert_to_tensor([final_output[:-1] + \n",
    "                                                          [0]*(MAX_LEN-len(final_output[:-1]))])), \n",
    "            training=False)\n",
    "    \n",
    "    embs.append(np.mean(transformer.encoder.enc_layers[-1].context_data[0][:final_input_len].numpy(), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3646ffca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 J.R. Tolkien\n",
      "---------- Tolkien, J.R. - 0.7846999764442444\n",
      "---------- M.R. Trout - 0.7739999890327454\n",
      "---------- Justin Earl Tolkien - 0.7416999936103821\n",
      "---------- Jarvis Richard Tolkien - 0.728600025177002\n",
      "\n",
      "\n",
      "1 Tolkien, J.R.\n",
      "---------- Gooding, E.Y. MD - 0.8529000282287598\n",
      "---------- Tolkien, Justin - 0.8291000127792358\n",
      "---------- Gooding, Emily - 0.817300021648407\n",
      "---------- Tolkien JE - 0.7897999882698059\n",
      "\n",
      "\n",
      "2 Justin Earl Tolkien\n",
      "---------- Jarvis Richard Tolkien - 0.9140999913215637\n",
      "---------- Tolkien, Justin - 0.8712999820709229\n",
      "---------- Tolkien JE - 0.795199990272522\n",
      "---------- Maximus Trout - 0.7799000144004822\n",
      "\n",
      "\n",
      "3 Jarvis Richard Tolkien\n",
      "---------- Justin Earl Tolkien - 0.9140999913215637\n",
      "---------- Jarvis Richard James - 0.8639000058174133\n",
      "---------- Tolkien, Justin - 0.8409000039100647\n",
      "---------- Gooding, Emily - 0.7947999835014343\n",
      "\n",
      "\n",
      "4 Tolkien, Justin\n",
      "---------- Gooding, Emily - 0.8968999981880188\n",
      "---------- Justin Earl Tolkien - 0.8712999820709229\n",
      "---------- Jarvis Richard Tolkien - 0.8409000039100647\n",
      "---------- Tolkien JE - 0.8327000141143799\n",
      "\n",
      "\n",
      "5 Tolkien JE\n",
      "---------- Tolkien, Justin - 0.8327000141143799\n",
      "---------- Justin Earl Tolkien - 0.795199990272522\n",
      "---------- Tolkien, J.R. - 0.7897999882698059\n",
      "---------- Gooding, Emily - 0.7893000245094299\n",
      "\n",
      "\n",
      "6 Max Trout\n",
      "---------- Maxe Trout - 0.9660999774932861\n",
      "---------- Maximus Trout - 0.8942999839782715\n",
      "---------- Mara Trout - 0.868399977684021\n",
      "---------- Trout MRF - 0.7038000226020813\n",
      "\n",
      "\n",
      "7 Maximus Trout\n",
      "---------- Maxe Trout - 0.9193000197410583\n",
      "---------- Max Trout - 0.8942999839782715\n",
      "---------- Mara Trout - 0.84170001745224\n",
      "---------- Tolkien, Justin - 0.8109999895095825\n",
      "\n",
      "\n",
      "8 Maxe Trout\n",
      "---------- Max Trout - 0.9660999774932861\n",
      "---------- Maximus Trout - 0.9193000197410583\n",
      "---------- Mara Trout - 0.8795999884605408\n",
      "---------- Tolkien, Justin - 0.7451000213623047\n",
      "\n",
      "\n",
      "9 Mara Trout\n",
      "---------- Maxe Trout - 0.8795999884605408\n",
      "---------- Max Trout - 0.868399977684021\n",
      "---------- Maximus Trout - 0.84170001745224\n",
      "---------- Trout MRF - 0.7171000242233276\n",
      "\n",
      "\n",
      "10 M.R. Trout\n",
      "---------- J.R. Tolkien - 0.7739999890327454\n",
      "---------- Trout MRF - 0.7261999845504761\n",
      "---------- Mara Trout - 0.6858999729156494\n",
      "---------- Tolkien, J.R. - 0.670199990272522\n",
      "\n",
      "\n",
      "11 Trout MRF\n",
      "---------- M.R. Trout - 0.7261999845504761\n",
      "---------- Tolkien, J.R. - 0.725600004196167\n",
      "---------- Tolkien JE - 0.7184000015258789\n",
      "---------- Mara Trout - 0.7171000242233276\n",
      "\n",
      "\n",
      "12 Gooding, Emily\n",
      "---------- Tolkien, Justin - 0.8968999981880188\n",
      "---------- Tolkien, J.R. - 0.817300021648407\n",
      "---------- Jarvis Richard Tolkien - 0.7947999835014343\n",
      "---------- Tolkien JE - 0.7893000245094299\n",
      "\n",
      "\n",
      "13 Gooding, E.Y. MD\n",
      "---------- Tolkien, J.R. - 0.8529000282287598\n",
      "---------- Gooding, Emily - 0.6960999965667725\n",
      "---------- Tolkien JE - 0.6650000214576721\n",
      "---------- Trout MRF - 0.6463000178337097\n",
      "\n",
      "\n",
      "14 Jarvis Richard James\n",
      "---------- Jarvis Richard Tolkien - 0.8639000058174133\n",
      "---------- Gooding, Emily - 0.7013000249862671\n",
      "---------- Justin Earl Tolkien - 0.6969000101089478\n",
      "---------- Tolkien, Justin - 0.6585000157356262\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_embeddings(embs, test_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaab349",
   "metadata": {},
   "source": [
    "#### Concat last 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2b08367d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J.R. Tolkien\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Tolkien, J.R.\n",
      "----Decoded name:  J. R. Tolkien\n",
      "Justin Earl Tolkien\n",
      "----Decoded name:  Justin Earl Tolkien\n",
      "Jarvis Richard Tolkien\n",
      "----Decoded name:  Jarvis Richard Tolkien\n",
      "Tolkien, Justin\n",
      "----Decoded name:  Justin Tolkien\n",
      "Tolkien JE\n",
      "----Decoded name:  J. E. Tolkien\n",
      "Max Trout\n",
      "----Decoded name:  Max Trout\n",
      "Maximus Trout\n",
      "----Decoded name:  Maximus Trout\n",
      "Maxe Trout\n",
      "----Decoded name:  Maxe Trout\n",
      "Mara Trout\n",
      "----Decoded name:  Mara Trout\n",
      "M.R. Trout\n",
      "----Decoded name:  M. R. Trout\n",
      "Trout MRF\n",
      "----Decoded name:  M. R. F. Trout\n",
      "Gooding, Emily\n",
      "----Decoded name:  Emily Gooding\n",
      "Gooding, E.Y. MD\n",
      "----Decoded name:  E. Y. Gooding\n",
      "Jarvis Richard James\n",
      "----Decoded name:  Richard James Jarvis\n"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "for test_name in test_names:\n",
    "    print(test_name)\n",
    "    start_end = tf.constant(name_tokenizer(['[START][END]'])['input_ids'][0])\n",
    "    start = start_end[0].numpy()\n",
    "    end = start_end[1].numpy()\n",
    "    encoder_input = name_tokenizer([f'[START]{test_name}[END]'])['input_ids'][0]\n",
    "    final_input_len = len(encoder_input)\n",
    "    encoder_input = tf.convert_to_tensor([encoder_input + [0]*(MAX_LEN-len(encoder_input))])\n",
    "    final_output = [start]\n",
    "    for i in range(MAX_LEN):\n",
    "        output = tf.convert_to_tensor([final_output + [0]*(MAX_LEN-len(final_output))])\n",
    "        predictions = transformer((encoder_input, output), training=False)\n",
    "\n",
    "        # Select the last token\n",
    "        predicted_id = tf.argmax(predictions[0][i]).numpy()\n",
    "\n",
    "        # Add to output\n",
    "        final_output.append(predicted_id)\n",
    "\n",
    "        if predicted_id == end:\n",
    "            break\n",
    "    \n",
    "    print(\"----Decoded name: \", name_tokenizer.decode(final_output, skip_special_tokens=True, \n",
    "                 clean_up_tokenization_spaces=True).replace(\" ##\", \"\").replace(\" - \", \"-\"))\n",
    "    _ = transformer((encoder_input, tf.convert_to_tensor([final_output[:-1] + \n",
    "                                                          [0]*(MAX_LEN-len(final_output[:-1]))])), \n",
    "            training=False)\n",
    "    \n",
    "    embs.append(np.mean(transformer.encoder.enc_layers[-4].context_data[0][1:final_input_len+1].numpy(), axis=0))\n",
    "    embs.append(np.mean(transformer.encoder.enc_layers[-3].context_data[0][1:final_input_len+1].numpy(), axis=0))\n",
    "    embs.append(np.mean(transformer.encoder.enc_layers[-2].context_data[0][1:final_input_len+1].numpy(), axis=0))\n",
    "    embs.append(np.mean(transformer.encoder.enc_layers[-1].context_data[0][1:final_input_len+1].numpy(), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3bd8c7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 J.R. Tolkien\n",
      "---------- Tolkien, Justin - 0.9211000204086304\n",
      "---------- Tolkien, J.R. - 0.8880000114440918\n",
      "---------- Tolkien JE - 0.8255000114440918\n",
      "---------- Maxe Trout - 0.8184999823570251\n",
      "\n",
      "\n",
      "1 Tolkien, J.R.\n",
      "---------- J.R. Tolkien - 0.8880000114440918\n",
      "---------- Justin Earl Tolkien - 0.8725000023841858\n",
      "---------- Tolkien JE - 0.8712000250816345\n",
      "---------- Tolkien, Justin - 0.8111000061035156\n",
      "\n",
      "\n",
      "2 Justin Earl Tolkien\n",
      "---------- Tolkien, J.R. - 0.8725000023841858\n",
      "---------- Max Trout - 0.8222000002861023\n",
      "---------- Jarvis Richard Tolkien - 0.7678999900817871\n",
      "---------- J.R. Tolkien - 0.7429999709129333\n",
      "\n",
      "\n",
      "3 Jarvis Richard Tolkien\n",
      "---------- Maximus Trout - 0.7853999733924866\n",
      "---------- Justin Earl Tolkien - 0.7678999900817871\n",
      "---------- Trout MRF - 0.7423999905586243\n",
      "---------- Tolkien, J.R. - 0.66839998960495\n",
      "\n",
      "\n",
      "4 Tolkien, Justin\n",
      "---------- J.R. Tolkien - 0.9211000204086304\n",
      "---------- Tolkien JE - 0.896399974822998\n",
      "---------- Tolkien, J.R. - 0.8111000061035156\n",
      "---------- Gooding, Emily - 0.8101999759674072\n",
      "\n",
      "\n",
      "5 Tolkien JE\n",
      "---------- Tolkien, Justin - 0.896399974822998\n",
      "---------- Tolkien, J.R. - 0.8712000250816345\n",
      "---------- Max Trout - 0.8454999923706055\n",
      "---------- J.R. Tolkien - 0.8255000114440918\n",
      "\n",
      "\n",
      "6 Max Trout\n",
      "---------- Tolkien JE - 0.8454999923706055\n",
      "---------- Justin Earl Tolkien - 0.8222000002861023\n",
      "---------- Tolkien, J.R. - 0.8004999756813049\n",
      "---------- Tolkien, Justin - 0.7633000016212463\n",
      "\n",
      "\n",
      "7 Maximus Trout\n",
      "---------- Jarvis Richard Tolkien - 0.7853999733924866\n",
      "---------- Trout MRF - 0.734499990940094\n",
      "---------- Max Trout - 0.7195000052452087\n",
      "---------- Tolkien JE - 0.6355999708175659\n",
      "\n",
      "\n",
      "8 Maxe Trout\n",
      "---------- Gooding, Emily - 0.9187999963760376\n",
      "---------- Mara Trout - 0.9031999707221985\n",
      "---------- J.R. Tolkien - 0.8184999823570251\n",
      "---------- Gooding, E.Y. MD - 0.8109999895095825\n",
      "\n",
      "\n",
      "9 Mara Trout\n",
      "---------- Maxe Trout - 0.9031999707221985\n",
      "---------- Gooding, E.Y. MD - 0.9000999927520752\n",
      "---------- M.R. Trout - 0.8932999968528748\n",
      "---------- Gooding, Emily - 0.8450000286102295\n",
      "\n",
      "\n",
      "10 M.R. Trout\n",
      "---------- Jarvis Richard James - 0.8988000154495239\n",
      "---------- Mara Trout - 0.8932999968528748\n",
      "---------- Gooding, E.Y. MD - 0.7919999957084656\n",
      "---------- Maxe Trout - 0.7850000262260437\n",
      "\n",
      "\n",
      "11 Trout MRF\n",
      "---------- M.R. Trout - 0.7598999738693237\n",
      "---------- Jarvis Richard Tolkien - 0.7423999905586243\n",
      "---------- Maximus Trout - 0.734499990940094\n",
      "---------- Mara Trout - 0.7167999744415283\n",
      "\n",
      "\n",
      "12 Gooding, Emily\n",
      "---------- Maxe Trout - 0.9187999963760376\n",
      "---------- Gooding, E.Y. MD - 0.9121000170707703\n",
      "---------- Mara Trout - 0.8450000286102295\n",
      "---------- Tolkien, Justin - 0.8101999759674072\n",
      "\n",
      "\n",
      "13 Gooding, E.Y. MD\n",
      "---------- Gooding, Emily - 0.9121000170707703\n",
      "---------- Mara Trout - 0.9000999927520752\n",
      "---------- Jarvis Richard James - 0.8776000142097473\n",
      "---------- Maxe Trout - 0.8109999895095825\n",
      "\n",
      "\n",
      "14 Jarvis Richard James\n",
      "---------- M.R. Trout - 0.8988000154495239\n",
      "---------- Gooding, E.Y. MD - 0.8776000142097473\n",
      "---------- Mara Trout - 0.7925999760627747\n",
      "---------- Gooding, Emily - 0.7713000178337097\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_embeddings(embs, test_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443ce2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0238b34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
